{"title":"Concurrency in Go","markdown":{"yaml":{"title":"Concurrency in Go","author":"Vahram Poghosyan","date":"2024-01-09","categories":["Golang"],"image":"concurrency_in_go.png","format":{"html":{"code-fold":true}},"jupyter":"python3","include-after-body":{"text":"<script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n"}},"headingText":"Convexity","containsRefs":false,"markdown":"\n\n  # Review of Linear Algebra and Geometry\n\nLet's start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that's commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\n> **Definition:** &nbsp; A set $C \\subseteq \\mathbb{R^d}$ is **convex** if, for all points $x_1,x_2 \\in C$ and any $\\theta \\in [0,1]$, the point $\\theta x_1 + (1-\\theta) x_2$ is also in $C$.\n\nThat is, a set is convex if the parametrized line segment between $x_1$ and $x_2$, any two points (or, more generally, vectors) in the set is also entirely inside the set.\n<br>\n\n::: {#fig-convexity layout-ncol=2}\n\n![Convex](../../assets/linear_algebra/convex.light.png){#fig-convex}\n\n![Non-convex](../../assets/linear_algebra/non_convex.light.png){#fig-non-convex}\n\nSet A is convex, set B is non-convex\n:::\n\n### Operations that Preserve Convexity\n\nScaling, skewing, and rotation (which can be thought of as *linear transformations*) preserve convexity as does shifting (an *affine transformation*). Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C' = \\{Ax + b : x \\in C \\}$ is convex provided that $C$ was convex.\n\nAn *intersection* of convex sets is also convex. That is, $C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection.\n\nHowever, *unions* of convex sets need not be convex.\n\n## Examples of Convex Sets\n\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A *point* and a *vector* mean the same thing.\n\n### Convex Hull of $n$ Points\n\nA *convex combination* of points $x_1, ..., x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ where $\\sum_{i = 1}^{n} \\theta_i = 1$ and $\\theta_i \\geq 0 \\ \\ \\forall i$.\n\nLet $x_1,x_2,...,x_n$ be $n$ points in space. Their *convex hull* is the set of all points which can be written as some convex combination of them. By varying parameters $\\theta_i$ we generate the convex hull as the set of all convex combinations of these points.\n\n::: {#fig-convex-hull}\n\n![Convex hull](../../assets/linear_algebra/convex_hull.light.png){width=300}\n\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points (if we imagine those to be pegs sticking out of the screen).\n:::\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon.\n\nFormally, the convex hull is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}$\n\n::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis's algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that [here](https://jeffe.cs.illinois.edu/teaching/compgeom/notes/14-convexhull.pdf).\n:::\n\n### Convex Hull of a Set\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there's an equivalent definition in terms of supersets.\n\nLet $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it's the intersection of all convex sets containing $C$. The result of such an intersection will be the unique $^{(†)}$ smallest convex superset of $C$, its convex hull. \n\n::: {.column-margin}\n(†) **Proof of uniqueness:** Let $C_1$ and $C_2$ be two convex hulls of $C$.  Let $c_1 \\in C_1$ be a point. Since $c_1 \\in C_1$, $c_1 \\in$ at least one of the convex supersets $C^{i}$of $C$. Hence, $c_1 \\in C_2$ since $C_2 = \\bigcap^{i=1 \\to n}C^{i}$. Similarly, it can be shown that any $c_2 \\in C_2$ also belongs to $C_1$. \nHence, $C_1 \\subseteq C_2$ and vice versa. This proves that $C_1 = C_2$ and completes the proof of uniqueness. \n:::\n\n::: {#fig-convex-hull}\n\n![Convex hull of a set](../../assets/linear_algebra/convex_hull_set.light.png){width=300}\n\nVisualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — it's simply the shape enclosed by a rubber band stretched around the non-convex set.\n:::\n\n\n### Affine Combination of $n$ Points\n\nAn *affine combination* of points $x_1,...,x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with $\\sum_{i=1}^{n}\\theta_i = 1$ but where the $\\theta_i$'s need not be non-negative. \n\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it's the *line* that passes through them, and for three points it's the *plane*. In general, it is the plane in $n$-dimensions passing through the $n$ points.\n\n### Linear Combinations - Hyperplanes and Halfspaces\n\nA *linear combination* of $n$ vectors, on the other hand, is all vectors of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with the $\\theta_i$'s totally unrestricted. \n\nThe set of all linear combinations of $n$ points is called their *span*. Formally, it is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}$.\n\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ points is a plane in $(n+1)$-dimensions which contains these points.\n\n\n#### Hyperplanes\n\nFor fixed weights $\\theta_i = a_i \\ \\ \\forall i$, a *hyperplane* is the set of all points $x \\in \\mathbb{R^n}$ whose linear combination equals a fixed constant $b \\in \\mathbb{R}$.\n\nFormally, a hyperplane is the set $\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}$ \n\nThere's a geometric interpretation of the parameters $a \\in \\mathbb{R^n}$ and $b \\in \\mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $\\{ x :  a^T x = 0\\}$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the *normal vector* to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the *offset* $b \\in \\mathbb{R}$ is introduced in the generalization $\\{ x : a^T x = b \\}$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that's been shifted from the origin by a distance of $\\frac{|b|}{\\|a\\|_2}$.\n\nSince the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we'll call $x_k$ for some $k \\in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane  in $\\mathbb{R^n}$ spans $n-1$ dimensions instead of $n$.\n<br>\n\n#### Halfspaces\n\nA *halfspace* is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $\\{ x : a^T x = b\\}$ are $\\{ x : a^T x \\geq b\\}$ and $\\{ x : a^T x \\leq b\\}$.\n\n### Conic Combinations of $n$ Points\n\nA *conic combination* of $x_1,...x_n$ is a point $x = \\sum_{i=1}^{n} \\theta_i x_i$ where $\\theta_i \\geq 0 \\ \\ \\forall i$. Note that the absence of the restriction that $\\sum_{i=1}^{n} \\theta_i = 1$ is what distinguishes a conic combination from a convex combination. \n\n### Ellipses\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the [sub-level sets](https://en.wikipedia.org/wiki/Level_set) of [quadratic forms](https://en.wikipedia.org/wiki/Quadratic_form). That is $\\{ x : (x-c)^T M (x-c) \\leq 1 \\}$ where $M \\succeq 0$ defines the stretch along each principal axis, and $c \\in \\mathbb{R^n}$ is the center. \n\nAn equivalent definition of an ellipse using the L2-norm is $\\{ x  : \\|Ax - b\\|_2 \\leq 1 \\}$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa.  \n\n::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\nMore generally, the ellipse is $\\{ x : (x-c)^T M (x-c) \\leq r \\}$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$'s positive semidefiniteness.\n:::\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$.\n\n$$\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n$$\n\nWhere the third equality is by the [spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices) of the real symmetric matrix $A^TA$, in which $D = diag(\\lambda_1,...,\\lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)$, we have the equivalent sub-level set definition of the ellipse. \n\n### Norm Balls\n\nRelated to ellipses are *Euclidean balls*, which are *norm balls* for the choice of the L2-norm. A Euclidean ball has the form $\\{ x : \\|x\\|_2 \\leq r \\}$, and is clearly convex as it's a generalizations of the sphere in $n$-dimensions. \n\nBut also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. \n\nIn general, norm balls $\\{ x : \\|x\\|_p \\leq r\\}$ where $\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p \\geq 1$.\n\n### Polyhedra\n\nWhere a halfspace is a set with one linear inequality constraint, a *polyhedron* is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A \\in \\mathbb{R^{m \\times n}}$ by vector $b \\in \\mathbb{R^m}$ multiplication form, making the polyhedron into the set $\\{x : Ax \\leq b\\}$.\n\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n### The Set of All Positive Semidefinite Matrices\n\nThe set of all PSD matrices $\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. \n\nNote that $Q \\mapsto x^TQx$ is a [linear functional](https://en.wikipedia.org/wiki/Linear_form) that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a \\mapsto x^Ta$ is a linear functional so, just as $\\{ a : x^Ta \\geq 0 \\}$ is a halfspace in the space of vectors, $H_x = \\{ Q : x^TQx \\geq 0 \\}$ for a given choice of $x \\in \\mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $\\{ Q :  x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x$, concluding the proof of its convexity. \n\n\n[^1]: **Proof of uniqueness of the minimal, convex superset:** \n    Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. But, any convex superset $D$ of\n    $C$ must necessarily contain the minimal, convex superset. Hence, $C_1 \\subseteq C_2$ and\n    similarly $C_2 \\subseteq C_1$, which implies $C_1 = C_2$.\n\n\n![](../../assets/linear_algebra/convex.dark.png){height=0}\n![](../../assets/linear_algebra/non_convex.dark.png){height=0}\n![](../../assets/linear_algebra/convex_hull.dark.png){height=0}\n![](../../assets/linear_algebra/convex_hull_set.dark.png){height=0}\n\n\n","srcMarkdownNoYaml":"\n\n  # Review of Linear Algebra and Geometry\n\nLet's start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that's commonly used in the subject.\n\n## Convexity\n\nSet convexity is defined as follows:\n\n> **Definition:** &nbsp; A set $C \\subseteq \\mathbb{R^d}$ is **convex** if, for all points $x_1,x_2 \\in C$ and any $\\theta \\in [0,1]$, the point $\\theta x_1 + (1-\\theta) x_2$ is also in $C$.\n\nThat is, a set is convex if the parametrized line segment between $x_1$ and $x_2$, any two points (or, more generally, vectors) in the set is also entirely inside the set.\n<br>\n\n::: {#fig-convexity layout-ncol=2}\n\n![Convex](../../assets/linear_algebra/convex.light.png){#fig-convex}\n\n![Non-convex](../../assets/linear_algebra/non_convex.light.png){#fig-non-convex}\n\nSet A is convex, set B is non-convex\n:::\n\n### Operations that Preserve Convexity\n\nScaling, skewing, and rotation (which can be thought of as *linear transformations*) preserve convexity as does shifting (an *affine transformation*). Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C' = \\{Ax + b : x \\in C \\}$ is convex provided that $C$ was convex.\n\nAn *intersection* of convex sets is also convex. That is, $C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection.\n\nHowever, *unions* of convex sets need not be convex.\n\n## Examples of Convex Sets\n\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A *point* and a *vector* mean the same thing.\n\n### Convex Hull of $n$ Points\n\nA *convex combination* of points $x_1, ..., x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ where $\\sum_{i = 1}^{n} \\theta_i = 1$ and $\\theta_i \\geq 0 \\ \\ \\forall i$.\n\nLet $x_1,x_2,...,x_n$ be $n$ points in space. Their *convex hull* is the set of all points which can be written as some convex combination of them. By varying parameters $\\theta_i$ we generate the convex hull as the set of all convex combinations of these points.\n\n::: {#fig-convex-hull}\n\n![Convex hull](../../assets/linear_algebra/convex_hull.light.png){width=300}\n\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points (if we imagine those to be pegs sticking out of the screen).\n:::\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon.\n\nFormally, the convex hull is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}$\n\n::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis's algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that [here](https://jeffe.cs.illinois.edu/teaching/compgeom/notes/14-convexhull.pdf).\n:::\n\n### Convex Hull of a Set\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there's an equivalent definition in terms of supersets.\n\nLet $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it's the intersection of all convex sets containing $C$. The result of such an intersection will be the unique $^{(†)}$ smallest convex superset of $C$, its convex hull. \n\n::: {.column-margin}\n(†) **Proof of uniqueness:** Let $C_1$ and $C_2$ be two convex hulls of $C$.  Let $c_1 \\in C_1$ be a point. Since $c_1 \\in C_1$, $c_1 \\in$ at least one of the convex supersets $C^{i}$of $C$. Hence, $c_1 \\in C_2$ since $C_2 = \\bigcap^{i=1 \\to n}C^{i}$. Similarly, it can be shown that any $c_2 \\in C_2$ also belongs to $C_1$. \nHence, $C_1 \\subseteq C_2$ and vice versa. This proves that $C_1 = C_2$ and completes the proof of uniqueness. \n:::\n\n::: {#fig-convex-hull}\n\n![Convex hull of a set](../../assets/linear_algebra/convex_hull_set.light.png){width=300}\n\nVisualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — it's simply the shape enclosed by a rubber band stretched around the non-convex set.\n:::\n\n\n### Affine Combination of $n$ Points\n\nAn *affine combination* of points $x_1,...,x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with $\\sum_{i=1}^{n}\\theta_i = 1$ but where the $\\theta_i$'s need not be non-negative. \n\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it's the *line* that passes through them, and for three points it's the *plane*. In general, it is the plane in $n$-dimensions passing through the $n$ points.\n\n### Linear Combinations - Hyperplanes and Halfspaces\n\nA *linear combination* of $n$ vectors, on the other hand, is all vectors of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with the $\\theta_i$'s totally unrestricted. \n\nThe set of all linear combinations of $n$ points is called their *span*. Formally, it is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}$.\n\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ points is a plane in $(n+1)$-dimensions which contains these points.\n\n\n#### Hyperplanes\n\nFor fixed weights $\\theta_i = a_i \\ \\ \\forall i$, a *hyperplane* is the set of all points $x \\in \\mathbb{R^n}$ whose linear combination equals a fixed constant $b \\in \\mathbb{R}$.\n\nFormally, a hyperplane is the set $\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}$ \n\nThere's a geometric interpretation of the parameters $a \\in \\mathbb{R^n}$ and $b \\in \\mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $\\{ x :  a^T x = 0\\}$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the *normal vector* to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the *offset* $b \\in \\mathbb{R}$ is introduced in the generalization $\\{ x : a^T x = b \\}$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that's been shifted from the origin by a distance of $\\frac{|b|}{\\|a\\|_2}$.\n\nSince the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we'll call $x_k$ for some $k \\in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane  in $\\mathbb{R^n}$ spans $n-1$ dimensions instead of $n$.\n<br>\n\n#### Halfspaces\n\nA *halfspace* is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $\\{ x : a^T x = b\\}$ are $\\{ x : a^T x \\geq b\\}$ and $\\{ x : a^T x \\leq b\\}$.\n\n### Conic Combinations of $n$ Points\n\nA *conic combination* of $x_1,...x_n$ is a point $x = \\sum_{i=1}^{n} \\theta_i x_i$ where $\\theta_i \\geq 0 \\ \\ \\forall i$. Note that the absence of the restriction that $\\sum_{i=1}^{n} \\theta_i = 1$ is what distinguishes a conic combination from a convex combination. \n\n### Ellipses\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the [sub-level sets](https://en.wikipedia.org/wiki/Level_set) of [quadratic forms](https://en.wikipedia.org/wiki/Quadratic_form). That is $\\{ x : (x-c)^T M (x-c) \\leq 1 \\}$ where $M \\succeq 0$ defines the stretch along each principal axis, and $c \\in \\mathbb{R^n}$ is the center. \n\nAn equivalent definition of an ellipse using the L2-norm is $\\{ x  : \\|Ax - b\\|_2 \\leq 1 \\}$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa.  \n\n::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\nMore generally, the ellipse is $\\{ x : (x-c)^T M (x-c) \\leq r \\}$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$'s positive semidefiniteness.\n:::\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$.\n\n$$\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n$$\n\nWhere the third equality is by the [spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices) of the real symmetric matrix $A^TA$, in which $D = diag(\\lambda_1,...,\\lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)$, we have the equivalent sub-level set definition of the ellipse. \n\n### Norm Balls\n\nRelated to ellipses are *Euclidean balls*, which are *norm balls* for the choice of the L2-norm. A Euclidean ball has the form $\\{ x : \\|x\\|_2 \\leq r \\}$, and is clearly convex as it's a generalizations of the sphere in $n$-dimensions. \n\nBut also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. \n\nIn general, norm balls $\\{ x : \\|x\\|_p \\leq r\\}$ where $\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p \\geq 1$.\n\n### Polyhedra\n\nWhere a halfspace is a set with one linear inequality constraint, a *polyhedron* is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A \\in \\mathbb{R^{m \\times n}}$ by vector $b \\in \\mathbb{R^m}$ multiplication form, making the polyhedron into the set $\\{x : Ax \\leq b\\}$.\n\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n### The Set of All Positive Semidefinite Matrices\n\nThe set of all PSD matrices $\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. \n\nNote that $Q \\mapsto x^TQx$ is a [linear functional](https://en.wikipedia.org/wiki/Linear_form) that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a \\mapsto x^Ta$ is a linear functional so, just as $\\{ a : x^Ta \\geq 0 \\}$ is a halfspace in the space of vectors, $H_x = \\{ Q : x^TQx \\geq 0 \\}$ for a given choice of $x \\in \\mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $\\{ Q :  x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x$, concluding the proof of its convexity. \n\n\n[^1]: **Proof of uniqueness of the minimal, convex superset:** \n    Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. But, any convex superset $D$ of\n    $C$ must necessarily contain the minimal, convex superset. Hence, $C_1 \\subseteq C_2$ and\n    similarly $C_2 \\subseteq C_1$, which implies $C_1 = C_2$.\n\n\n![](../../assets/linear_algebra/convex.dark.png){height=0}\n![](../../assets/linear_algebra/non_convex.dark.png){height=0}\n![](../../assets/linear_algebra/convex_hull.dark.png){height=0}\n![](../../assets/linear_algebra/convex_hull_set.dark.png){height=0}\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-after-body":{"text":"<script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n"},"output-file":"concurrency_in_go.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","grid":{"sidebar-width":"450px","margin-width":"350px","body-width":"1200px"},"theme":{"light":["flatly","../../theme-light.scss"],"dark":["flatly","../../theme-dark.scss"]},"title":"Concurrency in Go","author":"Vahram Poghosyan","date":"2024-01-09","categories":["Golang"],"image":"concurrency_in_go.png","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
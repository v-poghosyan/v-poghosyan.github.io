{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93fc4d8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Linear Programs and LP Geometry\"\n",
    "author: \"Vahram Poghosyan\"\n",
    "date: \"2022-01-23\"\n",
    "categories: [\"Optimization\"]\n",
    "image: \"introduction_to_optimization.png\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "  text: |\n",
    "    <script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea011175",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A linear program is a special type of convex optimization problem in $n$-dimensions that has a linear objective and a constraint set that's a polytope. That is, its constraint set is an intersection of $n$-dimensional linear inequalities (*halfspaces*) and linear equalities (*hyperplanes*). \n",
    "\n",
    "In matrix form, it may be stated as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546130d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A_1x \\leq b_1\n",
    "\\\\\n",
    "&A_2x \\geq b_2\n",
    "\\\\ \n",
    "&A_3x = b_3\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72456ed3",
   "metadata": {},
   "source": [
    "where $c \\in \\mathbb{R}^n$ is the *cost vector* of the objective function, $x \\in \\mathbb{R^n}$ is the *decision variable*, $A_1 \\in \\mathbb{R}^{m \\times n}$, $b_1 \\in \\mathbb{R}^m$ and $A_2 \\in \\mathbb{R}^{p \\times n}$, $b_2 \\in \\mathbb{R}^p$ together define the collection of linear inequality constraints, and $A_3 \\in \\mathbb{R}^{q \\times n}$ and $b_3 \\in \\mathbb{R}^q$ define the collection of linear equality constraints. \n",
    "\n",
    "As we will shortly prove, an LP in any form such as the one above can be converted into its *standard form*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f77339",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &Ax = b\n",
    "\\\\ \n",
    "&x \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases} \\dagger\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764cc9e",
   "metadata": {},
   "source": [
    "## Applications \n",
    "\n",
    "Linear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they're robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP's are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a [discrete failures scenario](../optimization/modeling_discrete_failures.ipynb), etc. \n",
    "\n",
    "The reason LP's are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5f58a",
   "metadata": {},
   "source": [
    "# Feasibility and Boundedness\n",
    "\n",
    "There are two ways in which LP's may fail to have an optimal solution, by either being *infeasible* or *unbounded*. Both of these cases are typically uninteresting in practice however they give important theoretical results. It's also useful to check whether or not a given LP is feasible and bounded before attempting to optimize. \n",
    "\n",
    "Infeasible LP's are those LP's that have an empty constraint set, whereas unbounded LP's have open constraint sets. However, it's important to understand that an LP may have an open constraint set without being unbounded. \n",
    "\n",
    "Consider the two examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842b11e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_{x_1}: x_1\n",
    "\\\\\n",
    "s.t.: x_1 \\leq 4\n",
    "\\end{cases} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc85e0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_{x_1}: x_1\n",
    "\\\\\n",
    "s.t.: x_1 \\geq 4\n",
    "\\end{cases} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3a887",
   "metadata": {},
   "source": [
    "The first problem is unbounded, since $x_1$ can be taken arbitrarily small. However, the second problem is bounded despite having an open constraint set. The optimal value of $(2)$ is $x_1 = 4$. \n",
    "\n",
    "Thus, an LP is said to be unbounded not when its constraint polytope is open, but when it's feasible and has no optimal solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87544385",
   "metadata": {},
   "source": [
    "# Converting to Standard Form\n",
    "\n",
    "Given any starting point, an LP can be written in standard form $\\dagger$. This is useful for standardization of the problem from an algorithmic perspective, and it's what the [Simplex algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm) relies on to solve LP's.\n",
    "\n",
    "In the most general case an LP can be stated with inequality constraints going in both directions and equality constraints as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6b7a8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A_1x \\leq  b_1\n",
    "\\\\\n",
    "&A_2x \\geq  b_2\n",
    "\\\\ \n",
    "&A_3x = b_3\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736029e0",
   "metadata": {},
   "source": [
    "But the inequality constraints can always be combined into $Ax \\leq b$ for $A = [A_1, A_2]$ \n",
    "and $b = [b_1, -b_2]^T$ and relabeled as $A_1$ and $b_1$. \n",
    "\n",
    "So there's no qualitative difference between having two types of inequalities versus just one. Simply concatenate and relabel the matrices to get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7f957",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A_1x \\leq  b_1\n",
    "\\\\\n",
    "&A_2x = b_2\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ad74b",
   "metadata": {},
   "source": [
    "The real challenge lies in converting the inequality constraints $A_1x \\leq b_1$ into equality constraints $A_1x = b_1$.\n",
    "\n",
    "## Introducing Slack Variables\n",
    "\n",
    "The inequality constraint $A_1x \\leq b_1$ has *slack*. Formally, we can define vector $s \\geq 0$ (component-wise), that bridges the gap between $A_1x$ and $b_1$, that is s.t. $A_1x + s = b_1$.\n",
    "\n",
    "Since this introduces new variables, we have to represent those in the objective and the equality constraints in a way that doesn't affect the optimization outcome.\n",
    "\n",
    "The LP becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01808ec7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx + \\mathbf{0}^Ts\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A_1x + s = b_1\n",
    "\\\\ \n",
    "&A_2x + 0s = b_2\n",
    "\\\\\n",
    "&s \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5af25",
   "metadata": {},
   "source": [
    "This LP is equivalent to the one before. Namely, if the previous optimizer was $x^*$, the optimizer in the new LP is the concatenation $[x^*, b_1 - A_1x]$ which gives the same optimal value in the objective function.\n",
    "\n",
    "This is almost in standard form, an LP with only equality constraints, and non-negativity constraints. However, the decision variable of this LP is the concatenation $[x,s]^T$, whereas the non-negativity applies to $s$ alone. \n",
    "\n",
    "The next step is to decompose $x$ as $x = x^+ - x^-$ where $x^+,x^- \\geq 0$ respectively contain only the positive and only the negative entries of $x$. That is, $x^+$, and $x^-$ have entries $x_i^+ = \\max\\{0, x_i\\}$ and $x_i^- = -\\min\\{0, x_i\\}$.\n",
    "\n",
    "With this substitution we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3aeab7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx^+ - c^Tx^- + \\mathbf{0}^Ts\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A_1x^+ - A_1x^- + s = b_1\n",
    "\\\\ \n",
    "&A_2x^+ - A_2x^- + 0s = b_2\n",
    "\\\\\n",
    "&x^+, x^-, s \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9b549",
   "metadata": {},
   "source": [
    "Which is an LP in standard form $\\dagger$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3600dd",
   "metadata": {},
   "source": [
    "# Geometry of Linear Programs\n",
    "\n",
    "Take a simple feasible, bounded LP in two dimensions that has a unique solution, draw its polygonal constraint set. Then draw the level sets of the objective function noting that the direction of steepest change (the positive or negative gradient) is perpendicular to the level sets. The conclusion is almost immediate — the unique optimal solution occurs at a vertex (i.e. an *extreme point*) of the polygonal constraint set.\n",
    "\n",
    "To formalize this, we need to introduce a few definitions and prove a theorem called The Extreme Point Theorem which can be found towards the end of this post.\n",
    "\n",
    "## Extreme Points - Geometric Definitions\n",
    "\n",
    "First, let's give a couple of geometric definitions of an extreme point.\n",
    "\n",
    "> **Definition 1:** &nbsp; A point $x$ is an **extreme point** of a polytope $P$ if it is *not* the convex combination of any other two points in the polytope.\n",
    "\n",
    "That is, if $\\exists y,z \\in P$ and $\\lambda \\in [0,1]$ s.t. $x = \\lambda y + (1- \\lambda)z$ then $x$ is *not* an extreme point of $P$.\n",
    "\n",
    "> **Definition 2:** &nbsp; A point $x$ is an **extreme point** of a polytope $P$ if it is the *unique* optimum for some cost vector $c$.\n",
    "\n",
    "That is, if $\\exists c \\in \\mathbb{R}^n$ s.t. $c^Tx < c^Ty \\ \\ \\forall y \\in P$ then $x$ is an extreme point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081762e",
   "metadata": {},
   "source": [
    "## Extreme Points - Algebraic Definition\n",
    "\n",
    "It's useful to define an extreme point algebraically. To that end, let's define the concept of a *basic feasible solution (BFS)*.\n",
    "\n",
    "Suppose we have the polytope $\\{x : Ax \\leq b, Dx = f\\}$. \n",
    "\n",
    "> **Definition:** &nbsp; An **active constraint at $x$** is a constraint that's satisfied through strict equality.\n",
    "\n",
    "That is, the $i$-th constraint is said to be active at x if $a_i^Tx = b_i$. \n",
    "\n",
    "This can be thought of as $x$ being on the edge of the halfspace defined by $a_i^Tx \\leq b_i$.\n",
    "\n",
    "We can also define the *active set* at $x$ as the set of all active constraints at $x$. \n",
    "\n",
    "So, the active set at $x$ is $\\mathcal{A}_x = \\{ a_i : a_i^Tx = b_i \\} \\cup \\{ d_i : d_i^Tx = f_i \\}$, where $\\{ d_i : d_i^Tx = f_i \\}$ is included for completeness. \n",
    "\n",
    "### Basic Feasible Solution\n",
    "\n",
    "We are now ready to define what it means for a point $x$ to be a basic feasible solution of a linear program.\n",
    "\n",
    "> **Definition:** &nbsp; The point $x$ is a **basic feasible solution (BFS)** of the linear program if its active set $\\mathcal{A}_x$ contains exactly $n$ linearly independent vectors where $n$ is the  dimension of $x$.\n",
    "\n",
    "Let's ponder the BFS definition for a minute. \n",
    "\n",
    "Imagine a closed polytope in 2D. Each of its vertices are defined by, at least, two intersecting lines. It's possible that a vertex is the result of the intersection of three or more lines, but deleting all but two of those lines will still retain the vertex. In other words, two linearly independent (i.e. non-parallel) constraints define an extreme point in 2D. \n",
    "\n",
    "The BFS definition is simply a generalization of this insight to $n$-dimensions.\n",
    "\n",
    "As we will prove shortly, BFS and extreme point are synonymous. In fact, the following are equivalent:\n",
    "\n",
    "* $x$ is an extreme point by *Definition 1*. \n",
    "* $x$ is an extreme point by *Definition 2*.\n",
    "* $x$ is a basic feasible solution.\n",
    "\n",
    "### Matrix-Vector Formulation of Basic Feasible Solutions\n",
    "\n",
    "Taking as our starting point an LP in standard form $\\dagger$ we can characterize basic feasible solutions in matrix-vector form. \n",
    "\n",
    "Take the standard constraint set $\\Omega = \\{ Ax = b, x \\geq 0 \\}$ and let's make a few simplifying assumptions. \n",
    "\n",
    "1. $A$ is $m \\times n$ with $m \\leq n$.\n",
    "2. $A$ is full-rank\n",
    "3. $b \\geq 0$\n",
    "4. $A$ has form $A = [B,D]$ where $B$ is an $m \\times m$ full-rank matrix and $D$ is the rest of $A$.\n",
    "\n",
    "Some of these assumptions impose restrictions on $\\Omega$, whereas others are without loss of generality.\n",
    "\n",
    "Assumption 1 is simply there to make the problem interesting. Were $n > m$, the system of equalities would be over-determined and the constraint set would either be empty or contain a single point, which itself would be the optimum. It is, therefore an assumption which is *not* done without loss of generality. \n",
    "\n",
    "Assumption 2 is equivalent to saying $rank(A) = m$. That is to say, all $m$ rows of $A$, as well as $m$ of the $n$ columns of $A$, are linearly independent. This assumption is also *not* done without loss of generality. Having less linearly independent rows corresponds to having less non-redundant constraints which clearly affects the constraint set $\\Omega$.\n",
    "\n",
    "Assumption 3 is done W.L.O.G. since the signs of $A$'s row entries can always be flipped. \n",
    "\n",
    "Assumption 4 is also done W.L.O.G. because if $A$ contains a full-rank $m \\times m$ submatrix per Assumption 2, then $A = [B,D]$ is a re-ordering of $A$ which adds no further restrictions on $\\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfded83",
   "metadata": {},
   "source": [
    "For $\\Omega$ that satisfies Assumptions 1-4, the basic feasible solutions can be reformulated as follows.\n",
    "\n",
    "> **Definition:** &nbsp; Let $x_B$ be such that. $Bx_B = b$. Then the concatenation $x = [x_B, 0]^T$ is a solution to $Ax = b$. Such solutions are called **feasible solutions**. Furthermore, if $x_B \\geq 0$, such solutions are called **basic feasible solutions**.\n",
    "\n",
    "Note that, for the case we're in, this is consistent with the earlier definition of a BFS.\n",
    "\n",
    "Let $x$ be a BFS according to this definition. $Ax = b$ poses a set of $m$ linearly independent constraints since $rank(A) = m$, whereas $x \\geq 0$ poses a set of $n$. But $x = [x_B, 0]^T$ is a vector at which all $m$ of the equality constraints $Ax = b$ are active, and $n-m$ of the inequality constraints $x \\geq 0$ are also active. So in total $n$ linearly independent constraints are active at a BFS, which is consistent with the earlier definition.\n",
    "\n",
    "\n",
    "### Basic Feasible Solutions and Extreme Points are Equivalent\n",
    "\n",
    "\n",
    "To formally prove that basic feasible solutions are extreme points in the geometric sense, consider the following theorem and its proof.\n",
    "\n",
    "> **Theorem:** &nbsp; The point $x$ is an extreme point of $\\Omega = \\{ Ax =b, x \\geq 0 \\}$ if and only if it is a basic feasible solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbd9dd",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "\n",
    "**Sufficiency $\\implies$:**\n",
    "\n",
    "Let $x$ be an extreme point of $\\Omega$. Since it's in $\\Omega$, $x \\geq 0$ and $Ax = b$. \n",
    "\n",
    "Equivalently, $\\sum_{i=1}^n x_ia_i = b$ where the $a_i$'s are the column vectors of $A$.\n",
    "\n",
    "Note that $x$ *must* contain zero entries, since it takes $n$ linearly independent active constraints to be an extreme point and only $m$ come from the equality constraints $Ax = b$. \n",
    "\n",
    "By Assumption 1, $m$ of $A$'s columns are linearly independent. We'd like to claim that these $m$ are exactly those corresponding to the non-zero $x_i$ entries. \n",
    "\n",
    "If this claim turns out to be true, then the full-rank $m \\times m$ submatrix $B$ will contain exactly those $m$ columns. And $x = [x_B, 0]^T$, where $x_B$ are the non-zero entries of $x$, would be a BFS. $\\ast$\n",
    "\n",
    "So, let's prove the linear independence claim using a contradiction argument. \n",
    "\n",
    "Without loss of generality, through rearrangement, let the first $m$ elements be the nonzero entries. That is, $x_1, ..., x_m > 0$, and $x_{m+1}, ... ,x_n = 0$.\n",
    "\n",
    "Then $\\sum_{i=1}^n x_ia_i = \\sum_{i=1}^m x_ia_i = b$.\n",
    "\n",
    "Towards contradiction, assume $a_1, ..., a_m$ are linearly dependent. Then $\\exists y_1, ..., y_m \\in \\mathbb{R}$ not all zero s.t. $y_1a_1 + ... + y_ma_m = 0$\n",
    "\n",
    "Take $\\epsilon > 0$ to be very small. Small enough so that $x_i \\pm \\epsilon y_i > 0 \\ \\ \\forall i = 1,...,m$.\n",
    "\n",
    "Define two points as \n",
    "\n",
    "$z^1 = [x_1 - \\epsilon y_1, ..., x_p - \\epsilon y_p, 0, ..., 0]^T$ and, \n",
    "$z^2 = [x_1 + \\epsilon y_1, ..., x_p + \\epsilon y_p, 0, ..., 0]^T$.\n",
    "\n",
    "These points clearly satisfy $z_1,z_2 \\geq 0$, so they they satisfy one of $\\Omega$'s constraints. \n",
    "\n",
    "Furthermore,\n",
    "\n",
    "$Az^1 = \\sum_{i=1}^m z^1_ia_i = \\sum_{i=1}^m x_ia_i - \\epsilon \\sum_{i=1}^m y_ia_i = b$ since $\\sum_{i=1}^m y_ia_i = 0$.\n",
    "\n",
    "and similarly $Az^2 = b$. \n",
    "\n",
    "So, $z^1$, and $z^2$ are indeed in $\\Omega$.\n",
    "\n",
    "But note that $x = \\frac{z^1 + z^2}{2}$ is a convex combination of two points in $\\Omega$, which contradicts the assumption that it's an extreme point. \n",
    "\n",
    "Hence, $a_1,...,a_m$ must be linearly independent. This concludes the proof by $\\ast$.\n",
    "\n",
    "**Necessity $\\impliedby$:**\n",
    "\n",
    "Suppose $x$ is a BFS and assume, towards contradiction, that it's *not* and extreme point of $\\Omega$. \n",
    "\n",
    "Then $\\exists y,z \\in \\Omega$ with $y \\ne z$ s.t. $x = \\alpha y + (1- \\alpha) z$ for some $\\alpha \\in (0,1)$.\n",
    "\n",
    "But since $y,z \\in \\Omega$ they satisfy $Ay = Az = b$, so $Ay - Az = A(y - z) = 0$. \n",
    "\n",
    "That is $(y_1 - z_1)a_1 + ... + (y_m - z_m)a_m = 0$. \n",
    "\n",
    "But since $y \\ne z$, not all $(y_i - z_i) = 0$. So, $a_1,..., a_m$ are linearly dependent. This contradicts the assumption that $x$ was a BFS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb45cc5",
   "metadata": {},
   "source": [
    "## The Extreme Point Theorem\n",
    "\n",
    "Why devote so much time defining extreme points geometrically, and then again algebraically? As hinted earlier and as shall be proved shortly, optima of linear programs occur at the extreme points. This is the reason LP's are a class of easy convex optimization problems — the search space for their optima can be reduced to a finite number of extreme points. \n",
    "\n",
    "> **The Extreme Point Theorem:** &nbsp; If a linear program has a finite optimum, and its constraint polytope has at least one extreme point,\n",
    "> then there is an extreme point which is optimal.\n",
    "\n",
    "So, if we want to solve linear programs we need only consider the extreme points.\n",
    "\n",
    "Let's prove the theorem through induction on the dimension. \n",
    "\n",
    "### Proof\n",
    "\n",
    "Take the following general LP and assume it has a finite optimum. Assume also that its constraint polytope has, at least, one extreme point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc8554",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\min_{x}: c^Tx\n",
    "\\\\\n",
    "s.t.: x \\in \\mathcal{P}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7da1b3",
   "metadata": {},
   "source": [
    "Assume the theorem is true for this LP with an $(n-1)$-dimensional constraint polytope. The objective is to show that it's also true for the same LP with an $n$-dimensional constraint polytope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa0ed4",
   "metadata": {},
   "source": [
    "Let $v$ be the optimal value of the LP.\n",
    "\n",
    "Let $Q = P \\cap \\{ x : c^Tx = v \\}$ be the intersection of the constraint polytope with the level set of the objective function at the optimal value. \n",
    "\n",
    "Since $Q$ is the intersection of an $n$-dimensional polytope $P$ with an additional linear constraint (a hyperplane), it is $(n-1)$-dimensional. \n",
    "\n",
    "By the inductive hypothesis, there is an extreme point $x^* \\in Q$ that's optimal for the LP.\n",
    "\n",
    "By a contradiction argument, $x^*$ is also an extreme point in $P$. \n",
    "\n",
    "Suppose it is *not* an extreme point in $P$. Then by *Definition 1* of extreme point, $x^*$ is a convex combination of two points in $P$. That is, $\\exists y,z \\in P$ s.t. $\\lambda y + (1- \\lambda)z = x^*$ for some $\\lambda \\in [0,1].$\n",
    "\n",
    "But then $\\lambda c^Ty + (1- \\lambda)c^Tz = c^Tx^* = v$, since $x^*$ is optimal. But the left hand side is a convex combination of scalars, so $c^Ty = c^Tz = v$. This means $y,z \\in Q$, which contradicts the fact that $x^*$ is an extreme point in $Q$. \n",
    "\n",
    "Hence, $x^*$ must be an extreme point in $P$.\n",
    "\n",
    "### Sketch for an Alternate Proof\n",
    "\n",
    "We can also prove the Extreme Point Theorem using a recursive argument. Recall that a continuous $1$-dimensional function $f: \\mathcal{D} \\rightarrow \\mathbb{R}$ on a closed interval $\\mathcal{D} \\subset \\mathbb{R}$ necessarily achieves a min/max either on the endpoints of $\\mathcal{D}$ or somewhere inside. If we additionally stipulate that $f$ is linear, the only possibilities are the endpoints. Extending this logic to linear programs in $n$-dimensions which have finite optimal solutions, we conclude that the optimal solution cannot occur at any interior point of the constraint polytope $\\mathcal{P}$ and, instead, must occur somewhere on its boundary. But now we can consider the $(n-1)$-dimensional polytopes forming $\\mathcal{P}$'s boundary separately and apply the same logic to each one recursively. In the base case, we reach the conclusion that the optimal solution must occur at an endpoint of a $1$-dimensional polytope — a line segment such as $\\mathcal{D}$. Such a point is an extreme point of the constraint polytope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f308f",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Linear programs are a sub-class of convex optimization problems for which the search space can be reduced to a finite set of basic feasible solutions or extreme points. Furthermore, all LP's can be written in a standard form which lends itself to being solved using a number of fast, iterative algorithms. So, although writing an optimization problem in LP form can be a difficult task, the payoff is worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98902f2a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Algorithms for Unconstrained Optimization\"\n",
    "author: \"Vahram Poghosyan\"\n",
    "date: \"2022-01-23\"\n",
    "categories: [\"Optimization\"]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "  text: |\n",
    "    <script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8a35d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "***Gradient Descent (GD)*** is a powerful, yet incredibly simple, iterative optimization algorithm. We can think of it as a ***greedy algorithm*** in the setting of continuous optimization. That is, one step of GD is our best attempt at local optimization given limited information about the objective $f(x)$, and having limited computational power at our disposal.\n",
    "\n",
    "We can further qualify what we mean by *'limited information'* about the objective by introducing a categorization on optimization algorithms – the ***Oracle Access Model (OAM)***. In this model, the objective is abstracted into a black box. For each input $x$, the black box gives the algorithm access to the value of the objective, $f(x)$, and, optionally, global information in the form of higher order behavior such as $\\nabla f(x)$, $\\nabla^2 f(x)$, etc. GD is what's known as a ***first-order oracle*** because it's only allowed access to $f(x)$ and first-order information in the form of $\\nabla f(x)$. \n",
    "\n",
    "It's important to note that the OAM is not all-inclusive, there are a number of optimization algorithms, such as ***composite optimization*** algorithms, that utilize structural information about the objective that goes beyond $n$-th order behavior. An example of such an algorithm is ***Proximal Gradient Descent (PGD)*** which, in addition to $f(x)$ and $\\nabla f(x)$, also has access to the ***prox operator***: $Prox_{h,\\eta}(x)$.\n",
    "\n",
    "We will cover these composite optimization algorithms in later posts. Many of the composite optimization algorithms, such as PGD, are simple modifications of vanilla GD. The modification done in PGD, for example, make it suitable for constrained optimization.\n",
    "For now, however, we focus on the case of unconstrained optimization with oracles, particularly on Gradient Descent, in order to develop the key algorithmic intuition.\n",
    "\n",
    "# The Gradient Descent Algorithm\n",
    "\n",
    "In this section, we will explore two ideas that give rise to the GD algorithm. As all iterative algorithms, gradient descent relies on an ***initialization step*** and an ***update step***.\n",
    "\n",
    "## Idea 1 - Greedy Choice of Direction\n",
    "\n",
    "Let $x$ be the initial iterate, and let the update be given by:\n",
    "\n",
    "$$x^+ = x + \\eta d$$ \n",
    "\n",
    "for some directional unit-vector $d$ and ***step-size*** parameter $\\eta > 0$.\n",
    "\n",
    "We base the algorithm on the assumption that the linear approximation of the objective at a the next iterate $x^+$ is a good-enough estimate of its true value at $x^+$. \n",
    "\n",
    "That is:\n",
    "\n",
    "$$f(x^+) = f(x + \\eta d) \\approx f(x) + \\eta \\nabla f(x)^T d \\ \\ \\forall d \\tag{1.1}$$\n",
    "\n",
    "\n",
    "Immediately, a locally optimal choice presents itself to us. Since we wish to minimize $f(x)$, it would be wise to insist that the objective at $x^+$ improves or, at least, does not worsen. \n",
    "\n",
    "That is, we insist: \n",
    "\n",
    "$$f(x^+) \\approx f(x) + \\eta \\nabla f(x)^T d \\leq f(x) \\tag{1.3}$$\n",
    "\n",
    "And, since we are greedy in our approach, we wish to make $f(x^+)$ as small as possible. Since, on the RHS, $f(x)$ is fixed and $\\eta > 0$, this amounts to minimizing the scaled inner-product $\\nabla f(x)^Td$. To that end, we choose $d$ opposite and parallel to the gradient, i.e. $d = - \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}$.\n",
    "\n",
    "The update step becomes:\n",
    "\n",
    "$$x^+ = x - \\eta \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}$$\n",
    "\n",
    "By re-labeling, $\\eta$ can absorb the normalization constant. This obtains the gradient descent update step as it's often introduced in the textbooks – a step in the negative gradient direction: \n",
    "\n",
    "$$x^+ = x - \\eta \\nabla f(x) \\tag{1.4}$$\n",
    "\n",
    "This makes intuitive sense because the negative gradient direction is the direction in which the objective decreases most. So, it's only natural that the update should take us in this most enticing direction.\n",
    "\n",
    "## Idea 2 - Greedy Choice of Next Iterate\n",
    "\n",
    "Instead of defining the update step $x^+ = x + \\eta d$ and then choosing the locally optimal direction $d$ greedily, we can choose the update step and the direction, both, in one fell swoop.\n",
    "\n",
    "Starting from the linear approximation:\n",
    "\n",
    "$$\n",
    "f(y) \\approx f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y \\tag{2.1}\n",
    "$$\n",
    "\n",
    "We can now insist, in a greedy fashion, that the next iterate $x^+$ be the minimizer of the linear approximation. That is, we insist:\n",
    "\n",
    "\n",
    "$$\n",
    "x^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) \\tag{2.2}\n",
    "$$\n",
    "\n",
    "But the linear approximation is only local, so it would be wise to distrust it for points far away from the current iterate. In this case, since the linear approximation is, in fact, unbounded below, $(2.2)$ would obtain $x^+ = \\pm \\infty$. To avoid this problem, we introduce a parametrized penalty term that prevents $x^+$ from venturing too far from the current iterate $x$. That is:\n",
    "\n",
    "$$\n",
    "x^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) + \\eta ||y - x||_2^2 \\tag{2.3}\n",
    "$$\n",
    "\n",
    "Now, since the RHS is a a simple quadratic in $y$, it has a unique minimizer which can be found by using the unconstrained optimality condition. This just boils down to taking the gradient of the RHS w.r.t. the optimization variable $y$, setting it to zero, and then solving for the unique root. This procedure obtains: \n",
    "\n",
    "$$x^+ = x - \\frac{1}{2 \\eta} \\nabla f(x)$$\n",
    "\n",
    "By re-labeling, we, once again, get the canonical form of the GD update step as in $(1.3)$ – a step in the negative gradient direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bb6f2",
   "metadata": {},
   "source": [
    "# Important Questions in Analysis\n",
    "\n",
    "Given the ease with which we came up with the algorithm, we should ask ourselves the following questions:\n",
    "\n",
    "1. Is GD sensitive to initialization?\n",
    "2. Is GD guaranteed to converge for all step-sizes?\n",
    "3. How should we choose a step-size that guarantees convergence? \n",
    "4. What's the rate of convergence of GD? Does the rate depend on step-size? Does it depend on properties of the objective function?\n",
    "5. How should we choose a step-size that maximizes convergence rate?\n",
    "\n",
    "We will shortly explore each of these questions and more. However, before doing so, it's worth taking a bird's eye look at the problem of convex optimization itself. Perhaps the most important question to ask ourselves is this: does gradient descent's convergence rate, for an optimally chosen step-size, give a taxonomy of easier-to-harder problems within the field of convex optimization? The answer, as it turns out, is *yes*.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "From this point on, we will limit our discussion to convex objectives in order to eliminate the possibility of strictly ***local optimizers*** (i.e. ***non-global optimizers***) and ***inflection points***, both of which GD gets stuck at if initialized poorly. This ensures the only ***stationary points***, points at which $\\nabla f(x) = 0$ and the GD update makes no further progress, are global minimizers. On such convex functions, as we will soon discover, GD has a convergence guarantee for all step-sizes independently of initialization.\n",
    "\n",
    "## Fixed Step-Size GD\n",
    "\n",
    "To kickstart our analysis of GD, we consider the fixed step-size algorithm first. Let's take two quintessential convex problems in $\\mathbb{R}$, $f(x) = x^2$ and $h(x) = |x|$, and analyze GD's performance on them.\n",
    "\n",
    "### Simple Analysis of Fixed Step-Size GD\n",
    "\n",
    "---\n",
    "\n",
    "First, let's run the algorithm on $h(x) = |x|$ for $x \\in \\mathbb{R}$:\n",
    "\n",
    "Since $|x|$ is non-differentiable at $x = 0$, the gradient has a discontinuity at $x = 0$. Non-differentiability, such as this, will eventually force us to introduce the notion of ***sub-gradients***, but for now we can get away with it simply by avoiding the gradient's behavior at $0$. So:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2ba8b",
   "metadata": {},
   "source": [
    "$$\n",
    "h'(x) = \n",
    "\\begin{cases} \n",
    "\\begin{aligned} \n",
    "-1 \\ &\\textrm{if $x < 0$} \\\\ \n",
    "1 \\ &\\textrm{if $x > 0$} \n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a87926",
   "metadata": {},
   "source": [
    "Then, for a fixed $\\eta > 0$, the update step is:\n",
    "\n",
    "$$x^+ = x \\pm \\eta$$\n",
    "\n",
    "where the sign of $\\eta$ depends on where the previous iterate, $x$, falls within the domain $(-\\infty, 0) \\cup (0, \\infty)$.\n",
    "\n",
    "---\n",
    "\n",
    "Now, consider $f(x) = x^2$ for $x \\in \\mathbb{R}$:\n",
    "\n",
    "\n",
    "The gradient of $f(x) = x^2$ is $f'(x) = 2x$, which means the fixed step-size update is:\n",
    "\n",
    "\n",
    "$$x^+ = x - 2\\eta x$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Note that $x^* = 0$ is the unique optimizer of both $f(x)$ and $h(x)$. With this in mind, there are two key observations to make. \n",
    "\n",
    "The first is that, for $x$ far away from $x^* = 0$, the update, $2\\eta x$, is large (in magnitude). So, if the iterate is far from the optimizer, GD makes fast progress towards it. \n",
    "\n",
    "The second observation is that, as $x \\rightarrow x^*$, the update becomes small in magnitude. So, as the iterate comes close to the optimizer, GD takes smaller and smaller steps which converge to $0$ in a summable way. This means, we can get the sub-optimality $f(x) - f(x^*)$ to be $\\epsilon$-arbitrarily small for *any* fixed step-size $\\eta$.\n",
    "\n",
    "Neither of these observations hold for GD on $h(x) = |x|$ since the update $\\eta$ is fixed regardless of the Euclidean distance between $x$ and $x^* = 0$. In particular, this means GD is *not* fast for $x$ far away from $x^*$ and *does not* slow down as $x$ nears $x^*$. Arbitrary accuracy is impossible with a fixed step-size, since the iterates eventually cycle between $x^T - \\eta$ and $x^T + \\eta$ where $x^T$ is the last unique iterate – that is $x^T \\in (-\\eta, \\eta)$. The sub-optimality, consequently, also cycles between two values which depend on the choice of $\\eta$. This is to say that the sub-optimality cannot be $\\epsilon$-arbitrary small for a fixed choice of $\\eta$. To be clear, there is still convergence but it's slow and not arbitrarily accurate. Arbitrary accuracy for such problems as this can only be achieved by choosing a sequence of diminishing step-sizes $\\{ \\eta_t \\}_{t=1}^T$ which reduce magnitude of the update since the gradient, itself, is constant. Of course, this sequence must be chosen with care since it's possible to *'run out of steam'*, so to speak, before reaching the optimizer. The precise criterion is $\\eta_t \\rightarrow 0$ as $t \\rightarrow \\infty$ s.t. $\\sum_t^\\infty \\eta_t = \\infty$.\n",
    "\n",
    "We say GD on $f(x) = x^2$ enjoys the ***self-tuning property***, whereas GD on $h(x) = |x|$ does not. This speaks to the fact that the self-tuning is a property of the objective functions, rather than GD itself. \n",
    "\n",
    "As an overview of the theory we will soon develop, functions *like* $x^2$ (or, more generally, any quadratic in $\\mathbb{R}^n$) will all have the self-tuning property while functions *like* $|x|$ will not. This is what ends up introducing the taxonomy of easier-to-harder convex optimization problems mentioned in the previous section. What it means, precisely, to be *like* $x^2$ or $|x|$ will be made rigorous in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b890dd",
   "metadata": {},
   "source": [
    "# Smoothness and Strong Convexity\n",
    "\n",
    "As we saw above, gradient descent with a fixed step-size behaved much better on $f(x) = x^2$ than on $h(x) = |x|$. Since both of these problems are convex, $x^2$ must have additional properties not shared by $|x|$ that make it more amenable to optimization by GD. These properties turn out to be ***smoothness*** and ***strong convexity***. We will see that these properties provide insight into choosing the best fixed-step size which guarantees faster convergence of GD.\n",
    "\n",
    "We start by asking ourselves what makes the two quintessential functions $f(x) = x^2$ and $h(x) = |x|$ different from one another. Since the GD update step relies on the gradient, it helps thinking in terms of the differences of the gradients instead of the objective functions themselves.\n",
    "\n",
    "The first difference of note is that $|x|$ has a discontinuity at $x = 0$ that's not present in $x^2$. At a point of discontinuity the gradient experiences an abrupt jump. So, in general, jumps in the gradient must pose a problem for GD. \n",
    "\n",
    "The second difference of note is that $|x|$ is flat compared to $x^2$. In flat regions, the gradient is constant. So, in general, constant regions in the gradient must pose a problem for GD. \n",
    "\n",
    "Both of these scenarios can be ruled out with a [***Lipschitz condition***](https://en.wikipedia.org/wiki/Lipschitz_continuity) on the gradient. Lipschitz conditions are both regularity conditions, as well as, growth conditions. So, they can rule out abrupt jumps and ensure there's, at least, some change in the gradient.\n",
    "\n",
    "We are ready to define the two properties mentioned in the beginning of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bba31",
   "metadata": {},
   "source": [
    "> **Smoothness:** &nbsp; We say a function $f(x)$ is ***M-smooth*** if its gradient is ***M-Lipschitz***. That is, if:\n",
    ">\n",
    "> $$\\exists M > 0 \\ \\ s.t. \\ \\ ||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f150740",
   "metadata": {},
   "source": [
    "This is a universal upper-bound on the change in gradient which rules out jumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4896d",
   "metadata": {},
   "source": [
    "> **Strong Convexity:** &nbsp; We say a function $f(x)$ is ***m-strongly-convex*** if:\n",
    ">\n",
    "> $$\\exists m > 0 \\ \\ s.t. \\ \\ ||\\nabla f(x) - \\nabla f(y)||_2 \\geq m||x-y||_2 \\ \\ \\forall x,y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da739b",
   "metadata": {},
   "source": [
    "This is a universal lower-bound on the change in gradient which rules out the possibility of a constant gradient.\n",
    "\n",
    "In particular, an $M$-smooth, and $m$-strongly-convex function $f(x)$ satisfies:\n",
    "\n",
    "$$m||x-y||_2 \\leq ||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y \\tag{3.1}$$\n",
    "\n",
    "For a twice-differentiable function, there's a more compact way to express these properties using the hessian which makes use of an ordering on matrices introduced by matrix [***definiteness***](https://en.wikipedia.org/wiki/Definite_matrix). After all, Lipschitzness of the gradient, which is nothing but a restriction on its growth, is a statement about the hessian.\n",
    "\n",
    "$$||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y$$\n",
    "$$\\iff$$\n",
    "$$||\\nabla^2 f(x)||_2 \\leq M \\ \\ \\forall x$$\n",
    "$$\\iff$$\n",
    "$$\\nabla^2 f(x) \\preceq  MI \\ \\ \\forall x \\tag{4.1}$$\n",
    "\n",
    "The first equivalence is by the [***Mean Value Theorem***](https://en.wikipedia.org/wiki/Mean_value_theorem) and the second follows from the definition of ***matrix norm***.\n",
    "\n",
    "Line $(3.1)$ should be read as *'the maximum eigenvalue of the hessian $\\nabla^2 f(x)$ is $M$'*. \n",
    "\n",
    "By a symmetric argument, we also have: \n",
    "\n",
    "$$\\nabla^2 f(x) \\succeq mI \\ \\ \\forall x \\tag{4.2}$$\n",
    "\n",
    "Which should be read as *'the minimum eigenvalue of the hessian $\\nabla^2 f(x)$ is $m$'*.\n",
    "\n",
    "Together, $(4.1)$ and $(4.2)$ give the analog of $(3.1)$ for twice-differentiable $M$-smooth and $m$-strongly-convex functions: \n",
    "\n",
    "$$mI \\preceq \\nabla^2 f(x) \\preceq MI \\ \\ \\forall x \\tag{3.2}$$\n",
    "\n",
    "Since the hessian represents the curvature of the function, $(3.2)$ is a two-sided bound on the curvature of $f(x)$. So, we see that smoothness and strong convexity also regulate function shape itself. The lower-bound rules out flatness, while the upper-bound rules out discontinuities like corners and cusps.\n",
    "\n",
    "## Self-Tuning Property\n",
    "\n",
    "For a convex function that's $M$-smooth and $m$-strongly-convex we have $(3.1)$ which, as a reminder, is:\n",
    "\n",
    "$$m||x-y||_2 \\leq ||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y$$\n",
    "\n",
    "Fixing iterate $x$, and replacing $y$ with the optimizer $x^*$ we have: \n",
    "\n",
    "$$m||x-x^*||_2 \\leq ||\\nabla f(x) - \\nabla f(x^*)||_2 \\leq M||x-x^*||_2$$\n",
    "\n",
    "Since $x^*$ is an optimizer $\\nabla f(x^*) = 0$, so the above becomes: \n",
    "\n",
    "$$m||x-x^*||_2 \\leq ||\\nabla f(x)||_2 \\leq M||x-x^*||_2$$\n",
    "\n",
    "The first inequality says that the magnitude of the gradient is *at least* a constant multiple of the distance from the optimizer. The second inequality says that the magnitude of the gradient is *at most* a constant multiple of the distance from the optimizer. So, the gradient is large for $x$ far from $x^*$ and gets smaller as $x \\rightarrow x^*$. Since the GD update depends on the magnitude of the gradient, this ensures GD has the self-tuning property. So, smoothness and strong-convexity were, indeed, the ideas needed to encapsulate the self-tuning property.\n",
    "\n",
    "## Quadratic Bounds\n",
    "\n",
    "Smoothness and strong-convexity, should they hold for a given convex function, give a universal quadratic point-wise upper and lower-bound, respectively, on the function. This is what it means to say that the function is *like* a quadratic. In a sense, all we're saying is that a function is *tightly* asymptotically bounded by a quadratic at every point. That is, at any given point, the function should neither grow slower nor faster than quadratically.\n",
    "\n",
    "To construct the upper and lower-bounds, we use the following two lemmas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc93da",
   "metadata": {},
   "source": [
    "> **Lemma 1:** &nbsp; If $f$ is convex and $L$-Lipschitz then $g(x) = \\frac{L}{2}x^Tx - f(x)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20572a",
   "metadata": {},
   "source": [
    "> **Lemma 2:** &nbsp; If $f$ is $m$-strongly-convex then $g(x) = f(x) - \\frac{m}{2}x^Tx$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e3eb6",
   "metadata": {},
   "source": [
    "Both lemmas are statements of comparative convexity in disguise. ***Lemma 1*** says that $\\frac{L}{2}x^Tx$ is more convex than $f$, whereas ***Lemma 2*** says that $f$ is more convex than $\\frac{m}{2}x^Tx$. \n",
    "\n",
    "It's not difficult to see how these lemmas can assist in sandwiching $f$ between an upper-bound that's more convex and a lower-bound that's less convex.\n",
    "\n",
    "The bounds themselves come from the quadratic approximation of $f$ as:\n",
    "\n",
    "$$f(y) \\approx f(x) + \\nabla f(x)^T(y-x) + \\frac{1}{2}(y-x)^T\\nabla^2 f(x)(y-x)\\ \\ \\forall y$$\n",
    "\n",
    "By replacing the hessian with its bounds $mI$ and $MI$ and using the above lemmas we obtain the two bounds as:\n",
    "\n",
    "$$f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{m}{2}||y-x||_2^2 \\ \\ \\forall y$$ \n",
    "$$\\textrm{and} \\tag {5.1}$$\n",
    "$$f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2 \\ \\ \\forall y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abed22",
   "metadata": {},
   "source": [
    "## The Optimal Fixed Step-Size\n",
    "\n",
    "We already showed that an $M$-smooth and $m$-strongly-convex function enjoys the advantage of self-tuning. But, without knowing how to choose the step-size, we can still cause GD to make slow progress or even diverge. \n",
    "\n",
    "After all, gradient descent is based on a local linear approximation of the objective. If we take big steps, we are counting on the linear approximation to be a good-enough estimate far from the current iterate. This may be too optimistic, in which case GD will diverge. Being too pessimistic, however, is also not good. While taking small steps won't cause GD to diverge, it will make GD painfully slow... Slow to the point of making it worthless in practice.\n",
    "\n",
    "Luckily, the quadratic upper-bound in $(5.1)$ informs our choice of step-size both in terms of a convergence guarantee and in terms of optimality. First, let's develop the convergence guarantee. \n",
    "\n",
    "By plugging the update step $x^+ = x - \\eta \\nabla f(x)$ as $y$ into the upper-bound in $(5.1)$ we obtain:\n",
    "\n",
    "$$f(x^+) \\leq f(x) + \\eta(1-\\frac{M}{2}\\eta)||\\nabla f(x)||_2^2 \\tag{5.2}$$\n",
    "\n",
    "As before, it would be wise to insist $f(x^+) \\leq f(x)$, which gives the convergence interval as $0 < \\eta < \\frac{2}{M}$. \n",
    "\n",
    "Here, it helps to consider the simple example of quadratics.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "Consider the quadratic form in $\\mathbb{R}$ given by $f(x) = \\frac{1}{2}M x^2$ where $x, M \\in \\mathbb{R}$. Here we can think of $M$ as the only, and therefore the largest, eigenvalue of the $1 \\times 1$ matrix $[M]$.\n",
    "\n",
    "Its GD update step looks like: \n",
    "\n",
    "$$x^+ = x - \\eta M x = (1 - \\eta M)x$$\n",
    "\n",
    "Which, by recursion from iteration $T$ down to the initial iteration, gives: \n",
    "\n",
    "$$x^T = (1- \\eta M)^Tx_0$$\n",
    "\n",
    "Then, since $x^* = 0$, convergence is guaranteed by ensuring $|1 - \\eta M| < 1$ or, equivalently, $0 < \\eta < \\frac{2}{M}$ as desired.\n",
    "\n",
    "This generalizes to higher dimensions as we shall see in the following example.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "Consider the quadratic form in $\\mathbb{R}^n$ given by $f(x) = \\frac{1}{2}x^TQx$ where $x \\in \\mathbb{R}^n$ and $Q \\succeq 0$.\n",
    "\n",
    "Its GD update step looks like: \n",
    "\n",
    "\n",
    "$$x^+ = x - \\eta Qx = (I - \\eta Q)x$$ \n",
    "\n",
    "Which, by recursion from iteration $T$ down to the initial iteration, gives: \n",
    "\n",
    "$$x^T = \\underbrace{(I - \\eta Q)\\ldots(I - \\eta Q)}_{\\text{$T$ times}}x_0$$\n",
    "\n",
    "\n",
    "The eigenvalues $\\tilde \\lambda_i$ of the matrix $I - \\eta Q$ are related to the eigenvalues $\\lambda_i$ of $Q$ according to:\n",
    "\n",
    "$$\\tilde \\lambda_i = 1 - \\eta \\lambda_i$$\n",
    "\n",
    "Hence, if $\\lambda_{min} = m$ and $\\lambda_{max} = M$, then $\\tilde \\lambda_{min} = 1 - \\eta M$ and $\\tilde \\lambda_{max} =1 - \\eta m$.\n",
    "\n",
    "The eigenvalues of $I - \\eta Q$ act on the current iterate by scaling it. So, in order to ensure convergence to $0$, we need $\\tilde \\lambda_i \\in (-1,1) \\ \\ \\forall i$. Or, equivalently:\n",
    "\n",
    "$$\\tilde \\lambda_{min} > -1$$\n",
    "$$\\textrm{and}$$\n",
    "$$\\tilde \\lambda_{max} < 1$$\n",
    "\n",
    "Both of these together give us $0 < \\eta < \\frac{2}{M}$ as desired.\n",
    "\n",
    "---\n",
    "\n",
    "But $0 < \\eta < \\frac{2}{M}$ is an interval, not a greedy choice. It's just a condition that guarantees convergence. By making the greedy choice we can find the optimal step-size within this interval.\n",
    "\n",
    "Since the RHS of $(5.2)$ is strongly convex, the quadratic upper-bound is guaranteed to have a unique minimizer. \n",
    "\n",
    "The idea is similar to other instances of making a greedy choice we've seen so far. Since the function value is upper-bounded by this quadratic, minimizing this upper-bound gives the best guarantee of smallness for the function value available to us without access to higher order information about the objective. So, the greedy choice for the next iterate is:\n",
    "\n",
    "$$\n",
    "x^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2\n",
    "$$\n",
    "\n",
    "As always, minimizing a quadratic is easy. After going through the motions we obtain:\n",
    "\n",
    "$$x^+ = x - \\frac{1}{M}\\nabla f(x)$$\n",
    "\n",
    "So, the optimal fixed step-size is $\\eta = \\frac{1}{M}$.\n",
    "\n",
    "We will see this idea of minimizing a quadratic approximation of the objective, instead of the objective itself, repeat itself when we get to ***Newton's Method (NM)***. However, NM is a second-oder oracle which has access to $\\nabla^2 f(x)$, and hence the quadratic approximation at all points in the domain of the objective is accessible to NM. The remarkable thing about $M$-smoothness and $m$-strong-convexity is that they give gradient descent, a first-order oracle, access to *universal* quadratic bounds which eliminates the need to know $\\nabla^2 f(x)$. These upper-bounds are, as we saw, what inform GD's choice of step-size which ends up guaranteeing convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1c348",
   "metadata": {},
   "source": [
    "## Convergence Rate\n",
    "\n",
    "As shown above, the theoretical best fixed step-size for an $M$-smooth objective $f(x)$ is $\\eta = \\frac{1}{M}$. With this choice of step-size, we can derive convergence guarantees for GD as well as its convergence rate. \n",
    "\n",
    "### $M$-Smooth Objectives\n",
    "\n",
    "Fixing the current iterate as $x$, and plugging in $x^+ = x - \\frac{1}{M} \\nabla f(x)$ into the upper-bound in $(5.1)$, we obtain the quadratic upper-bound on the next iterate in terms of the magnitude of the gradient: \n",
    "\n",
    "\n",
    "$$f(x^+) \\leq f(x) - \\frac{1}{2M}||\\nabla f(x)||_2^2 \\tag{5.3}$$\n",
    "\n",
    "Furthermore, since the underlying assumption throughout this post is that the objective is convex, we have a linear lower-bound $\\forall y$, and particularly at the optimizer $y = x^*$, as:\n",
    "\n",
    "$$f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) \\tag{5.4}$$ \n",
    "\n",
    "By reversing $(5.4)$ and adding it to $(5.3)$ we get:\n",
    "\n",
    "$$f(x^+) \\leq f(x^*) + \\nabla f(x)^T(x - x^*) - \\frac{1}{2M}||\\nabla f(x)||_2^2$$\n",
    "\n",
    "With a bit of algebraic finessing, we can bring the above to the form:\n",
    "\n",
    "$$f(x^+) \\leq f(x^*) + \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x - \\frac{1}{M}\\nabla f(x) - x^*||_2^2\\right]$$\n",
    "\n",
    "But $x - \\frac{1}{M}\\nabla f(x) = x^+$, so we have:\n",
    "\n",
    "$$f(x^+) - f(x^*) \\leq \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x^+ - x^*||_2^2\\right]$$\n",
    "\n",
    "We recognize, $||x^+ - x^*||_2^2$ as the sub-optimality of the next iterate, and $||x - x^*||_2^2$ as the sub-optimality of the previous iterate. When both sides are summed over $T$ iterations, the RHS sum telescopes and we're left with:\n",
    "\n",
    "$$\\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2}||x_0 - x^*||_2^2$$\n",
    "\n",
    "The LHS is the sum of sub-optimalities across all $T$ iterations, and the RHS is a quantity that's proportional to the initial condition. Taking the average error across all iterations, we get:\n",
    "\n",
    "$$\\frac{1}{T} \\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2$$\n",
    "\n",
    "But, we know that the algorithm with fixed step-size $\\eta = \\frac{1}{M}$ has the descent property since $0 < \\frac{1}{M} < \\frac{2}{M}$. So, the last sub-optimality $f(x_{T}) - f(x^*)$ must be the smallest. In particular, it must be smaller than the average. So, we have: \n",
    "\n",
    "$$f(x_{T}) - f(x^*) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2 \\tag{5.5}$$\n",
    "\n",
    "So, the error gets better with more iterations and, conversely, gets worse the larger $M$, a measure of how abruptly the gradient changes anywhere on its domain, is. \n",
    "\n",
    "Note that $M$, as well as $||x_0 - x^*||_2^2$ are fixed in $(5.5)$. So, the convergence rate of GD on an $M$-smooth objective is $O(\\frac{1}{T})$.\n",
    "\n",
    "### $M$-Smooth and $m$-Strongly-Convex Objectives\n",
    "\n",
    "The situation is drastically better if, in addition to $M$-smoothness, we also have $m$-strong-convexity. Not only do we get a much faster convergence rate, we also guarantee convergence in the iterates themselves. Note that, so far, we've discussed sub-optimality in objective values only. That is, the only convergence guarantee we've seen so far is $f(x^T) \\rightarrow f(x^*)$ as $T \\rightarrow \\infty$. Sometimes more is needed, we may actually want convergence of the iterates themselves. That is, we may want $x^T \\rightarrow x^*$ as $T \\rightarrow \\infty$? Since strong convexity guarantees the existence of a unique optimizer $x^*$, we can discuss this type of sub-optimality for $m$-strongly-convex objectives.\n",
    "\n",
    "In this scenario, we have the analog of $(5.3)$ for the quadratic lower-bound. \n",
    "\n",
    "Just as $x - \\frac{1}{M} \\nabla f(x)$ minimized the quadratic upper-bound, $x - \\frac{1}{m} \\nabla f(x)$ minimizes the quadratic lower-bound. Plugging it into the lower-bound, we get $(5.3)$'s analog as:\n",
    "\n",
    "\n",
    "$$f(y) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\ \\ \\forall y \\tag{5.6}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baf8bb",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\n",
    "This result is stronger than its analog $(5.3)$ because it holds $\\forall y$ as opposed to $(5.3)$ which is only guaranteed to hold at the minimizer $x^+ = x - \\frac{1}{M}\\nabla f(x)$. This is expected because $(5.6)$ is the result of minimizing a universal lower-bound as opposed to $(5.3)$ which is the result of minimizing a universal upper-bound.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2832ddc",
   "metadata": {},
   "source": [
    "Since $(5.6)$ holds $\\forall y$, it holds, in particular, at the optimizer $y=x^*$:\n",
    "\n",
    "$$f(x^*) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\tag{5.7}$$\n",
    "\n",
    "We can now solve for $||\\nabla f(x)||_2^2$ in $(5.7)$ and plug the result into $(5.3)$. From $(5.7)$, we get:\n",
    "\n",
    "$$||\\nabla f(x)||_2^2 \\geq 2m(f(x)-f(x^*))$$\n",
    "\n",
    "Which, when plugged into $(5.3)$, gives:\n",
    "\n",
    "$$f(x^+) - f(x^*) \\leq \\left(1-\\frac{m}{M}\\right)(f(x)-f(x^*))$$\n",
    "\n",
    "We recognize the LHS as the sub-optimality at the next iteration, and the RHS as the sub-optimality at the current iteration. Recursion from iteration $T$ down to the initial iteration, gives: \n",
    "\n",
    "$$f(x_T) - f(x^*) \\leq  \\left(1-\\frac{m}{M}\\right)^T (f(x_0) - f(x^*)) \\tag{5.8}$$\n",
    "\n",
    "And, since $m \\leq M$ and both strictly positive, $0 < \\frac{m}{M} \\leq 1$ which guarantees convergence. \n",
    "\n",
    "Note that $m$, $M$, and the initial sub-optimality $f(x_0) - f(x^*)$ are fixed quantities in $(5.8)$. So, the convergence rate of GD on a smooth and strongly-convex objective is $O(c^{-T})$ for the constant $c^{-1} = 1 - \\frac{m}{M}$. That is, the error decreases *exponentially* in the number of iterations. However, historically, mathematicians were concerned with the logarithm of the error, rather than the error itself, and hence this type of convergence is known as ***linear convergence***.\n",
    "\n",
    "As promised, we also have convergence of the iterates themselves. From the quadratic lower-bound in $(5.1)$ we can derive an upper-bound on $||x - x^*||_2$ as follows. As before, letting $y = x^*$ in the quadratic lower-bound gives us:\n",
    "\n",
    "$$f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) + \\frac{m}{2}||x^* - x||_2^2$$\n",
    "\n",
    "But, by the [***Cauchy-Schwarz Inequality***](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), we further have: \n",
    "\n",
    "$$f(x^*) \\geq f(x) - ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2$$\n",
    "\n",
    "But, since $f(x^*) \\leq f(x)$ by the optimality of $x^*$, we must have:\n",
    "\n",
    "$$- ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2 \\leq 0$$\n",
    "\n",
    "From which it follows that:\n",
    "\n",
    "$$||x - x^*||_2 \\leq \\frac{2}{m}||\\nabla f(x)||_2 \\tag{5.9}$$\n",
    "\n",
    "As GD converges to $f(x^*)$, $\\nabla f(x) \\rightarrow \\nabla f(x^*) = 0$ where the last equality is by optimality of $x^*$. So, $x \\rightarrow x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16ffc5",
   "metadata": {},
   "source": [
    "### Affine Invariance\n",
    "\n",
    "Not only does $(5.8)$ give the rate of convergence of GD, it also predicts its performance on objectives with roughly spherical vs roughly elliptical level-sets. These are the level-sets of what's referred to as ***badly*** and ***well conditioned*** objectives respectively. \n",
    "\n",
    "To say that an objective is $M$-smooth and $m$-strongly convex is to say $(3.2)$ which, as a reminder, is: \n",
    "\n",
    "$$mI \\preceq \\nabla^2 f(x) \\preceq MI \\ \\ \\forall x$$\n",
    "\n",
    "This implies that all the eigenvalues of the hessian are bounded between $m$ and $M$. The eigenvalues of the hessian represent the stretch of the level sets in the principal directions. So, to say that $\\frac{m}{M} \\approx 1$ is to say that $m \\approx M$, which means that the level-sets are not more or less stretched in any particular direction. This implies that the level-sets are roughly spherical. As we can see from $(5.8)$, GD converges quite fast in such cases since $\\left( 1 - \\frac{m}{M} \\right)$, the factor of decrease, is small. The opposite is true in the case when the level sets are elongated.\n",
    "\n",
    "This brings us to an important property called ***affine invariance*** which GD lacks. Simply put, an affine transformation of the input space (i.e. a mere change of coordinates/basis) may affect GD's performance drastically.\n",
    "\n",
    "It helps to look at an example where the objective is a simple quadratic in $\\mathbb{R}^n$.\n",
    "\n",
    "---\n",
    "\n",
    "Take the quadratic objective $f(x) = \\frac{1}{2}x^TQx$ where $x \\in \\mathbb{R}^n$ is a coordinate vector in the standard basis. Now, consider the change of coordinates from the standard basis to a basis $Z$ given by $Az = x$ where $A$ is a matrix whose column vectors are the basis vectors in $Z$. \n",
    "\n",
    "The key observation is that we can choose $A$ in such a way as to make the objective in the new coordinate system have more or less elliptical level-sets which would affect GD's performance.\n",
    "\n",
    "First, let's come up with the same quadratic in $Z$-coordinates. \n",
    "\n",
    "$$f(x) = f(Az) = \\frac{1}{2}(Az)^TQAz = z^TA^TQAz$$\n",
    "\n",
    "Let $\\tilde Q = A^TQA$ and define the quadratic in $Z$-coordinates as $\\tilde f(z) = \\frac{1}{2}z^T \\tilde Q z$ so that $\\tilde f(z) = f(x)$ for all $z$ in the $Z$-coordinates corresponding to $x$ in the standard basis. In particular, optimizing in either coordinate system yields the same result in terms of an optimum, that is $\\tilde f(z^*) = f(x^*)$.\n",
    "\n",
    "The GD factor of decrease on $\\tilde f$ is $\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)$, where $\\tilde m = \\lambda_{\\min}(\\tilde Q)$ and $\\tilde M = \\lambda_{\\max}(\\tilde Q)$. But $\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)$ is under no obligation to be equal to $\\left(1 - \\frac{m}{M} \\right)$, the GD factor of decrease in the standard basis, proving that GD is *not* affine invariant.\n",
    "\n",
    "Another perspective on affine invariance comes from comparing the GD update steps in both spaces.\n",
    "\n",
    "The GD update in the standard basis is: \n",
    "\n",
    "$$x^+ = x - \\eta \\nabla f(x) = x - \\eta Qx$$\n",
    "\n",
    "Whereas the GD update in $Z$-coordinates is:\n",
    "\n",
    "$$z^+ = z - \\eta \\nabla \\tilde f(z) = z - \\eta \\tilde Qz$$\n",
    "\n",
    "To go from $Z$-coordinates back to the standard basis, we apply $A$ to the LHS which necessitates its application to the RHS as well. We obtain: \n",
    "\n",
    "$$Az^+ = Az - \\eta A\\tilde Qz = x - \\eta AA^TQx$$\n",
    "\n",
    "Which is *not* the same as $x - \\eta Qx$. So, although $Az = x$ the linear relationship breaks down for the next iterates produced by GD (that is $Az^+ \\ne x^+$). So, doing a step of GD in the standard basis it's not the same as doing a step of GD in $Z$-coordinates (up to a change of basis by $A$). So, gradient descent is doing something radically different in the $Z$-coordinates compared to what it does in the standard basis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Best Affine Transformation\n",
    "\n",
    "A natural question to ask, at this point, is which choice of $A$ makes GD perform faster on a quadratic objective? \n",
    "\n",
    "Algebraically, the best we can hope for is $A$ s.t. $\\tilde m \\approx \\tilde M$. One way we can accomplish this is by forcing *all* of the eigenvalues of $\\tilde Q$ to be the same. Particularly, letting them all be $1$ by enforcing $\\tilde Q = A^TQA = I$ works. \n",
    "\n",
    "Since $Q \\succeq 0$, it has an eigendecomposition as $Q = PDP^T$ where $D$ is diagonal and $P$ is orthonormal. Then its matrix square root exists and is given by $Q^{-1/2} = PD^{-1/2}P^T$.\n",
    "Letting $A = Q^{-1/2}$ we, indeed, have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TQA &= (PD^{-1/2}P^T)^TPDP^TPD^{-1/2}P^T \\\\ \n",
    "&= PD^{-1/2}DD^{-1/2}P^T \\\\\n",
    "&= PP^T \\\\\n",
    "&= I \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Where we have used the fact that $P^TP = PP^T = I$ since $P$ is orthonormal, and the fact that diagonal matrices are raised to a power simply by raising their diagonal entries to that power.\n",
    "\n",
    "Geometrically, the choice of $\\tilde Q = I$ forces the level-sets to be spherical. A level-set of $f(x) = \\frac{1}{2}x^TQx$ in the standard coordinate system, i.e. an ellipse in $\\mathbb{R}^n$, is given by $x^TQx = c$ for some constant $c$ which has absorbed $\\frac{1}{2}$. In the $Z$-coordinates, the same level-set is given by $\\tilde f(z) = z^T\\tilde Qz = c$. If $\\tilde Q = I$, as is the case for the choice $A = Q^{-1/2}$, then the level-set in the $Z$-coordinates becomes $z^Tz = c$ which is, indeed, a sphere in $\\mathbb{R}^n$.\n",
    "\n",
    "In conclusion, if the objective is quadratic, we can improve GD's convergence rate by applying the above change of basis. If the objective is not quadratic, we may still assume that it's locally quadratic. This allows us to apply the same idea to non-quadratic objectives, but, since it requires coming up with a different matrix $A$ at each iteration, the payoff from this procedure may not be worth it. So, we explore other ways of accelerating the performance of gradient descent on badly conditioned objectives by developing ***Accelerated Gradient Descent (AGD)*** which will be explained shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6080e6",
   "metadata": {},
   "source": [
    "# Variable Step-Size GD\n",
    "\n",
    "While smoothness gives the theoretical best step-size $\\eta = \\frac{1}{M}$, for most problems it can be quite difficult, or impossible, to come up with the smoothness parameter $M$. So, while informative, the discussion we've had so far has been largely theoretical. In practice, there are subroutines such as ***Exact Line Search (ELS)*** or ***Backtracking Line Search (BTLS)*** which choose an appropriate step-size $\\eta_t$ at each iteration. These subroutines can also be modified and used with other optimization algorithms. For example, BTLS is the state of the art way of choosing a step-size in Newton's and ***Quasi-Newton's Methods***.\n",
    "\n",
    "## Exact Line Search (ELS)\n",
    "\n",
    "Let's go back to the general iterative update step $x^+ = x + \\eta d$.\n",
    "\n",
    "By restricting the objective in the direction of the update $d$ we can find the optimal step-size, $\\eta^*$, at each iteration by solving the following one-dimensional, unconstrained optimization problem in $\\eta$:\n",
    "\n",
    "$$\\eta^* = \\arg \\min_{\\eta} f(x + \\eta d) \\tag{6.1}$$\n",
    "\n",
    "We proceed with the optimization by defining the restriction of $f$ in the direction $d$ as $\\phi(\\eta) := f(x + \\eta d)$. Then, we can use the chain-rule to find the stationary point $\\eta^*$ for which $\\nabla \\phi(\\eta^*) = 0$.\n",
    "\n",
    "By chain-rule:\n",
    "\n",
    "$$\\nabla \\phi(\\eta) = \\nabla f(x + \\eta d)^T d \\tag{6.2}$$\n",
    "\n",
    "In the case of GD, $d = -\\nabla f(x)$, so we have: \n",
    "\n",
    "$$\\nabla \\phi(\\eta) = \\nabla f(x - \\eta \\nabla f(x))^T (-\\nabla f(x)) \\tag{6.3}$$\n",
    "\n",
    "An interesting geometric consequence of this is that GD with ELS takes perpendicular steps that end at a point of tangency with a level-set. Setting $\\nabla \\phi(\\eta) = 0$ to find the optimal step-size obtains $\\eta^*$ s.t. $\\nabla f(x - \\eta^* \\nabla f(x))$ is perpendicular to $- \\nabla f(x)$. In other words, GD with ELS goes in the negative gradient direction until the gradient at $x - \\eta^* \\nabla f(x)$ is perpendicular to the gradient at the current iterate $x$. At the next iteration, GD will take a step in the $- \\nabla f(x - \\eta^* \\nabla f(x))$ direction which is still perpendicular to $- \\nabla f(x)$. This means, at each iteration, GD takes a step that's perpendicular to the step it took in the previous iteration. Furthermore, since gradients are always perpendicular to level-sets, the new iterate $x - \\eta^* \\nabla f(x)$ is a point of tangency with the level-set at that point.\n",
    "\n",
    "Although this subroutine is very natural, it may be infeasible to solve an optimization problem (even a one-dimensional one) at each iteration. Hence, we introduce backtracking line search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c69edb",
   "metadata": {},
   "source": [
    "## Backtracking Line Search (BTLS)\n",
    "\n",
    "As we know a convex objective $f$ is always lower-bounded by its linear approximation. That is:\n",
    "\n",
    "$$f(y) \\geq f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y$$\n",
    "\n",
    "Plugging $x^+ = x + \\eta d$ into the above lower-bound obtains:\n",
    "\n",
    "$$f(x^+) \\geq f(x) + \\eta \\nabla f(x)^T d$$\n",
    "\n",
    "So, the greatest possible reduction in value from $f(x)$ to $f(x^+)$ we can hope for is $\\eta \\nabla f(x)^T d$ which, recall, is non-positive by a choice of $d$ that guarantees descent (such as $d = -\\nabla f(x)$ in gradient descent). Unless the objective is linear, in which case the above linear approximation holds with equality, we can not hope to achieve the full $\\eta \\nabla f(x)^T d$ reduction in value. This is just a consequence of convexity and is, therefore, also the case when using exact line search. \n",
    "\n",
    "The idea behind backtracking line search is to ensure we achieve, at least, a fraction of this maximum reduction in value by introducing a parameter $0 < \\alpha < 1$.\n",
    "\n",
    "Since $f(x) + \\eta \\nabla f(x)^T d$, the linear underestimate of $f(x^+)$, is the tangent line to $f$ at $x$ in the direction $d$, $f(x) + \\alpha \\eta \\nabla f(x)^T d$ (for $0 < \\alpha < 1$) is a secant line of $f$ at $x$ in the direction $d$. Setting $\\alpha = \\frac{1}{2}$, a typical choice in practice, and finding the *largest* $\\eta$ s.t. $f(x^+) = (x + \\eta d)$ is still below this secant line ensures we get approximately half of the maximum reduction in value $\\eta \\nabla f(x)^T d$. \n",
    "\n",
    "This is exactly what the BTLS subroutine, detailed below, tries to achieve:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc8b83",
   "metadata": {},
   "source": [
    "### The BTLS Subroutine\n",
    "\n",
    "1. The BTLS subroutine takes as input $0 < \\alpha < 1$, and $0 < \\beta < 1$. \n",
    "\n",
    "2. While $f(x + \\eta d) > f(x) + \\alpha \\eta \\nabla f(x)^T d$, it reduces $\\eta$ by the update $\\eta \\leftarrow$ $\\beta \\eta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f1668",
   "metadata": {},
   "source": [
    "### Convergence Guarantees of GD with BTLS\n",
    "\n",
    "Gradient descent with backtracking line search has a promising convergence guarantee for convex objectives that are also $M$-smooth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7f134",
   "metadata": {},
   "source": [
    "> **Convergence of GD with BTLS:** &nbsp; If $f$ is $M$-smooth and convex then the step-size given by BTLS is s.t.\n",
    "> \n",
    "> $\\eta_{BTLS} \\geq \\frac{\\beta}{M}$. Furthermore, $f(x^+) - f(x) \\leq \\frac{\\alpha \\beta}{M}||\\nabla f(x)||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001b1c3",
   "metadata": {},
   "source": [
    "So, compared to the theoretical best step-size $\\eta = \\frac{1}{M}$, which may not be accessible to us, we have $\\frac{\\beta}{M} \\leq \\eta_{BTLS} < \\frac{1}{M}$. So, $\\beta_{BTLS}$ is less than the optimal step-size but, interestingly, it still keeps $M$ in view despite the latter being unknown to us. \n",
    "\n",
    "Furthermore, for an $M$-smooth objective, the final sub-optimality after $T$ iterations can be found as:\n",
    "\n",
    "$$f(x_T) - f(x^*) \\leq \\frac{M}{2T \\alpha \\beta}||x_0 - x^*||_2^2 \\tag{7.1}$$\n",
    "\n",
    "Which is only a constant factor $\\alpha \\beta$ worse than GD with the theoretical best fixed step-size. So, it's still $O(1/T)$.\n",
    "\n",
    "For an $M$-smooth objective that's also $m$-strongly convex, we have: \n",
    "\n",
    "$$f(x_T) - f(x^*) \\leq \\left (1 - \\min \\left\\{ 2m\\alpha, \\frac{2\\alpha\\beta m}{M} \\right \\} \\right )^T||x_0 - x^*||_2^2 \\tag{7.2}$$\n",
    "\n",
    "Which is, again, comparable to GD with the theoretical best fixed step-size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e77583",
   "metadata": {},
   "source": [
    "# Theoretical Error Bounds on First Order Oracles\n",
    "\n",
    "As we saw the convergence rates of GD were upper-bounded by $O\\left (\\frac{1}{T} \\right )$ and $O\\left (\\left(1 - \\frac{m}{M} \\right)^T \\right)$ for $M$ and $m$ conditioned objectives. We also mentioned that GD can be accelerated, which begs the question: how much can we improve the error? \n",
    "\n",
    "Turns out, there are asymptotic lower-bounds on the error produced by *any* first-order oracle. So, just as the error of GD cannot be worse that its asymptotic upper-bounds, it cannot be better than its asymptotic lower-bounds. \n",
    "\n",
    "First, we define a first order oracle more generally as any iterative algorithm of the form: \n",
    "\n",
    "$$x_{t+1} \\in x_t + span \\{ \\nabla f(x_t), \\nabla f(x_{t-1}), ..., \\nabla f(x_0)\\}$$\n",
    "\n",
    "Note that GD falls in this category as it only uses $\\nabla f(x_t)$ which is, indeed, in the span.\n",
    "\n",
    "For any such algorithm, the lower-bounds on the error are given as the following two-part theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70ef6b",
   "metadata": {},
   "source": [
    "> **First-Order Oracle Error Lower-Bounds:** &nbsp; For any first-order Oracle:\n",
    "> \n",
    "> 1. $\\exists$ a convex, $M$-smooth function $f$ for which the error is $\\Omega \\left (\\frac{1}{T^2} \\right )$\n",
    "> 2. $\\exists$ an $M$-smooth, $m$-strongly-convex function $f$ for which the error is $\\Omega \\left (\\left (1-\\sqrt{\\frac{m}{M}} \\right )^T \\right )$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572cdcf",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"Note\" appearance=\"minimal\" collapse=\"false\"}\n",
    "Since $0 < \\frac{m}{M} \\leq 1$, its square root is larger. Hence, $1 - \\sqrt{\\frac{m}{M}}$ is smaller than $1 - \\frac{m}{M}$. So $\\left (1-\\sqrt{\\frac{m}{M}} \\right )^T$, the lower bound, indeed grows slower than $\\left ( 1 - \\frac{m}{M} \\right )^T$, the upper-bound.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cba70",
   "metadata": {},
   "source": [
    "Any first-order oracle, including Accelerated GD, cannot hope for error that's asymptotically better than these lower-bounds. In fact, we shall see that AGD achieves these lower-bounds exactly, thereby closing the first-order oracle error gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ec185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

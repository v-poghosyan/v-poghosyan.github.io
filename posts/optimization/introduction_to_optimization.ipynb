{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8262545b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Introduction to Optimization\"\n",
    "author: \"Vahram Poghosyan\"\n",
    "date: \"2022-01-23\"\n",
    "categories: [\"Optimization\"]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "  text: |\n",
    "    <script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eae137",
   "metadata": {},
   "source": [
    "# Introduction to Optimization\n",
    "\n",
    "An optimization problem can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e. minimize or maximize) some objective function. The objective function can be almost anything — cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\n",
    "\n",
    "The problems of minimization and maximization, summed up as *optimization* in one word, are basically the same problem. Formally, if the objective function is $f: \\mathbb{R^n} \\to \\mathbb{R}$, and it has a minimizer $x^* \\in \\mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$. It follows that $-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$, so $x^*$ is a maximizer for $-f$, the reflected objective function.\n",
    "\n",
    "## Model of a Convex Optimization Problem\n",
    "\n",
    "This post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n",
    "\n",
    "$$\\textrm{minimize}: f(x)$$\n",
    "$$\\textrm{subject to}: x \\in \\mathcal{X}$$\n",
    "\n",
    "where the *objective function* $f$ is a *convex function*, and the *constraint set* $\\mathcal{X}$ is a *convex set*.\n",
    "\n",
    "We will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in [this post](../optimization/robust-lps-modelling-discrete-failures.ipynb).\n",
    "\n",
    "## Why Convex Optimization?\n",
    "\n",
    "First, let's define the *size* of an optimization problem as: The dimensionality of the parameter $x$, added to the number of the problem constraints.\n",
    "\n",
    "Convex optimization problems are a class of *easy* optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

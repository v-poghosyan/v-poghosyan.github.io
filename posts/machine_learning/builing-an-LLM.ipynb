{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building an LLM\"\n",
    "author: \"Vahram Poghosyan\"\n",
    "date: \"2023-01-13\"\n",
    "categories: [\"Machine Learning\", \"Large Language Models\", \"Deep Learning\"]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 5\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "  text: |\n",
    "    <script type=\"application/javascript\" src=\"../../javascript/light-dark.js\"></script>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "## Step 1: Data Curation\n",
    "\n",
    "GPT-3 was trained on 0.5T tokens, today's leading models are often trained on 5T tokens and above. \n",
    "\n",
    "### Web-Scraping\n",
    "\n",
    "Write a script to download the whole internet. The internet is composed of a lot of Wikipedia articles (which are of particular value, as sources that have been referenced from Wikipedia are often given more weight in the final output of the model), forums, books, scientific articles, news articles, code bases, etc. \n",
    "\n",
    "For a taste of web scraping, read about my project for [scraping prices of goods and sending notifications of price drops](../web_scraping/web_scraping.ipynb). \n",
    "\n",
    "### Use Public Datasets\n",
    "\n",
    "* [Common Crawl](https://commoncrawl.org/)\n",
    "* [The Pile](https://pile.eleuther.ai/)\n",
    "* [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index)\n",
    "\n",
    "Private datasets are also available, like FinPile (used by BloombergGPT). \n",
    "\n",
    "### Auxillary LLMs to Generate Synthetic Data \n",
    "\n",
    "Alpaca. \n",
    "\n",
    "## Step 2: Training\n",
    "\n",
    "### Pre-Training\n",
    "\n",
    "### Post-Training\n",
    "\n",
    "## Step 3: Architecture Choices\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Output is an embedding, this is most useful for classification purposes.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "Does not see the future. No attention is paid by tokens to future tokens in the sentence. The output is a probability distribution over the entire corpus or, effectively, the next predictable token.\n",
    "\n",
    "### Encoder-Decoder\n",
    "\n",
    "Transformers are encoder-decoder models.\n",
    "\n",
    "\n",
    "### Tweaking Architecture\n",
    "\n",
    "An LLM outputs a probability distributiom over the entire corpus. So, how does such a device work for the explicit task of multiple choice answering, for example? Well, we can tokenize and pass the entire question, along with the multiple choice answers, as raw tokens and hope the model outputs \"A,\" \"B,\" \"C,\" or \"D\" as the most probable next token. We can, of course, do much better by constraining the choices to those tokens and comparing which of those has the highest probability. We can also template our prompts. We can instruct the LLM to understand that every prompt follows a template of \"question\" and \"answer choices.\" There are many options for tweaking our models in these ways as part of pre-training or post-training. \n",
    "\n",
    "## Step 4: Evaluation\n",
    "\n",
    "### Choice of Loss\n",
    "\n",
    "### Benchmarking Datasets\n",
    "\n",
    "Multiple-choice tasks: [ARC](https://github.com/fchollet/ARC-AGI), [Hellaswag](https://rowanzellers.com/hellaswag/), [MMLU](https://paperswithcode.com/dataset/mmlu),...\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "### NLP Metrics\n",
    "\n",
    "Quantifies the quality of the output via metrics such as Perplexity, [BLEU](https://en.wikipedia.org/wiki/BLEU), or [ROGUE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scores.\n",
    "\n",
    "Perplexity is a measure of how many tokens our LLM was hesitating on choosing as the most probable next token (out of the entire corpus). Fornmally... \n",
    "\n",
    "Another option is using auxillary fine-tuned LLMs which was used in the [TruthfulQA paper](https://arxiv.org/pdf/2109.07958) to compare output to some ground truth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

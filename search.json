[
  {
    "objectID": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter.html",
    "href": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter.html",
    "title": "Using JavaScript and D3 in Jupyter",
    "section": "",
    "text": "By far the quickest, and dirtiest way to use D3 in Jupyter notebooks is using the Jupyter kernel. This poses disadvantages, namely that we need to write the JavaScript and D3 code non-interactively, then pass it as a raw string inside a &lt;script&gt; tag. We may stub some data fields (like $data) and, on-the-fly, switch out the stub with data gathered using Python. But we can also use Deno, a JavaScript runtime environment from the maker of Node.js, as a kernel instead (more on that here)\nLet’s look at a minimal example.\n\nfrom IPython.display import display, HTML\n\n\nHTML('&lt;p&gt; Hello world! &lt;/p&gt;')\n\n Hello world! \n\n\nLet’s get some simple styling options.\n\nHTML('''\n&lt;style scoped&gt;\n.steely {\n    color: steelblue;\n    font: 16px script;\n}\n&lt;/style&gt;\n&lt;p class=\"steely\"&gt; Hello world! &lt;/div&gt;\n'''\n)\n\n\n\n Hello world!"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html",
    "href": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html",
    "title": "Using JavaScript and D3 in Jupyter with Deno",
    "section": "",
    "text": "In this post we will set up D3 and JavaScript inside a Jupyter notebook using Deno which enables the use of modern JavaScript, TypeScript, and npm inside Jupyter.\n\n\nIn order to run JavaScript and, by extension, D3 code inside Jupyter notebooks we need a JavaScript runtime environment (a JS kernel for the Jupyter notebook). Deno is a new contender to Node. Download and install Deno by issuing the following CURL.\ncurl -fsSL https://deno.land/install.sh | sh\nWe need to register Deno as a kernel for Jupyter. The following command does the trick.\ndeno jupyter --install\nNote that we may need to activate the Deno kernel. To do so, just issue this command.\ndeno jupyter --unstable\nFinally, inside our Jupyer editor (for me that’s just VS Code), we need to select the Deno kernel.\nAfter all of this, let’s verify that we can run JavaScript inside this Juptyer notebook.\n\nconsole.log(\"Hello from this Jupyter notebook!\")\n\nHello from this Jupyter notebook!\n\n\n\n\n\nWe will use GeoJSON, and its extension TopoJSON, as our map data in order to visualize an interactive map of the United States (down to the county-level). The TopoJSON extension project is more than just a raw re-representation of the US political map in GeoJSON, it also provides an interface to manipulate the raw data in various useful ways. We will import the data as well as the extra tooling, but first let’s focus on getting the data.\n\n\nWe will use this convenient re-distribution of the US map data.\n\n// Fetch the US map TopoJSON data\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\"; \nconst us = await (await fetch(url)).json();\n\nIf we examine the TopoJSON file us obtained above, we can notice useful patterns. For example, we can log the first state’s id, and name.\n\nconsole.log(JSON.stringify(us.objects.states.geometries.slice(0,1), [\"id\", \"properties\", \"name\"], 2))\n\n[\n  {\n    \"id\": \"01\",\n    \"properties\": {\n      \"name\": \"Alabama\"\n    }\n  }\n]\n\n\nNow let’s do the same for the counties TopoJSON dataset.\n\n// Fetch the US map TopoJSON data\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/counties-10m.json\"; \nconst us = await (await fetch(url)).json();\n\n\nconsole.log(JSON.stringify(us.objects.counties.geometries.slice(0,1), [\"id\", \"properties\", \"name\"], 2))\n\n[\n  {\n    \"id\": \"04015\",\n    \"properties\": {\n      \"name\": \"Mohave\"\n    }\n  }\n]\n\n\nNotice that both states and counties are represented in a similar JSON format (specifically, TopoJSON). They both have an id field (which uniquely identifies a region), and a properties.name field containing the name of the region.\nAs it turns out, there is further structure in this representation that we can exploit. We can easily identify all counties belonging to a given state. The field counties.geometry.id is tied to states.geometries.id by its first two digits. Meaning, if Mohave county above has id = 04015, then the state of Arizona must have id = 04.\nLet’s define a utility function to do just that.\n\n// Helper function to get the counties of a given state based on `state.id`\nconst getCountiesOfState = (us, stateId) =&gt; \n    us.objects.counties.filter((county) =&gt; county.id.slice(0,2) == stateId);\n\n\n\n\n\n\nFirstly, we will obviously need D3. With Deno, it is as simple as:\n\nimport * as d3 from \"npm:d3\"\n\nD3 is a JavaScript library for visualization. The three D-s in “D3” stand for “Data Driven Documents.” Think DOM elements that are styled, positioned, scaled, etc. according to some attached source of data.\nIn fact, a core pattern in D3 is the following:\n\nSelect some array of DOM elements\n‘Attach’ an array of data to that array of DOM elements\nPerform some actions for each DOM element formed from the data array\n\nThe first two steps above roughly translates to the following API.\n\narrayOfElements.selectAll(\"div\")\n  .data(correspondingDataset)\n  .join(\"div\")\n\nWe can imagine &lt;divs&gt; being bars in a bar graph scaled vertically according to the values inside correspondingDataset.\nThe third step, that of performing further actions on the D3 selection, follows the same pattern of chained function calls. At each step, a D3 Selection is returned and ready to be manipulated again by D3.\n\n\n\nSecondly, we need the means to work with the above data format, and to display it inside a browser-less JavaScript runtime. Why? Remember that it’s Chrome that implements an HTML DOP API, and an HTML Canvas API on top of the V8 JavaScript engine. We’re going to need a way to create DOM elements using just the Deno JavaScript runtime, without Chrome.\nFor DOM support, and to render rich media types (like HTML, Markdown, SVG, etc.) in Deno, we have two options. The first is to use the skia-canvas module, the second is to use a Deno (or Node.js) DOM API like Linkedom which offers a mock DOM on top of Deno’s JavaScript runtime.\nWe also need a number of utilities from TopoJSON. We’ll need feature, and mesh for now (but we import the entire library).\nFrom D3, we’ll specifically need geoPath – a geographic path generator with the default parameters (such as the type of geographc projection into 2D). There is a default projection if not supplied, but we can also use the d3.geoAlbersUsa projection explicitly.\n\n\n\n\nAs a minimal example, let’s focus on just displaying the map from the data we downloaded and converted earlier. We will add hover-to-highlight interactive feature, but not much else. We will add all the different interactive features later.\nTo render the DOM we will be constructing, we will use Deno.jupyer.display which provides a way to render objects as rich media types (such as HTML, Markdown, etc.). This builds upon Jupyter’s support of mimetypes. All we have to do, to display an object as a rich media type, is simply create an object that invokes the function Jupyter.display like so:\n\n{\n    [Symbol.for('Jupyter.display')]: () =&gt; ({\n      'text/html': '&lt;p&gt; Hello from this Jupyter notebook!&lt;p&gt;'\n    })\n}\n\n Hello from this Jupyter notebook!\n\n\nNow let’s see how to render the map as a non-interactive &lt;svg&gt; component.\n\n// Imports: D3, TopoJSON, and Linkedom\nimport * as d3 from \"npm:d3\";\nimport * as topojson from \"npm:topojson\";\nimport { DOMParser } from \"npm:linkedom\";\n\n// Create a fake DOM via Linkedom\nconst document = new DOMParser().parseFromString(\n  `&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;`,\n  \"text/html\",\n);\n\n// Fetch and parse the US map TopoJSON\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\";\nconst us = await (await fetch(url)).json();\nconst states = topojson.feature(us, us.objects.states) as any; // GeoJSON 'FeatureCollection'\n\n// Create an &lt;svg&gt; wrapper in Linkedom's fake DOM using D3\nconst width = 975;\nconst height = 610;\nconst body = d3.select(document.body);\nconst svg = body\n  .append(\"svg\")\n  .attr(\"xmlns\", \"http://www.w3.org/2000/svg\") \n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .attr(\"viewBox\", `0 0 ${width} ${height}`);\nconst group = svg.append(\"g\");\nconst groupOfStates = group.append(\"g\")\n  .attr(\"fill\", \"#EBE8E7\")\n  .attr(\"stroke\", \"#191516\")\n\n// Create a geo-projection & a path generator\nconst projection = d3.geoAlbersUsa().fitSize([width, height], states);\nconst path = d3.geoPath(projection);\n\n// Append one &lt;path&gt; per state\ngroupOfStates.selectAll(\"path\")\n  .data(states.features)\n  .join(\"path\")\n  // Define the 'd' attribute of the &lt;path&gt; which gives it its shape\n  .attr(\"d\", path);\n  \n\n// Get the markup from Linkedom to display it\nconst htmlOutput = document.documentElement.outerHTML; // Entire &lt;html&gt;…&lt;/html&gt;\n\n// Use 'Deno.jupyter.display' to display the raw &lt;html&gt; string as actual HTML\nawait Deno.jupyter.display({\n  \"text/html\": htmlOutput,\n}, { raw: true });\n\n\n\n\n\n\n\nWe saw how to render the map as non-inteactive &lt;svg&gt;, but it’s pretty useless by itself without interactivity features. Let’s see how to add those!\nIn D3, we may register a JavaScript event listener using D3’s own selection.on API. This does not directly change the HTML attributes (e.g. onmouseover=\"...\") of the &lt;path&gt; elements in the DOM (unlike using the selection.attr API when adding event listeners). Instead, it attaches a listener in-memory -— conceptually the same as calling element.addEventListener(\"mouseover\", callback) in vanilla JavaScript.\nBoth attribute-based event listeners and such, in-memory, ones work together (this can, potentially, introduce styling conflicts). Nevertheless, we explore both options below. Much of the code is the same, so it’s collapsed by default.\n\n\nClick to expand the code snippet\n\n\n// Imports: D3, TopoJSON, and Linkedom\nimport * as d3 from \"npm:d3\";\nimport * as topojson from \"npm:topojson\";\nimport { DOMParser } from \"npm:linkedom\";\n\n// Create a fake DOM via Linkedom\nconst document = new DOMParser().parseFromString(\n  `&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;`,\n  \"text/html\",\n);\n\n// Fetch and parse the US map TopoJSON\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\";\nconst us = await (await fetch(url)).json();\nconst states = topojson.feature(us, us.objects.states) as any; // GeoJSON 'FeatureCollection'\n\n// Create an &lt;svg&gt; wrapper in Linkedom's fake DOM using D3\nconst width = 975;\nconst height = 610;\nconst body = d3.select(document.body);\nbody\n  .append(\"p\")\n  .text(\"💡 Tip: Try hovering over a county!\")\n  .style(\"text-align\", \"center\")\nconst svg = body\n  .append(\"svg\")\n  .attr(\"xmlns\", \"http://www.w3.org/2000/svg\") \n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .attr(\"viewBox\", `0 0 ${width} ${height}`);\nconst group = svg.append(\"g\");\nconst groupOfStates = group.append(\"g\")\n  .attr(\"fill\", \"#EBE8E7\")\n  .attr(\"stroke\", \"#191516\")\n\n// Create a geo-projection & a path generator\nconst projection = d3.geoAlbersUsa().fitSize([width, height], states);\nconst path = d3.geoPath(projection);\n\n\n// Append one &lt;path&gt; per state\ngroupOfStates.selectAll(\"path\")\n  .data(states.features)\n  .join(\"path\")\n  // Define the 'd' attribute of the &lt;path&gt; which gives it its shape\n  .attr(\"d\", path)\n  // Add an `id` (useful for debugging)\n  .attr(\"id\", \"state-path\")\n  // Interactivity: Hover to display tooltip\n  .on(\"mouseover\", mouseover)\n  // Interactivity: Hover to change color\n  .attr(\"cursor\", \"pointer\")\n  .attr(\"onmouseover\", \"this.style.fill = '#F9C22E';\")\n  .attr(\"onmouseout\", \"this.style.fill = '#EBE8E7';\");\n\nfunction mouseover(event, d) {\n  console.log(\"HERE\")\n}\n\n[Function: mouseover]\n\n\n\nD3 has a small convenience. If we do:\nconst currentListener = d3.select(someElement).on(\"click\");\nIf a listener function is currently assigned to the HTML DOM event \"click\" on the first matched element in that D3 selection, currentListener will be that function. Else it will be undefined. This gives us a good way to debug events.\nIf the events aren’t triggering, for some reason, make sure to check the value of currentListener inside the browser’s console (don’t just log it inside this Deno Jupyter notebook). The DOM may be re-rendering, at some point, and replacing a &lt;path&gt; element after D3 sets the event listener (with selection.on). This may cause the attached listener to be lost. The source of truth is always the browser’s environment.\nNote that we’ll have to import D3 within the browser’s console because, by default, it won’t be inside the global window object (which is used to define global variables for a given browser session). It’s local to the scope of wherever this notebook’s code ends up being executed from within the browser. To use it in the console, we’ll have to import it within some new scope (i.e. inside some function). We can also explicitly attach it to global scope (e.g., window.d3 = d3) like so:\nfunction d3Module(require) {\n    window.d3 = require('d3');\n    const currentListener = window.d3.select(\"#state-path\").on(\"click\");\n    console.log(currentListener);\n};\n\n// Get the new markup from Linkedom to display it\nconst htmlOutput = document.documentElement.outerHTML; // Entire &lt;html&gt;…&lt;/html&gt;\n\n// Use 'Deno.jupyter.display' to display the raw &lt;html&gt; string, now containing the county &lt;paths&gt;-s, as actual HTML\nawait Deno.jupyter.display({\n  \"text/html\": htmlOutput,\n}, { raw: true });\n\n💡 Tip: Try hovering over a county!\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html#jupyter-deno-setup",
    "href": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html#jupyter-deno-setup",
    "title": "Using JavaScript and D3 in Jupyter with Deno",
    "section": "",
    "text": "In order to run JavaScript and, by extension, D3 code inside Jupyter notebooks we need a JavaScript runtime environment (a JS kernel for the Jupyter notebook). Deno is a new contender to Node. Download and install Deno by issuing the following CURL.\ncurl -fsSL https://deno.land/install.sh | sh\nWe need to register Deno as a kernel for Jupyter. The following command does the trick.\ndeno jupyter --install\nNote that we may need to activate the Deno kernel. To do so, just issue this command.\ndeno jupyter --unstable\nFinally, inside our Jupyer editor (for me that’s just VS Code), we need to select the Deno kernel.\nAfter all of this, let’s verify that we can run JavaScript inside this Juptyer notebook.\n\nconsole.log(\"Hello from this Jupyter notebook!\")\n\nHello from this Jupyter notebook!"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html#example-visualization---map-of-the-us",
    "href": "unpublished_posts/mathematical_visualization/js_and_d3_in_jupyter_with_deno.html#example-visualization---map-of-the-us",
    "title": "Using JavaScript and D3 in Jupyter with Deno",
    "section": "",
    "text": "We will use GeoJSON, and its extension TopoJSON, as our map data in order to visualize an interactive map of the United States (down to the county-level). The TopoJSON extension project is more than just a raw re-representation of the US political map in GeoJSON, it also provides an interface to manipulate the raw data in various useful ways. We will import the data as well as the extra tooling, but first let’s focus on getting the data.\n\n\nWe will use this convenient re-distribution of the US map data.\n\n// Fetch the US map TopoJSON data\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\"; \nconst us = await (await fetch(url)).json();\n\nIf we examine the TopoJSON file us obtained above, we can notice useful patterns. For example, we can log the first state’s id, and name.\n\nconsole.log(JSON.stringify(us.objects.states.geometries.slice(0,1), [\"id\", \"properties\", \"name\"], 2))\n\n[\n  {\n    \"id\": \"01\",\n    \"properties\": {\n      \"name\": \"Alabama\"\n    }\n  }\n]\n\n\nNow let’s do the same for the counties TopoJSON dataset.\n\n// Fetch the US map TopoJSON data\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/counties-10m.json\"; \nconst us = await (await fetch(url)).json();\n\n\nconsole.log(JSON.stringify(us.objects.counties.geometries.slice(0,1), [\"id\", \"properties\", \"name\"], 2))\n\n[\n  {\n    \"id\": \"04015\",\n    \"properties\": {\n      \"name\": \"Mohave\"\n    }\n  }\n]\n\n\nNotice that both states and counties are represented in a similar JSON format (specifically, TopoJSON). They both have an id field (which uniquely identifies a region), and a properties.name field containing the name of the region.\nAs it turns out, there is further structure in this representation that we can exploit. We can easily identify all counties belonging to a given state. The field counties.geometry.id is tied to states.geometries.id by its first two digits. Meaning, if Mohave county above has id = 04015, then the state of Arizona must have id = 04.\nLet’s define a utility function to do just that.\n\n// Helper function to get the counties of a given state based on `state.id`\nconst getCountiesOfState = (us, stateId) =&gt; \n    us.objects.counties.filter((county) =&gt; county.id.slice(0,2) == stateId);\n\n\n\n\n\n\nFirstly, we will obviously need D3. With Deno, it is as simple as:\n\nimport * as d3 from \"npm:d3\"\n\nD3 is a JavaScript library for visualization. The three D-s in “D3” stand for “Data Driven Documents.” Think DOM elements that are styled, positioned, scaled, etc. according to some attached source of data.\nIn fact, a core pattern in D3 is the following:\n\nSelect some array of DOM elements\n‘Attach’ an array of data to that array of DOM elements\nPerform some actions for each DOM element formed from the data array\n\nThe first two steps above roughly translates to the following API.\n\narrayOfElements.selectAll(\"div\")\n  .data(correspondingDataset)\n  .join(\"div\")\n\nWe can imagine &lt;divs&gt; being bars in a bar graph scaled vertically according to the values inside correspondingDataset.\nThe third step, that of performing further actions on the D3 selection, follows the same pattern of chained function calls. At each step, a D3 Selection is returned and ready to be manipulated again by D3.\n\n\n\nSecondly, we need the means to work with the above data format, and to display it inside a browser-less JavaScript runtime. Why? Remember that it’s Chrome that implements an HTML DOP API, and an HTML Canvas API on top of the V8 JavaScript engine. We’re going to need a way to create DOM elements using just the Deno JavaScript runtime, without Chrome.\nFor DOM support, and to render rich media types (like HTML, Markdown, SVG, etc.) in Deno, we have two options. The first is to use the skia-canvas module, the second is to use a Deno (or Node.js) DOM API like Linkedom which offers a mock DOM on top of Deno’s JavaScript runtime.\nWe also need a number of utilities from TopoJSON. We’ll need feature, and mesh for now (but we import the entire library).\nFrom D3, we’ll specifically need geoPath – a geographic path generator with the default parameters (such as the type of geographc projection into 2D). There is a default projection if not supplied, but we can also use the d3.geoAlbersUsa projection explicitly.\n\n\n\n\nAs a minimal example, let’s focus on just displaying the map from the data we downloaded and converted earlier. We will add hover-to-highlight interactive feature, but not much else. We will add all the different interactive features later.\nTo render the DOM we will be constructing, we will use Deno.jupyer.display which provides a way to render objects as rich media types (such as HTML, Markdown, etc.). This builds upon Jupyter’s support of mimetypes. All we have to do, to display an object as a rich media type, is simply create an object that invokes the function Jupyter.display like so:\n\n{\n    [Symbol.for('Jupyter.display')]: () =&gt; ({\n      'text/html': '&lt;p&gt; Hello from this Jupyter notebook!&lt;p&gt;'\n    })\n}\n\n Hello from this Jupyter notebook!\n\n\nNow let’s see how to render the map as a non-interactive &lt;svg&gt; component.\n\n// Imports: D3, TopoJSON, and Linkedom\nimport * as d3 from \"npm:d3\";\nimport * as topojson from \"npm:topojson\";\nimport { DOMParser } from \"npm:linkedom\";\n\n// Create a fake DOM via Linkedom\nconst document = new DOMParser().parseFromString(\n  `&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;`,\n  \"text/html\",\n);\n\n// Fetch and parse the US map TopoJSON\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\";\nconst us = await (await fetch(url)).json();\nconst states = topojson.feature(us, us.objects.states) as any; // GeoJSON 'FeatureCollection'\n\n// Create an &lt;svg&gt; wrapper in Linkedom's fake DOM using D3\nconst width = 975;\nconst height = 610;\nconst body = d3.select(document.body);\nconst svg = body\n  .append(\"svg\")\n  .attr(\"xmlns\", \"http://www.w3.org/2000/svg\") \n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .attr(\"viewBox\", `0 0 ${width} ${height}`);\nconst group = svg.append(\"g\");\nconst groupOfStates = group.append(\"g\")\n  .attr(\"fill\", \"#EBE8E7\")\n  .attr(\"stroke\", \"#191516\")\n\n// Create a geo-projection & a path generator\nconst projection = d3.geoAlbersUsa().fitSize([width, height], states);\nconst path = d3.geoPath(projection);\n\n// Append one &lt;path&gt; per state\ngroupOfStates.selectAll(\"path\")\n  .data(states.features)\n  .join(\"path\")\n  // Define the 'd' attribute of the &lt;path&gt; which gives it its shape\n  .attr(\"d\", path);\n  \n\n// Get the markup from Linkedom to display it\nconst htmlOutput = document.documentElement.outerHTML; // Entire &lt;html&gt;…&lt;/html&gt;\n\n// Use 'Deno.jupyter.display' to display the raw &lt;html&gt; string as actual HTML\nawait Deno.jupyter.display({\n  \"text/html\": htmlOutput,\n}, { raw: true });\n\n\n\n\n\n\n\nWe saw how to render the map as non-inteactive &lt;svg&gt;, but it’s pretty useless by itself without interactivity features. Let’s see how to add those!\nIn D3, we may register a JavaScript event listener using D3’s own selection.on API. This does not directly change the HTML attributes (e.g. onmouseover=\"...\") of the &lt;path&gt; elements in the DOM (unlike using the selection.attr API when adding event listeners). Instead, it attaches a listener in-memory -— conceptually the same as calling element.addEventListener(\"mouseover\", callback) in vanilla JavaScript.\nBoth attribute-based event listeners and such, in-memory, ones work together (this can, potentially, introduce styling conflicts). Nevertheless, we explore both options below. Much of the code is the same, so it’s collapsed by default.\n\n\nClick to expand the code snippet\n\n\n// Imports: D3, TopoJSON, and Linkedom\nimport * as d3 from \"npm:d3\";\nimport * as topojson from \"npm:topojson\";\nimport { DOMParser } from \"npm:linkedom\";\n\n// Create a fake DOM via Linkedom\nconst document = new DOMParser().parseFromString(\n  `&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;`,\n  \"text/html\",\n);\n\n// Fetch and parse the US map TopoJSON\nconst url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json\";\nconst us = await (await fetch(url)).json();\nconst states = topojson.feature(us, us.objects.states) as any; // GeoJSON 'FeatureCollection'\n\n// Create an &lt;svg&gt; wrapper in Linkedom's fake DOM using D3\nconst width = 975;\nconst height = 610;\nconst body = d3.select(document.body);\nbody\n  .append(\"p\")\n  .text(\"💡 Tip: Try hovering over a county!\")\n  .style(\"text-align\", \"center\")\nconst svg = body\n  .append(\"svg\")\n  .attr(\"xmlns\", \"http://www.w3.org/2000/svg\") \n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .attr(\"viewBox\", `0 0 ${width} ${height}`);\nconst group = svg.append(\"g\");\nconst groupOfStates = group.append(\"g\")\n  .attr(\"fill\", \"#EBE8E7\")\n  .attr(\"stroke\", \"#191516\")\n\n// Create a geo-projection & a path generator\nconst projection = d3.geoAlbersUsa().fitSize([width, height], states);\nconst path = d3.geoPath(projection);\n\n\n// Append one &lt;path&gt; per state\ngroupOfStates.selectAll(\"path\")\n  .data(states.features)\n  .join(\"path\")\n  // Define the 'd' attribute of the &lt;path&gt; which gives it its shape\n  .attr(\"d\", path)\n  // Add an `id` (useful for debugging)\n  .attr(\"id\", \"state-path\")\n  // Interactivity: Hover to display tooltip\n  .on(\"mouseover\", mouseover)\n  // Interactivity: Hover to change color\n  .attr(\"cursor\", \"pointer\")\n  .attr(\"onmouseover\", \"this.style.fill = '#F9C22E';\")\n  .attr(\"onmouseout\", \"this.style.fill = '#EBE8E7';\");\n\nfunction mouseover(event, d) {\n  console.log(\"HERE\")\n}\n\n[Function: mouseover]\n\n\n\nD3 has a small convenience. If we do:\nconst currentListener = d3.select(someElement).on(\"click\");\nIf a listener function is currently assigned to the HTML DOM event \"click\" on the first matched element in that D3 selection, currentListener will be that function. Else it will be undefined. This gives us a good way to debug events.\nIf the events aren’t triggering, for some reason, make sure to check the value of currentListener inside the browser’s console (don’t just log it inside this Deno Jupyter notebook). The DOM may be re-rendering, at some point, and replacing a &lt;path&gt; element after D3 sets the event listener (with selection.on). This may cause the attached listener to be lost. The source of truth is always the browser’s environment.\nNote that we’ll have to import D3 within the browser’s console because, by default, it won’t be inside the global window object (which is used to define global variables for a given browser session). It’s local to the scope of wherever this notebook’s code ends up being executed from within the browser. To use it in the console, we’ll have to import it within some new scope (i.e. inside some function). We can also explicitly attach it to global scope (e.g., window.d3 = d3) like so:\nfunction d3Module(require) {\n    window.d3 = require('d3');\n    const currentListener = window.d3.select(\"#state-path\").on(\"click\");\n    console.log(currentListener);\n};\n\n// Get the new markup from Linkedom to display it\nconst htmlOutput = document.documentElement.outerHTML; // Entire &lt;html&gt;…&lt;/html&gt;\n\n// Use 'Deno.jupyter.display' to display the raw &lt;html&gt; string, now containing the county &lt;paths&gt;-s, as actual HTML\nawait Deno.jupyter.display({\n  \"text/html\": htmlOutput,\n}, { raw: true });\n\n💡 Tip: Try hovering over a county!\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let’s define a package: com.myservice."
  },
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let’s define a package: com.myservice."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "",
    "text": "In this post we’ll be going over how to set up our local development environment for making machine learning applications and blogging about the process. This is my attempt at installing the required software packages on a Windows machine. I’ll do my best to keep things general but some of the steps will be Windows specific."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Installing Python - System Level",
    "text": "Installing Python - System Level\nFirst, we download and install the latest version of Python for our OS from the official website.\n\n\n\n\n\n\n💡 Tip\n\n\n\n\n\nMake sure to tick the “add to PATH” box during the installation so that the path of the Python executible is added to our system’s PATH environment variable. The path in question, by default, is ~\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n\n\n\n\n\n\n\n\n\n🔧 Troubleshooting\n\n\n\n\n\nIf the command python is unrecognized on Windows after installation, try py. We should be able to issue the command py to invoke the Python interpreter. Running py --version should return the version number (e.g. Python 3.11.5)"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Why use Conda",
    "text": "Why use Conda\nWhile pip is Python’s built-in package manager and venv is its built-in virtual environment manager, we use Conda because it attempts to do more than what pip and venv try to accomplish do individually by extending support to library dependencies not written in Python.\nOccasionally, when a Conda distribution is not available, but an PyPI distribution exists, it makes sense to combine use of conda and pip. This is done by:\n\nInstalling pip within a Conda environment: conda install pip\nInstalling the required package from inside the active Conda environment: pip install &lt;package_name&gt;\n\nThis way, the packages do not go to the system-level Python’s packages directory C:\\Users\\&lt;username&gt;\\AppData\\Local\\Python\\&lt;version&gt;\\ (or Roaming instead of Local, if Python was installed only for a specific user on Windows). Instead, pip installs them in the Conda environment’s C:\\ProgramData\\anaconda3\\Lib\\site-packages (or similar) package directory. We can check each package, along with its installation destination by running pip list -v.\n\nInstalling Anaconda Navigator (or Miniconda)\nNext, download and install Anaconda Navigator (or Miniconda, which installs the Conda scientific package and Python environment manager without additional software and without the GUI navigator). This installation includes tools like Jupyter Notebooks, Spyder, PyCharm, and other scientific packages and IDEs.\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nAnaconda’s built in Python distribution: Anaconda comes with its own latest Python version distribution (by default installed into path c:\\ProgramData\\anaconda3\\python.exe). The installer will prompt us to select an option which enables third-party editors, such as VSCode, to recognize this Python distribution.\n\n\n\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nDifferent Python distributions can live on the same machine: Running python --version in the Anaconda Prompt returns Python 3.11.4 as of the time of writing this, which is the version of Python that Conda installed in its base environment. Crucially, running py --version, even in the Anaconda Prompt, still returns Python 3.11.5, which is the system’s version of Python."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Choosing the Right Python Kernel in VSCode",
    "text": "Choosing the Right Python Kernel in VSCode\nIn VSCode, we can open the Command Palette and run the command Notebook: Select Notebook Kernel. At first, this will prompt us to install the Jupyter and Python VSCode extensions. Once that’s done, we can rerun the command and select the Python kernel in the desired Conda environment (by default base)."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Initializing Conda in the Shell",
    "text": "Initializing Conda in the Shell\nBefore we can use the full capabilities of Conda in the terminal, we need to initialize it by running the command:\nconda init &lt;bash|powershell|tsh|...&gt; # Depending on the shell we're using\nRestart your terminal for changes to take hold.\n\n\n\n\n\n\n🔧 Troubleshooting\n\n\n\n\n\nFor Windows users, Powershell may throw the following error in trying to load the user profile: execution of scripts is disabled on this system. This is Powershell’s security measure against command hijacking, its way of enforcing control of execution and establishing identity. If this is the case, run cmd.exe as Administrator and execute command powershell Set-ExecutionPolicy RemoteSigned -Scope CurrentUser. We should now see the active environment in parentheses (e.g. base) to the left of the input in Powershell."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "href": "unpublished_posts/local_development_setup/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Conda Commands",
    "text": "Conda Commands\nSome common Conda commands are:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nconda env list\nShows all the Conda environments (the active environment is marked with *)\n\n\nconda list\nShows all the packages installed in the currently active environment\n\n\nconda update --all\nUpdates all packages in the active environment (frequently resolves environment is inconsistent errors)\n\n\nconda info\nShows, among other things, the directory where the environment is stored\n\n\nconda activate &lt;myenv&gt;\nActivate environment &lt;myenv&gt;\n\n\nconda deactivate\nDeactivates the currently active environment\n\n\nconda create --name &lt;myenv&gt; [python=3.7.0 [ipython]]\nCreate a new empty environment (or an environment with a python version and ipython)\n\n\nconda create --name &lt;myenv&gt; --clone base\nClone the base environment\n\n\nconda env export -f &lt;path/to/envfile.yml&gt;\nExport the package list of the active environment (e.g. conda env export -f  /Users/&lt;username&gt;/Documents/MyFiles/personal-blog.yml)\n\n\nconda compare &lt;path/to/envfile.yml&gt;\nCompare the active environment to the exported file of another environment\n\n\nconda remove --name &lt;myenv&gt; --all\nDeletes the environment\n\n\n\n\nComparing Conda Environments\nOften we need to compare the packages between two environments. Here’s the workflow to do that:\n\nActivate one of the environments using activate\nExport its package list using export as a .yml file to a destination of our choice\nActivate the second environment\nExecute the compare command, providing the path to the .yml file created in the previous step"
  },
  {
    "objectID": "unpublished_posts/algorithms/merge_sort.html",
    "href": "unpublished_posts/algorithms/merge_sort.html",
    "title": "Merge Sort",
    "section": "",
    "text": "Mergesort consists of two phases (or subroutines). The function mergesort itself (i.e. the divide phase), and merge (or the conquer) phase. mergesort divides the problem into two half-sized sub-problems recursively. At any given stack frame of mergesort, once both recursive calls exit, we’re left with two separately ordered subarrays that the merge subroutine accepts, and merges together while preserving sortedness of the result.\nLike most sorting algorithms, Mergsort is \\(O(n * log n)\\) and, in this case, it’s easy to intuit why. The process of dividing an \\(n\\)-sized array into equal halves can be done, loosely speaking, \\(log n\\) times. Each time it’s done, the merge subroutine will be called which takes \\(O(n)\\) time to merge the resulting half-sized sub-arrays.\nFirst let’s write down the algorithm, then we will visualize it using Manim."
  },
  {
    "objectID": "unpublished_posts/machine_learning/builing-an-LLM.html",
    "href": "unpublished_posts/machine_learning/builing-an-LLM.html",
    "title": "Building an LLM",
    "section": "",
    "text": "GPT-3 was trained on 0.5T tokens, today’s leading models are often trained on 5T tokens and above.\n\n\nWrite a script to download the whole internet. The internet is composed of a lot of Wikipedia articles (which are of particular value, as sources that have been referenced from Wikipedia are often given more weight in the final output of the model), forums, books, scientific articles, news articles, code bases, etc.\nFor a taste of web scraping, read about my project for scraping prices of goods and sending notifications of price drops.\n\n\n\n\nCommon Crawl\nThe Pile\nHugging Face Datasets\n\nPrivate datasets are also available, like FinPile (used by BloombergGPT).\n\n\n\nAlpaca.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput is an embedding, this is most useful for classification purposes.\n\n\n\nDoes not see the future. No attention is paid by tokens to future tokens in the sentence. The output is a probability distribution over the entire corpus or, effectively, the next predictable token.\n\n\n\nTransformers are encoder-decoder models.\n\n\n\nAn LLM outputs a probability distributiom over the entire corpus. So, how does such a device work for the explicit task of multiple choice answering, for example? Well, we can tokenize and pass the entire question, along with the multiple choice answers, as raw tokens and hope the model outputs “A,” “B,” “C,” or “D” as the most probable next token. We can, of course, do much better by constraining the choices to those tokens and comparing which of those has the highest probability. We can also template our prompts. We can instruct the LLM to understand that every prompt follows a template of “question” and “answer choices.” There are many options for tweaking our models in these ways as part of pre-training or post-training.\n\n\n\n\n\n\n\n\n\nMultiple-choice tasks: ARC, Hellaswag, MMLU,…\n\n\n\n\n\n\nQuantifies the quality of the output via metrics such as Perplexity, BLEU, or ROGUE scores.\nPerplexity is a measure of how many tokens our LLM was hesitating on choosing as the most probable next token (out of the entire corpus). Fornmally…\nAnother option is using auxillary fine-tuned LLMs which was used in the TruthfulQA paper to compare output to some ground truth."
  },
  {
    "objectID": "unpublished_posts/machine_learning/builing-an-LLM.html#step-1-data-curation",
    "href": "unpublished_posts/machine_learning/builing-an-LLM.html#step-1-data-curation",
    "title": "Building an LLM",
    "section": "",
    "text": "GPT-3 was trained on 0.5T tokens, today’s leading models are often trained on 5T tokens and above.\n\n\nWrite a script to download the whole internet. The internet is composed of a lot of Wikipedia articles (which are of particular value, as sources that have been referenced from Wikipedia are often given more weight in the final output of the model), forums, books, scientific articles, news articles, code bases, etc.\nFor a taste of web scraping, read about my project for scraping prices of goods and sending notifications of price drops.\n\n\n\n\nCommon Crawl\nThe Pile\nHugging Face Datasets\n\nPrivate datasets are also available, like FinPile (used by BloombergGPT).\n\n\n\nAlpaca."
  },
  {
    "objectID": "unpublished_posts/machine_learning/builing-an-LLM.html#step-3-architecture-choices",
    "href": "unpublished_posts/machine_learning/builing-an-LLM.html#step-3-architecture-choices",
    "title": "Building an LLM",
    "section": "",
    "text": "Output is an embedding, this is most useful for classification purposes.\n\n\n\nDoes not see the future. No attention is paid by tokens to future tokens in the sentence. The output is a probability distribution over the entire corpus or, effectively, the next predictable token.\n\n\n\nTransformers are encoder-decoder models.\n\n\n\nAn LLM outputs a probability distributiom over the entire corpus. So, how does such a device work for the explicit task of multiple choice answering, for example? Well, we can tokenize and pass the entire question, along with the multiple choice answers, as raw tokens and hope the model outputs “A,” “B,” “C,” or “D” as the most probable next token. We can, of course, do much better by constraining the choices to those tokens and comparing which of those has the highest probability. We can also template our prompts. We can instruct the LLM to understand that every prompt follows a template of “question” and “answer choices.” There are many options for tweaking our models in these ways as part of pre-training or post-training."
  },
  {
    "objectID": "unpublished_posts/machine_learning/builing-an-LLM.html#step-4-evaluation",
    "href": "unpublished_posts/machine_learning/builing-an-LLM.html#step-4-evaluation",
    "title": "Building an LLM",
    "section": "",
    "text": "Multiple-choice tasks: ARC, Hellaswag, MMLU,…\n\n\n\n\n\n\nQuantifies the quality of the output via metrics such as Perplexity, BLEU, or ROGUE scores.\nPerplexity is a measure of how many tokens our LLM was hesitating on choosing as the most probable next token (out of the entire corpus). Fornmally…\nAnother option is using auxillary fine-tuned LLMs which was used in the TruthfulQA paper to compare output to some ground truth."
  },
  {
    "objectID": "unpublished_posts/general_computer_science/recursion_optimizations.html",
    "href": "unpublished_posts/general_computer_science/recursion_optimizations.html",
    "title": "Recursion Optimizations",
    "section": "",
    "text": "Recursion Optimizations\nRecursive algorithms, while elegant and expressive, can sometimes lead to performance issues due to the overhead of function calls and potential for repeated computations. Several optimization techniques can be employed to enhance their efficiency. One common method is memoization, which stores the results of expensive function calls and reuses them when the same inputs occur again, avoiding repeated computations. Another technique is tail recursion optimization, where the recursive call is the final operation in the function, allowing the system to reuse the current stack frame for each recursive call. Tail recursion optimization reduces the space complexity from \\(O(n)\\) to \\(O(1)\\). Additionally, iterative solutions can often be more efficient than their recursive counterparts, so converting a recursive algorithm to an iterative one can be considered an optimization. Understanding these techniques, and variations on them, can greatly improve the performance of our recursive algorithms.\n\nTail Recursion - Avoiding Stack Overflow\nStack overflow (which is when the system runs out of short term memory) is a common concern when working with recursive functions or when doing functional programming, where function composition is the mode in which we think. Tail recursion optimization helps us drastically cut the number of stack frames and, as mentioned earlier, makes our recursive algorithms take a constant amount of space instead of a linear amount (or worse).\n\nClassic Example: Factorial\nLet’s take the classic example of calculating a factorial.\nNaive Recursive Implementation:\n\ndef factorial(n):\n    if n == 0: # Base case: 0! = 1\n        return 1\n    else: \n        return n * factorial(n-1) # Recursive step\n\nfactorial(4)\n\n24\n\n\nFrom the below untangled definition of factorial (for \\(n=4\\)) we can surmise what goes on in the stack. The stack first fills up with stack frames for factorial(n) down to factorial(0), which is the last frame on the stack before it begins to pop and actual evaluation happens.\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = 4 * factorial(3) \\\\\n        & = 4 * (3 * factorial(2)) \\\\\n        & = 4 * (3 * (2 * factorial(1))) \\\\\n        & = 4 * (3 * (2 * (1 * factorial(0)))) \\\\\n        & = 4 * (3 * (2 * (1 * 1))) \\\\\n        & = 4 * (3 * (2 * 1)) \\\\\n        & = 4 * (3 * 2) \\\\\n        & = 4 * 6 \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nAs we can see the function is called for \\(n = 4\\) down to the base case of \\(n = 0\\) (each call stacking up in memory) before evaluation even begins. Evaluation then happens step-by-step inside each stack frame until all of them have popped.\nIt’s not immediately clear how to make the calls independent of each other given that there is a multiplicative factor in front of the recursive call (which is what makes this particular function fail to be tail-recursive). It helps to think in terms of carried state. In this case the idea is simple, if we can carry the state of the current stack frame into the next one as input, then we can pop each frame right after it calls the next frame. Why? Because at that point, having carried its state into the next frame, the current frame exhausts its usefulness.\nIn the case of the factorial function above, this means that in the tail-recursive implementation the stack is not filled up with as many frames of recursive factorial calls as the input (\\(n\\)) is big. There are still \\(n\\) total calls, however the memory used in the stack is held constant (at a single frame in this case) as each old frame gives way to the new one.\nSo, for now, let’s define a magic function called go(n,acc) with inputs n and what’s called an accumulator acc such that factorial(n) := go(n,1). We take this to be by construction. The function go will be the tail-recursive helper of factorial. The accumulator acc, which is initialized to 1, will be used to remember the state inside the current stack frame (in this case just the multiplicative factor before the recursive call).\nBut so far we’ve only given go(n, acc) its desired properties without actually defining it. The following is the tail-recursive version of factorial which includes the definition of go.\nTail-Recursive Factorial:\n\ndef factorial(n):\n    def go(n,acc): # Helper function with an accumulator\n        if n == 0: # Base case: 0! = 1\n            return acc\n        else:\n            return go(n-1, n * acc) # Tail-recusrive step\n        \n    return go(n,1) # Delegate the problem solution to a helper function\n\nfactorial(4)\n\n24\n\n\nLet’s unpack this:\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = go(4,1) \\\\\n        & = go(3,4) \\\\\n        & = go(2,12) \\\\\n        & = go(1,24) \\\\\n        & = go(0,24) \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nRight away we can see that, with this approach, we can pop the previous stack frame at any time without losing any information it holds because all state is carried over from the previous frame into the current one by the accumulator and, finally, returned at the end. A visual cue of this fact is that in the expression above evaluation happens immediately, rather than step-by-step (with each step corresponding to the popping of a stack frame), as is the case in the naive implementation.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIt’s important to note that this effort only pays off if the language compiler in question supports TCO (Tail Call Optimization). Most, in fact, do. If the language supports TCO the compiler can recognize tail calls and simply pop the current stack frame after the recursive call, replacing it with the subsequent call (rather than blindly stacking frames on top of each other as in the naive recursive algorithm)"
  },
  {
    "objectID": "unpublished_posts/python/ny_housing_market_analysis.html",
    "href": "unpublished_posts/python/ny_housing_market_analysis.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\nfile_path = \"C://Users/vapog/Downloads/airbnb_data.csv\"\n\n# Examining the dataframe\ndf = pd.read_csv(file_path)\nprint(df.head(10))\n\n     id                                              name  host_id  \\\n0  2539                Clean & quiet apt home by the park     2787   \n1  2595                             Skylit Midtown Castle     2845   \n2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n3  3831                   Cozy Entire Floor of Brownstone     4869   \n4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n5  5099         Large Cozy 1 BR Apartment In Midtown East     7322   \n6  5121                                   BlissArtsSpace!     7356   \n7  5178                  Large Furnished Room Near B'way      8967   \n8  5203                Cozy Clean Guest Room - Family Apt     7490   \n9  5238                Cute & Cozy Lower East Side 1 bdrm     7549   \n\n     host_name neighbourhood_group       neighbourhood  latitude  longitude  \\\n0         John            Brooklyn          Kensington  40.64749  -73.97237   \n1     Jennifer           Manhattan             Midtown  40.75362  -73.98377   \n2    Elisabeth           Manhattan              Harlem  40.80902  -73.94190   \n3  LisaRoxanne            Brooklyn        Clinton Hill  40.68514  -73.95976   \n4        Laura           Manhattan         East Harlem  40.79851  -73.94399   \n5        Chris           Manhattan         Murray Hill  40.74767  -73.97500   \n6        Garon            Brooklyn  Bedford-Stuyvesant  40.68688  -73.95596   \n7     Shunichi           Manhattan      Hell's Kitchen  40.76489  -73.98493   \n8    MaryEllen           Manhattan     Upper West Side  40.80178  -73.96723   \n9          Ben           Manhattan           Chinatown  40.71344  -73.99037   \n\n         room_type  price  minimum_nights  number_of_reviews last_review  \\\n0     Private room    149               1                  9  2018-10-19   \n1  Entire home/apt    225               1                 45  2019-05-21   \n2     Private room    150               3                  0         NaN   \n3  Entire home/apt     89               1                270  2019-07-05   \n4  Entire home/apt     80              10                  9  2018-11-19   \n5  Entire home/apt    200               3                 74  2019-06-22   \n6     Private room     60              45                 49  2017-10-05   \n7     Private room     79               2                430  2019-06-24   \n8     Private room     79               2                118  2017-07-21   \n9  Entire home/apt    150               1                160  2019-06-09   \n\n   reviews_per_month  calculated_host_listings_count  availability_365  \n0               0.21                               6               365  \n1               0.38                               2               355  \n2                NaN                               1               365  \n3               4.64                               1               194  \n4               0.10                               1                 0  \n5               0.59                               1               129  \n6               0.40                               1                 0  \n7               3.47                               1               220  \n8               0.99                               1                 0  \n9               1.33                               4               188  \n\n\n\n# Number of rentals by New York borough\nbronx_df = df[df['neighbourhood_group'] == 'Bronx']\nprint(\"Bronx: # of rentals \",bronx_df.shape[0])\nbrooklyn_df = df[df['neighbourhood_group'] == 'Brooklyn']\nprint(\"Brooklyn: # of rentals \",brooklyn_df.shape[0])\nmanhattan_df = df[df['neighbourhood_group'] == 'Manhattan']\nprint(\"Manhattan: # of rentals \",manhattan_df.shape[0])\nstaten_island_df = df[df['neighbourhood_group'] == 'Staten Island']\nprint(\"Staten Island: # of rentals \",staten_island_df.shape[0])\n\nBronx: # of rentals  1091\nBrooklyn: # of rentals  20104\nManhattan: # of rentals  21661\nStaten Island: # of rentals  373\n\n\n\n# Most popular neighborhood by number of reviews\nsorted_df = df.sort_values(by='number_of_reviews', ascending=False)\nlocations_and_num_reviews_df = sorted_df[['neighbourhood','number_of_reviews']]\ntop_20_df = locations_and_num_reviews_df.head(20)\n\n# Group by neighborhood and take aggregate mean\ngrouped_df = top_20_df.groupby('neighbourhood')['number_of_reviews'].mean()\nprint(grouped_df)\n\nneighbourhood\nAstoria            441.0\nBushwick           480.0\nEast Elmhurst      485.2\nEast Village       451.0\nFlushing           474.0\nHarlem             564.0\nJamaica            553.0\nLower East Side    540.0\nPark Slope         488.0\nSouth Slope        467.0\nTribeca            447.0\nName: number_of_reviews, dtype: float64\n\n\n\n# Filtering for a client by price in Manhattan's Upper East Side\nupper_east_df = df[df['neighbourhood'] == 'Upper East Side']\nninetieth_percentile = np.quantile(upper_east_df['number_of_reviews'], 0.85) # Normalizes the results by getting rid of bad postings\nupper_east_df = upper_east_df[upper_east_df['number_of_reviews'] &gt;= ninetieth_percentile]\n\n\n## Used to examine the unique room types available\nprint(df['room_type'].unique())\n\n## Cheapest private room type rental in Manhattan's Upper East Side\nprivate_rooms_df = upper_east_df[upper_east_df['room_type'] == 'Private room']\nprivate_rooms_df = private_rooms_df.sort_values('price', ascending=True)\nprivate_rooms_df = private_rooms_df[['neighbourhood','room_type','price']]\nprint(private_rooms_df.head(5))\n\n## Cheapest entire homes/appartments\nentire_homes_df = upper_east_df[upper_east_df['room_type'] == 'Entire home/apt']\nentire_homes_df = entire_homes_df.sort_values('price', ascending=True)\nentire_homes_df = entire_homes_df[['neighbourhood','room_type','price']]\nprint(entire_homes_df.head(5))\n\n['Private room' 'Entire home/apt' 'Shared room']\n         neighbourhood     room_type  price\n8416   Upper East Side  Private room     49\n40185  Upper East Side  Private room     50\n35976  Upper East Side  Private room     60\n21283  Upper East Side  Private room     65\n19830  Upper East Side  Private room     65\n         neighbourhood        room_type  price\n18882  Upper East Side  Entire home/apt     69\n7181   Upper East Side  Entire home/apt     75\n5759   Upper East Side  Entire home/apt     92\n22122  Upper East Side  Entire home/apt     95\n27040  Upper East Side  Entire home/apt     95"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html",
    "href": "unpublished_posts/python/introduction-to-pandas.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python’s equivalent of Excel.\nWe import Pandas into our development environment as we import any other library — using the import command.\nimport pandas\nIt’s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time.\nimport pandas as pd\nThis gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#series",
    "href": "unpublished_posts/python/introduction-to-pandas.html#series",
    "title": "Introduction to Pandas",
    "section": "Series",
    "text": "Series\nThe basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It’s a one-dimensional data structure that’s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren’t limited to the integer values \\(0,1,...n\\) — they can also be descriptive labels.\nLet’s create a Series object representing the populations of the G-7 countries in units of millions.\n\n\nCode\nimport pandas as pd\ng7_pop = pd.Series([35,63,80,60,127,64,318])\n\n\nAs we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor.\n\nIndexing\nIndexing a Series object is similar to indexing a Python list. For instance, let’s print the first element in the above series.\n\n\nCode\ng7_pop[0]\n\n\n35\n\n\nLet’s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten.\n\n\nCode\ng7_pop.index = [\n    'Canada',\n    'France',\n    'Germany',\n    'Italy',\n    'Japan',\n    'UK',\n    'US'\n]\n\n\nNow, we can print the first element of the series using its descriptive label.\n\n\nCode\ng7_pop['Canada']\n\n\n35\n\n\nWe may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it’s possible to create a labeled Series object directly from a Python dictionary.\n\n\nCode\ng7_pop = pd.Series({\n    'Canada' : 35,\n    'France' : 63,\n    'Germany' : 80,\n    'Italy' : 60,\n    'Japan' : 127,\n    'UK' : 64,\n    'US' : 318\n})\ng7_pop\n\n\nCanada      35\nFrance      63\nGermany     80\nItaly       60\nJapan      127\nUK          64\nUS         318\ndtype: int64\n\n\nIn the event of overriding the integer-based indices, it’s still possible to access the elements of a Series sequentially using the iloc method (short for “integer location”). To retrieve the population of Canada, we can do as follows:\n\n\nCode\ng7_pop.iloc[0]\n\n\n35\n\n\nIt’s also possible to use a range when indexing. For instance, suppose we’d like to retrieve the populations of the first three countries from g7_pop. We can simply write:\n\n\nCode\ng7_pop['Canada':'Germany']\n\n\nCanada     35\nFrance     63\nGermany    80\ndtype: int64\n\n\nOr equivalently:\n\n\nCode\ng7_pop.iloc[0:3]\n\n\nCanada     35\nFrance     63\nGermany    80\ndtype: int64\n\n\nSince the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask.\nFor instance, to print the populations of Canada and Germany at the same time, we can pass in the list ['Canada','Germany'] or the Boolean mask [True, False, True, False, False, False, False].\n\n\nCode\ng7_pop[['Canada','Germany']]\n\n\nCanada     35\nGermany    80\ndtype: int64\n\n\n\n\nCode\ng7_pop[[True, False, True, False, False, False, False]]\n\n\nCanada     35\nGermany    80\ndtype: int64\n\n\n\n\nBroadcasted and Vectorized Operations\nSince it’s based on a NumPy array, a Series object also supports vectorization and broadcasted operations.\nAs a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood.\nBroadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. It’s an overloading of arithmetic and Boolean operators.\nFor instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row (or using a list comprehension), we can simply use broadcasted addition.\n\n\nCode\ng7_2030_pop = g7_pop + 10\ng7_2030_pop\n\n\nCanada      45\nFrance      73\nGermany     90\nItaly       70\nJapan      137\nUK          74\nUS         328\ndtype: int64\n\n\n\n\nFiltering\nThanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it’s possible to write concise and readable filtering expressions on Series.\nFor instance, let’s return the list of countries with a population over 70 mln.\n\n\nCode\ng7_pop[g7_pop &gt;= 70]\n\n\nGermany     80\nJapan      127\nUS         318\ndtype: int64\n\n\nThe expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask.\nAs another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population.\n\n\nCode\ng7_pop[g7_pop &gt;= g7_pop.mean()]\n\n\nJapan    127\nUS       318\ndtype: int64"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#dataframe",
    "href": "unpublished_posts/python/introduction-to-pandas.html#dataframe",
    "title": "Introduction to Pandas",
    "section": "DataFrame",
    "text": "DataFrame\nEach DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself.\nThe DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary.\nIf we’re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object.\n\n\nCode\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndf = pd.DataFrame(data, index = ['R1','R2','R3'], columns = ['C1', 'C2', 'C3'])\ndf\n\n\n\n\n\n\n\n\n\nC1\nC2\nC3\n\n\n\n\nR1\n1\n2\n3\n\n\nR2\n4\n5\n6\n\n\nR3\n7\n8\n9\n\n\n\n\n\n\n\nWe may bypass specifying columns manually by passing in a dictionary instead:\n\n\nCode\ndata = {\n    'C1' : [1, 2, 3],\n    'C2' : [4, 5, 6],\n    'C3' : [7, 8, 9]\n}\ndf = pd.DataFrame(data, index = ['R1','R2','R3'])\ndf\n\n\n\n\n\n\n\n\n\nC1\nC2\nC3\n\n\n\n\nR1\n1\n4\n7\n\n\nR2\n2\n5\n8\n\n\nR3\n3\n6\n9\n\n\n\n\n\n\n\n\nNote: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they’re the column labels.\n\nIn practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so:\n\n\nCode\n#hide-output\ncsv_path = 'file.csv' #Stores the path to a CSV\ndf = pd.read_csv(csv_path)\n\n\n\nTip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header.\n\n\n\nTip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method.\n\n\nCommon Methods\nHere are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up.\n\ndf.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display.\ndf.tail() shows the last 5 rows, and also accepts an integer argument.\ndf.info() gives a bird’s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types.\ndf.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values.\ndf.shape - returns the dimension of the dataset as an \\((m,n)\\) tuple.\n\n\n\nIndexing\nLet’s add to the dataset of G-7 countries the columns 'GDP' and 'Surface Area'.\n\n\nCode\ng7_df = pd.DataFrame({\n    'Population' : g7_pop,\n    'GDP' : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7],\n    'Surface Area' : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0]\n})\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nWhereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected.\n\n\nCode\ng7_df['GDP']\n\n\nCanada     1.7\nFrance     2.8\nGermany    3.8\nItaly      2.1\nJapan      4.6\nUK         2.9\nUS         1.7\nName: GDP, dtype: float64\n\n\nMulti-indexing also works in the familiar way:\n\n\nCode\ng7_df[['Population','GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\nItaly\n60\n2.1\n\n\nJapan\n127\n4.6\n\n\nUK\n64\n2.9\n\n\nUS\n318\n1.7\n\n\n\n\n\n\n\nIf we want to index rows, however, we must use the loc or iloc methods.\nFor instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so:\n\n\nCode\ng7_df.loc['Canada':'Germany']\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\n\n\n\n\n\nIf we’re only interested in the population and GDP of the first three countries, then we could instead do the following:\n\n\nCode\ng7_df[['Population', 'GDP']].loc['Canada':'Germany']\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\n\n\n\n\n\nNoting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows:\n\n\nCode\ng7_df.loc['Canada':'Germany', ['Population', 'GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\n\n\n\n\n\n\n\nFiltering\nSince the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows:\n\n\nCode\ng7_df.loc[g7_df['Population'] &gt;= 70, 'GDP']\n\n\nGermany    3.8\nJapan      4.6\nUS         1.7\nName: GDP, dtype: float64\n\n\nAs additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers.\n\n\nCode\ng7_df.loc[g7_df['Surface Area'] &lt;= 1.0, ['Population','GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\nItaly\n60\n2.1\n\n\nJapan\n127\n4.6\n\n\nUK\n64\n2.9\n\n\n\n\n\n\n\n\n\nAdding, Dropping, and Renaming Columns and Rows\nLet’s add a 'Languages' column to g7_df. We simply follow the same syntax as adding a key to a dictionary…\n\n\nCode\n# A DataFrame row is a series, so first we define one...\nlanguages = pd.Series({\n    'Canada' : 'French, English',\n    'France' : 'French',\n    'Germany' : 'German',\n    'Italy' : 'Italian',\n    'Japan' : 'Japanese',\n    'UK' : 'English',\n    'US' : 'English'\n})\n# Next, we add the series as a column to the DataFrame\ng7_df['Languages'] = languages\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nLanguages\n\n\n\n\nCanada\n35\n1.7\n9.0\nFrench, English\n\n\nFrance\n63\n2.8\n0.6\nFrench\n\n\nGermany\n80\n3.8\n0.3\nGerman\n\n\nItaly\n60\n2.1\n0.3\nItalian\n\n\nJapan\n127\n4.6\n0.3\nJapanese\n\n\nUK\n64\n2.9\n0.2\nEnglish\n\n\nUS\n318\n1.7\n9.0\nEnglish\n\n\n\n\n\n\n\nWe can also drop a column or a row using the drop() method.\nLet’s drop the newly created 'Languages' column. To drop a column, we specify a columns argument in the drop() method like so:\n\n\nCode\ng7_df.drop(columns = 'Languages')\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nThe method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let’s print it:\n\n\nCode\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nLanguages\n\n\n\n\nCanada\n35\n1.7\n9.0\nFrench, English\n\n\nFrance\n63\n2.8\n0.6\nFrench\n\n\nGermany\n80\n3.8\n0.3\nGerman\n\n\nItaly\n60\n2.1\n0.3\nItalian\n\n\nJapan\n127\n4.6\n0.3\nJapanese\n\n\nUK\n64\n2.9\n0.2\nEnglish\n\n\nUS\n318\n1.7\n9.0\nEnglish\n\n\n\n\n\n\n\nAs we can see the 'Languages' column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place.\n\n\nCode\ng7_df.drop(columns = 'Languages', inplace = True)\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nIn order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset.\n\n\nCode\ng7_df.drop(index = ['Canada', 'Italy'])\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nIt is also possible to rename a column or a row using the rename() method.\nSuppose we’d like to rename the columns to include the units of measurement, and suppose we’d also like to expand the UK and US to their full names.\n\n\nCode\ng7_df.rename(\n    columns = {\n        'Population' : 'Population (mln)',\n        'GDP' : 'GDP (USD)',\n        'Surface Area' : 'Surface Area (mln sq. km)'\n    },\n    index = {\n        'UK' : 'United Kingdom',\n        'US' : 'United States'\n    }\n)\n\n\n\n\n\n\n\n\n\nPopulation (mln)\nGDP (USD)\nSurface Area (mln sq. km)\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUnited Kingdom\n64\n2.9\n0.2\n\n\nUnited States\n318\n1.7\n9.0\n\n\n\n\n\n\n\n\n\nManipulating Columns\nThose of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations.\nFor instance, suppose we’d like to add a new 'GDP Per Capita' column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population.\nUsing broadcasted division, the code is simply:\n\n\nCode\ng7_df['GDP Per Capita'] = g7_df['GDP'] / g7_df['Population']\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nGDP Per Capita\n\n\n\n\nCanada\n35\n1.7\n9.0\n0.048571\n\n\nFrance\n63\n2.8\n0.6\n0.044444\n\n\nGermany\n80\n3.8\n0.3\n0.047500\n\n\nItaly\n60\n2.1\n0.3\n0.035000\n\n\nJapan\n127\n4.6\n0.3\n0.036220\n\n\nUK\n64\n2.9\n0.2\n0.045312\n\n\nUS\n318\n1.7\n9.0\n0.005346\n\n\n\n\n\n\n\n\n\nWorked Example - Bitcoin Price Timeseries: Cleaning and Reindexing\nSometimes we may wish to use a certain column to index a DataFrame. For instance, if we’re working with a dataset of Bitcoin prices, it would be wise to use the 'Time' column as the index so that we can easily access the value of Bitcoin at a given time.\n\nNote: Data that’s indexed by time is called a timeseries…\n\nFor this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below.\n\n\nCode\n#collapse-hide\n\nimport requests\nimport json\n\n# Specifying request URL, payload, and headers\nurl = 'https://api.coincap.io/v2/assets/bitcoin/history?interval=d1'\npayload = {}\nheaders = {}\n# Making the request and parsing it as JSON\nresponse = requests.request('GET', url, headers = headers, data = payload)\njson_data = json.loads(response.text)['data']\n# Converting the result into a DataFrame\nbitcoin_df = pd.json_normalize(json_data)\n# Cleanup\nbitcoin_df.rename(\n    columns = {\n        'priceUsd' : 'priceInUSD',\n        'time' : 'Time',\n        'date' : 'Date'\n    },\n    inplace = True\n)\n\n\nThe result of this is the following DataFrame:\n\n\nCode\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nTime\nDate\n\n\n\n\n0\n37480.8939504110899664\n1610668800000\n2021-01-15T00:00:00.000Z\n\n\n1\n36853.8623471143090244\n1610755200000\n2021-01-16T00:00:00.000Z\n\n\n2\n35670.6623897365179078\n1610841600000\n2021-01-17T00:00:00.000Z\n\n\n3\n36061.4760792230247237\n1610928000000\n2021-01-18T00:00:00.000Z\n\n\n4\n36868.3293610208260669\n1611014400000\n2021-01-19T00:00:00.000Z\n\n\n\n\n\n\n\nNow that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work.\nFirst order of business is to re-index the dataset based on the 'Time' column. We can set a column as index using the set_index() method in the following way:\n\n\nCode\nbitcoin_df.set_index('Time', inplace=True)\nbitcoin_df.index.name = None # The index column shouldn't have a name — this removes the name 'Time'\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nDate\n\n\n\n\n1610668800000\n37480.8939504110899664\n2021-01-15T00:00:00.000Z\n\n\n1610755200000\n36853.8623471143090244\n2021-01-16T00:00:00.000Z\n\n\n1610841600000\n35670.6623897365179078\n2021-01-17T00:00:00.000Z\n\n\n1610928000000\n36061.4760792230247237\n2021-01-18T00:00:00.000Z\n\n\n1611014400000\n36868.3293610208260669\n2021-01-19T00:00:00.000Z\n\n\n\n\n\n\n\nAs we can see the column that was previously named 'Time' now acts as index.\nNext, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. While doing this, let’s also convert the entries of the 'Date' column which are currently in string format.\n\n\nCode\nbitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit='us') # Converting index to datetime from Unix seconds\nbitcoin_df['Date'] = pd.to_datetime(bitcoin_df['Date']) # Converting 'Date' to datetime from string\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nDate\n\n\n\n\n2021-01-15\n37480.8939504110899664\n2021-01-15 00:00:00+00:00\n\n\n2021-01-16\n36853.8623471143090244\n2021-01-16 00:00:00+00:00\n\n\n2021-01-17\n35670.6623897365179078\n2021-01-17 00:00:00+00:00\n\n\n2021-01-18\n36061.4760792230247237\n2021-01-18 00:00:00+00:00\n\n\n2021-01-19\n36868.3293610208260669\n2021-01-19 00:00:00+00:00\n\n\n\n\n\n\n\nNow we can comfortably access the price of Bitcoin on any given day. Suppose we’d like to know its price on 2021-12-28, the day of writing this post… We can simply do:\n\n\nCode\nbitcoin_df.loc['2021-12-28', 'priceInUSD']\n\n\n'48995.0145281203441155'"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#finding-the-unique-elements-in-a-column",
    "href": "unpublished_posts/python/introduction-to-pandas.html#finding-the-unique-elements-in-a-column",
    "title": "Introduction to Pandas",
    "section": "Finding the Unique Elements in a Column",
    "text": "Finding the Unique Elements in a Column\nPandas comes with the method unique() which can be applied to a Series object.\nLet’s fetch some data about the planets in our solar system from the devstronomy repository.\n\n\nCode\nplanets_df = pd.read_csv('https://raw.githubusercontent.com/devstronomy/nasa-data-scraper/master/data/csv/planets.csv')\nplanets_df\n\n\n\n\n\n\n\n\n\nplanet\nmass\ndiameter\ndensity\ngravity\nescape_velocity\nrotation_period\nlength_of_day\ndistance_from_sun\nperihelion\n...\norbital_period\norbital_velocity\norbital_inclination\norbital_eccentricity\nobliquity_to_orbit\nmean_temperature\nsurface_pressure\nnumber_of_moons\nhas_ring_system\nhas_global_magnetic_field\n\n\n\n\n0\nMercury\n0.3300\n4879\n5427\n3.7\n4.3\n1407.6\n4222.6\n57.9\n46.0\n...\n88.0\n47.4\n7.0\n0.205\n0.034\n167\n0\n0\nNo\nYes\n\n\n1\nVenus\n4.8700\n12104\n5243\n8.9\n10.4\n-5832.5\n2802.0\n108.2\n107.5\n...\n224.7\n35.0\n3.4\n0.007\n177.400\n464\n92\n0\nNo\nNo\n\n\n2\nEarth\n5.9700\n12756\n5514\n9.8\n11.2\n23.9\n24.0\n149.6\n147.1\n...\n365.2\n29.8\n0.0\n0.017\n23.400\n15\n1\n1\nNo\nYes\n\n\n3\nMars\n0.6420\n6792\n3933\n3.7\n5.0\n24.6\n24.7\n227.9\n206.6\n...\n687.0\n24.1\n1.9\n0.094\n25.200\n-65\n0.01\n2\nNo\nNo\n\n\n4\nJupiter\n1898.0000\n142984\n1326\n23.1\n59.5\n9.9\n9.9\n778.6\n740.5\n...\n4331.0\n13.1\n1.3\n0.049\n3.100\n-110\nUnknown*\n79\nYes\nYes\n\n\n5\nSaturn\n568.0000\n120536\n687\n9.0\n35.5\n10.7\n10.7\n1433.5\n1352.6\n...\n10747.0\n9.7\n2.5\n0.057\n26.700\n-140\nUnknown*\n62\nYes\nYes\n\n\n6\nUranus\n86.8000\n51118\n1271\n8.7\n21.3\n-17.2\n17.2\n2872.5\n2741.3\n...\n30589.0\n6.8\n0.8\n0.046\n97.800\n-195\nUnknown*\n27\nYes\nYes\n\n\n7\nNeptune\n102.0000\n49528\n1638\n11.0\n23.5\n16.1\n16.1\n4495.1\n4444.5\n...\n59800.0\n5.4\n1.8\n0.011\n28.300\n-200\nUnknown*\n14\nYes\nYes\n\n\n8\nPluto\n0.0146\n2370\n2095\n0.7\n1.3\n-153.3\n153.3\n5906.4\n4436.8\n...\n90560.0\n4.7\n17.2\n0.244\n122.500\n-225\n0.00001\n5\nNo\nUnknown\n\n\n\n\n9 rows × 21 columns\n\n\n\nIf we want to find out the unique number of moons each planet has, we can simply do:\n\n\nCode\nplanets_df['number_of_moons'].unique()\n\n\narray([ 0,  1,  2, 79, 62, 27, 14,  5], dtype=int64)\n\n\nAs we can see, the 9 planets in our solar system (counting Pluto) have 8 unique number of moons. This is because, as we can see from the dataset, Mercury and Venus both have 0 moons."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#saving-data",
    "href": "unpublished_posts/python/introduction-to-pandas.html#saving-data",
    "title": "Introduction to Pandas",
    "section": "Saving Data",
    "text": "Saving Data\nAfter all the data manipulation, it would be useful to save the resulting dataset locally on our machine. Pandas offers us a way to do that using the DataFrame.to_csv() method.\nWorking with the planets_df defined above, we can narrow the dataset down to the planets which have a gravitational force that’s close to that of the Earth (\\(9.8  \\ m/s^2\\)).\n\n\nCode\nearthlike_planets_df = planets_df[(planets_df['gravity'] &gt;= 9.8 - 1) & (planets_df['gravity'] &lt;= 9.8 + 1)]\nearthlike_planets_df\n\n\n\n\n\n\n\n\n\nplanet\nmass\ndiameter\ndensity\ngravity\nescape_velocity\nrotation_period\nlength_of_day\ndistance_from_sun\nperihelion\n...\norbital_period\norbital_velocity\norbital_inclination\norbital_eccentricity\nobliquity_to_orbit\nmean_temperature\nsurface_pressure\nnumber_of_moons\nhas_ring_system\nhas_global_magnetic_field\n\n\n\n\n1\nVenus\n4.87\n12104\n5243\n8.9\n10.4\n-5832.5\n2802.0\n108.2\n107.5\n...\n224.7\n35.0\n3.4\n0.007\n177.4\n464\n92\n0\nNo\nNo\n\n\n2\nEarth\n5.97\n12756\n5514\n9.8\n11.2\n23.9\n24.0\n149.6\n147.1\n...\n365.2\n29.8\n0.0\n0.017\n23.4\n15\n1\n1\nNo\nYes\n\n\n5\nSaturn\n568.00\n120536\n687\n9.0\n35.5\n10.7\n10.7\n1433.5\n1352.6\n...\n10747.0\n9.7\n2.5\n0.057\n26.7\n-140\nUnknown*\n62\nYes\nYes\n\n\n\n\n3 rows × 21 columns\n\n\n\n\nTip: Pandas prefers the use of bitwise Boolean operators & and |, instead of the Python’s default and and or. This is because Pandas relies on NumPy, which in turn relies on the capacity of the bitwise operators to be overloaded.\n\n\nWe can now save this new dataset to our desktop as follows:\n\n\nCode\nearthlike_planets_df.to_csv('C:/Users/Vahram/Desktop/earthlike_planets.csv')"
  },
  {
    "objectID": "unpublished_posts/game_development/conways_game_of_life.html",
    "href": "unpublished_posts/game_development/conways_game_of_life.html",
    "title": "Conway’s Game of Life - First Scala Project",
    "section": "",
    "text": "Conway’s Game of Life\nConway’s Game of Life is a zero-player game on a two dimensional grid of cells with 4 simple rules:\n\nAny live cell with &lt; 2 live neighbors dies (as if by underpopulation)\nAny live cell with 2-3 live neighbors lives on to the next generation\nAny live cell with &gt; 3 live neighbors dies (as if by overpopulation)\nAny dead cell with exactly 3 live cells becomes alive (as if by reproduction)\n\nThese rules are inteded to loosely model the mechanisms of reproduction in evolutionary biology. Interestingly, these simple rules result in a highly complex world in which perpetual patterns (such as gliders) can be used to transmit information over long distances and execute computational tasks by coming together in specific arrangements to form logic gates. To see this in action, check out this cool post by Nicholas Carlini. The existence of logic gates in Conway’s Game of Life makes it possible to program within the game, leading to incredible projects like Life in Life, where Conway’s Game of Life runs itself.\nFor the sake of practice, we will use functional programming to implement the game. Functional programming draws inspiration from the mathematical definition of a function – a well-defined operation on sets. For more information see the this post on the topic of FP."
  },
  {
    "objectID": "unpublished_posts/instructibles/retroarch_setup.html",
    "href": "unpublished_posts/instructibles/retroarch_setup.html",
    "title": "RetroArch Setup",
    "section": "",
    "text": "RetroArch is a front end for a number of game console emulators that brings all of your video game titles, potentially across different systems, into one centralized UI. According to the RetroArch website itself, “settings are also unified, so configuration is done once and for all,” and “…you are able to run original game discs (CDs) from RetroArch.” Time to find all of those original PS1 discs laying around the house…\n\n\nThis is a Tom Raider Game Engine for RetroArch. Check out the project here."
  },
  {
    "objectID": "unpublished_posts/instructibles/retroarch_setup.html#openlara",
    "href": "unpublished_posts/instructibles/retroarch_setup.html#openlara",
    "title": "RetroArch Setup",
    "section": "",
    "text": "This is a Tom Raider Game Engine for RetroArch. Check out the project here."
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "Secure Sockets Layer (SSL) and its successor Transport Layer Security (TLS) are cryptographic protocols that provide security in communication over a computer network using a combination of symmetric and asymmetric encryption (both of which are introduced later in this post). The protocol is widely used for such applications as HTTPS (HTTP protocol extended with TLS encryption), email, instant messaging, etc.\n\n\n\n\n\n\n\n\n\n\n\n(a) Private and public keys\n\n\n\n\n\nFigure 1: Each party has a set of public and private keys.\n\n\n\n\n\n\n\n\n\nSSL figure 1\n\n\n\n\n\n\n\n\nSSL figure 3\n\n\n\n\n\n\n\n\nSSL figure w"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "(a) Private and public keys\n\n\n\n\n\nFigure 1: Each party has a set of public and private keys."
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure 1"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure 3"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure w"
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html",
    "href": "unpublished_posts/neuroscience_notes/grief.html",
    "title": "Nature of Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "href": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "title": "Nature of Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "unpublished_posts/software_engineering/build-a-ci-cd-pipeline.html",
    "href": "unpublished_posts/software_engineering/build-a-ci-cd-pipeline.html",
    "title": "Build a CI/CD Pipeline using Github Actions and Terraform",
    "section": "",
    "text": "Question 1\nHow can I write a pipeline that deploys AWS infrastructure using either Terraform or Ansible? Do I need other tools? If so, which?\n\n\nAnswer 1\nTo create a pipeline that deploys AWS infrastructure using Terraform, you’ll need to set up a Continuous Integration and Continuous Deployment (CI/CD) process. Here’s how you can do it and the tools you’ll need.\n\n\n1. Use Terraform for Infrastructure Deployment\nTerraform is an open-source Infrastructure as Code (IaC) tool that allows you to define and provision infrastructure using code. It is well-suited for deploying AWS resources.\n\n\n2. Set Up a Version Control System (VCS)\nUse a VCS like Git to manage your Terraform code. Host your repositories on platforms such as:\n\nGitHub\nGitLab\nBitbucket\n\n\n\n3. Choose a CI/CD Tool\nSelect a CI/CD tool to automate your deployment pipeline. Popular options include:\n\nJenkins\nGitHub Actions\nGitLab CI/CD\nCircleCI\nAWS CodePipeline\n\nFor seamless AWS integration, AWS CodePipeline is a good choice.\n\n\n4. Configure AWS Credentials\nSecurely manage AWS credentials required for deployment:\n\nUse AWS Identity and Access Management (IAM) roles and users.\nStore credentials securely using environment variables or the CI/CD tool’s secret management features.\nNever hardcode credentials in your code.\n\n\n\n5. Set Up Remote State Management\nTerraform uses state files to keep track of resource mappings. Store the state remotely:\n\nUse an S3 bucket with DynamoDB for state locking.\nAlternatively, use Terraform Cloud or Terraform Enterprise.\n\n\n\n6. Build the Pipeline\n\na. Write Terraform Configuration\nDefine your AWS infrastructure in .tf files:\n# provider.tf\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\n# main.tf\nresource \"aws_s3_bucket\" \"my_bucket\" {\n  bucket = \"my-example-bucket\"\n  acl    = \"private\"\n}\n\n\nb. Initialize Git Repository\nInitialize a Git repository and commit your Terraform code:\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main\n\n\nc. Configure the CI/CD Pipeline\n\nOption 1: Using GitHub Actions\nCreate a workflow file .github/workflows/deploy.yml:\nname: Terraform Deploy\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout Code\n      uses: actions/checkout@v3\n\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v2\n\n    - name: Terraform Init\n      run: terraform init\n\n    - name: Terraform Apply\n      run: terraform apply -auto-approve\n      env:\n        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\nStore AWS Credentials: Add AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to your repository’s secrets.\n\n\n\nOption 2: Using AWS CodePipeline\n\nCreate a Pipeline: Set up AWS CodePipeline with stages for source, build, and deploy.\nUse CodeBuild: Configure AWS CodeBuild to run Terraform commands.\n\n\n\n\nd. Securely Manage State\nConfigure Terraform to use remote state:\n# backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state-bucket\"\n    key            = \"state/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-state-lock\"\n    encrypt        = true\n  }\n}\n\n\ne. Test the Pipeline\n\nCommit and Push: Your CI/CD tool will trigger the pipeline on new commits.\nReview Outputs: Monitor the pipeline for successful deployment.\n\n\n\n\n7. Additional Tools\n\nTerraform Modules: Reusable modules for organizing code.\nTerragrunt: Enhances Terraform with wrappers for state and configuration.\nLinting and Formatting:\n\nUse terraform fmt for formatting.\nUse tflint for linting your code.\n\nSecurity Scanning:\n\nIntegrate tools like Checkov or TFSec to scan for security issues.\n\n\n\n\n8. Optional: If You Meant Ansible Instead of Angular\nAnsible is another automation tool that can manage infrastructure:\n\nUse Ansible playbooks to define AWS resources.\nRequires AWS modules and credentials setup.\nCan be integrated into a CI/CD pipeline similarly.\n\n\n\nImportant Notes\n\nAngular is a web application framework and not used for infrastructure deployment. If you intended to refer to another tool (like Ansible), consider using that instead.\nTesting: Always test your infrastructure changes in a non-production environment first.\nMonitoring: Implement monitoring and logging for your infrastructure using AWS CloudWatch or similar services.\n\n\nSummary\nTo deploy AWS infrastructure using Terraform:\n\nWrite your infrastructure code with Terraform.\nUse a VCS to manage code changes.\nSet up a CI/CD pipeline with a tool like GitHub Actions or AWS CodePipeline.\nSecurely manage AWS credentials and Terraform state.\nAutomate the deployment process through the pipeline.\nMonitor and maintain your infrastructure post-deployment.\n\nBy integrating these tools and practices, you’ll have an automated pipeline that helps you efficiently manage and deploy your AWS infrastructure.\n\n\n\nQuestion 2\n“Store AWS Credentials: Add AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to your repository’s secrets.”\nExpand on this step. Also expand on “using an S3 bucket with DynamoDB for state locking.”\n\n\nAnswer 2\nExpanding on Storing AWS Credentials in Repository Secrets\nWhen setting up your CI/CD pipeline to deploy AWS infrastructure using Terraform, it’s crucial to securely manage your AWS credentials. These credentials are necessary for Terraform to authenticate with AWS and provision resources on your behalf. Exposing these credentials in your code or configuration files poses significant security risks. Instead, store them securely using your repository’s secret management features.\n\n\n1. Storing AWS Credentials as Repository Secrets\n\na. Generate AWS Access Keys\nBefore storing credentials, you need AWS access keys with appropriate permissions:\n\nCreate an IAM User:\n\nNavigate to the AWS IAM Console.\nClick Users → Add users.\nEnter a user name (e.g., terraform-ci-user).\nSelect Programmatic access for Access type.\nClick Next: Permissions.\nAssign permissions:\n\nBest Practice: Attach policies granting least privilege necessary (e.g., custom policy or predefined policies like AmazonS3FullAccess and specific permissions for other services).\n\nComplete the steps to create the user.\n\nDownload Credentials:\n\nAfter creation, download the Access Key ID and Secret Access Key.\nImportant: Store these securely; you cannot retrieve the secret key again after this point.\n\n\n\n\nb. Store Credentials in Repository Secrets\n\nGitHub Actions\n\nNavigate to Repository Settings:\n\nGo to your GitHub repository.\nClick on the Settings tab.\nIn the left sidebar, select Secrets and variables → Actions.\n\nAdd New Secrets:\n\nClick New repository secret.\nAdd the Access Key ID:\n\nName: AWS_ACCESS_KEY_ID\nValue: Your AWS Access Key ID\n\nAdd another secret for the Secret Access Key:\n\nName: AWS_SECRET_ACCESS_KEY\nValue: Your AWS Secret Access Key\n\n\nReference Secrets in Workflow:\n\nIn your .github/workflows/deploy.yml, reference the secrets:\nenv:\n  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\n\n\n\nGitLab CI/CD\n\nNavigate to Project Settings:\n\nGo to your GitLab project.\nClick Settings → CI/CD.\nExpand the Variables section.\n\nAdd Variables:\n\nClick Add Variable.\n\nKey: AWS_ACCESS_KEY_ID\nValue: Your AWS Access Key ID\nFlags: Masked, Protected\n\nAdd another variable:\n\nKey: AWS_SECRET_ACCESS_KEY\nValue: Your AWS Secret Access Key\nFlags: Masked, Protected\n\n\nReference Variables in .gitlab-ci.yml:\nvariables:\n  AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY\n\n\n\n\nc. Security Best Practices\n\nNever Hardcode Credentials:\n\nDo not include credentials in your code or commit them to version control.\n\nUse Least Privilege:\n\nGrant only the necessary permissions to the IAM user.\n\nRotate Credentials Regularly:\n\nPeriodically update your access keys for better security.\n\nAudit and Monitor Access:\n\nUse AWS CloudTrail and other monitoring tools to keep track of usage.\n\n\n\nExpanding on Using an S3 Bucket with DynamoDB for State Locking\nTerraform uses state files to keep track of the resources it manages. Storing the state remotely allows team collaboration and provides a central source of truth. Using an S3 bucket with DynamoDB for state locking ensures that only one operation can modify the state at a time, preventing conflicts.\n\n\n\n\n2. Setting Up Remote State with S3 and DynamoDB\n\na. Create an S3 Bucket for Remote State\n\nNavigate to S3 Console:\n\nGo to the AWS S3 Console.\n\nCreate a New Bucket:\n\nClick Create bucket.\nBucket Name: Must be globally unique (e.g., my-terraform-state-bucket-1234).\nRegion: Choose the AWS region closest to your resources.\n\nConfigure Bucket Options:\n\nVersioning: Enable versioning to keep track of changes to the state file.\nEncryption: Enable server-side encryption (SSE-S3 or SSE-KMS) for security.\nBlock Public Access: Ensure all public access is blocked.\n\nReview and Create:\n\nReview settings and create the bucket.\n\n\n\n\nb. Create a DynamoDB Table for State Locking\n\nNavigate to DynamoDB Console:\n\nGo to the AWS DynamoDB Console.\n\nCreate a New Table:\n\nClick Create table.\nTable Name: e.g., terraform-lock-table.\nPartition Key: LockID (String).\nSettings: Use default settings unless you have specific requirements.\n\nCreate the Table:\n\nComplete the creation process.\n\n\n\n\nc. Configure Terraform Backend\nIn your Terraform configuration, specify the backend as follows:\n# backend.tf\n\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state-bucket-1234\"\n    key            = \"global/s3/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-lock-table\"\n    encrypt        = true\n  }\n}\n\nbucket: The name of your S3 bucket.\nkey: The path within the bucket for the state file.\nregion: AWS region for S3 and DynamoDB.\ndynamodb_table: Your DynamoDB table name.\nencrypt: Enables server-side encryption.\n\n\n\nd. Initialize Terraform with Backend\n\nRun terraform init to initialize the backend configuration.\nterraform init\nTerraform will detect the backend settings and use the S3 bucket for state storage.\n\n\n\ne. IAM Permissions\nEnsure the IAM user or role used by Terraform has the necessary permissions:\n\nS3 Permissions:\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"s3:PutObject\",\n    \"s3:GetObject\",\n    \"s3:DeleteObject\",\n    \"s3:ListBucket\"\n  ],\n  \"Resource\": [\n    \"arn:aws:s3:::my-terraform-state-bucket-1234\",\n    \"arn:aws:s3:::my-terraform-state-bucket-1234/*\"\n  ]\n}\nDynamoDB Permissions:\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"dynamodb:PutItem\",\n    \"dynamodb:GetItem\",\n    \"dynamodb:DeleteItem\",\n    \"dynamodb:UpdateItem\",\n    \"dynamodb:Scan\",\n    \"dynamodb:Query\"\n  ],\n  \"Resource\": \"arn:aws:dynamodb:us-east-1:123456789012:table/terraform-lock-table\"\n}\n\nReplace my-terraform-state-bucket-1234 with your bucket name and ensure the AWS account ID and region are correct.\n\n\nf. Benefits of Using S3 and DynamoDB\n\nRemote State Storage:\n\nCentralized state file accessible by your CI/CD pipeline and team members.\n\nState Locking with DynamoDB:\n\nPrevents simultaneous operations that could corrupt the state.\n\nVersioning and Encryption:\n\nState file history is preserved, and data is secured.\n\n\n\n\ng. Security Best Practices\n\nRestrict Access to State Files:\n\nLimit S3 bucket access to only necessary users and roles.\n\nEncrypt Sensitive Data:\n\nUse encrypt = true in the backend configuration.\n\nMonitor State Access:\n\nUse AWS CloudTrail to audit access to S3 and DynamoDB.\n\n\n\nSummary\nBy securely storing your AWS credentials in repository secrets and configuring Terraform to use an S3 bucket with DynamoDB for remote state and locking, you enhance the security and reliability of your infrastructure deployment pipeline.\n\nAWS Credentials:\n\nUse your CI/CD platform’s secret management to store credentials securely.\nReference these secrets in your pipeline configuration.\nApply security best practices for credential management.\n\nTerraform Remote State:\n\nStore the state file in an S3 bucket with versioning and encryption.\nUse a DynamoDB table for state locking to prevent concurrent modifications.\nEnsure IAM policies grant only the necessary permissions.\n\n\nFeel free to ask if you need further details on any of these steps or assistance with configuring your pipeline."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "About This Site\nHi, I’m Vahram!\nWelcome to my digital garden — a collection of my notes and ideas shared on the internet.\nThis journey began in early 2021, inspired by some college notes I had compiled using Jupyter Notebooks and various online resources I explored during my free time. Recently, I’ve been experimenting with static page generators (like Quarto) to transform my notes into a format that’s both functional and visually appealing for the web.\nI strive to clearly distinguish between original work and borrowed content, always providing proper citations where applicable. For more details on content usage and attribution, please refer to the Copyright section.\n\nLicense\nCopyright (c) 2023 Vahram Poghosyan\nThe materials on this site — including the styling, custom images, videos, code, and most written content — are protected under copyright law, unless explicitly stated otherwise (e.g., content attributed to others or in the public domain).\nIf you’d like to reuse any assets or content, feel free to reach out to me at vapogyan@gmail.com with your request. Exceptions will be readily granted for reasonable use!"
  },
  {
    "objectID": "manim-sandbox/lib/python3.9/site-packages/mapbox_earcut-1.0.2.dist-info/LICENSE.html",
    "href": "manim-sandbox/lib/python3.9/site-packages/mapbox_earcut-1.0.2.dist-info/LICENSE.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "The bindings to the earcut library are licensed under the following terms:\n\nISC License\nCopyright (c) 2018, Samuel Kogler\nPermission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.\nTHE SOFTWARE IS PROVIDED “AS IS” AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nFor the license of the actual earcut library, see the license file in the include folder."
  },
  {
    "objectID": "manim-sandbox/lib/python3.9/site-packages/numpy/random/LICENSE.html",
    "href": "manim-sandbox/lib/python3.9/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "manim-sandbox/lib/python3.9/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "manim-sandbox/lib/python3.9/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html",
    "href": "posts/functional_programming/functional_programming.html",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "",
    "text": "Functional programming draws inspiration from the mathematical definition of a function which is a well-defined operation on sets."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "href": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "Mathematical functions",
    "text": "Mathematical functions\nTake \\(f: X \\rightarrow Y\\), which is a function that maps elements of the set \\(X\\) to those of the set \\(Y\\), such that for each \\(x\\) in \\(X\\) (also denoted “\\(x \\in X\\)”), there’s one and only one \\(y \\in Y\\) that satisfies the equation \\(f(x) = y\\). In plain language we say that a mathematical function maps any given input to its own unique output (that depends only on the input). That’s not to say that \\(f\\) can’t map the two different inputs, \\(x_i\\), and \\(x_j\\), to the same output \\(y\\), but it cannot map the same input \\(x_i\\) to more than one output.\nNotice that the output in \\(Y\\) depends only on the input from set \\(X\\), and that the function \\(f\\) only operates on set \\(X\\) and nothing external to it. In other words there is no hidden state (some value outside of \\(X\\)) that affects \\(f\\)-s output, so \\(f\\) always produces predictable output. What’s more \\(f\\) doesn’t really alter any element in \\(X\\) itself (or, for that matter, in \\(Y\\)). The expression \\(f(x)\\) is simply understood as the function \\(f\\) applied to an element \\(x \\in X\\) which maps it to an element in set \\(Y\\). However, it’s not like the specific element in the set \\(X\\) is somehow retrieved (as it is sometimes, by reference, in programming) and overwritten in any way.\nThe idea behind functional programming is to bring code close to this mathematical elegance, allowing us to better reason about the systems we write."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "href": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "Pure functions and side effects",
    "text": "Pure functions and side effects\nWe can define some rules for the functions we write in our code to match the mathematical properties of functions, bridging the concrete world of mathematics with the practical world of software engineering.\nA pure function, in the FP sense, is a function which depends only on its input (and not on any other value stored elsewhere in external computer memory or other external source). A pure function affects nothing outside itself. Additionally, pure functions must output a value and that value must be unique for a given input.\nTo recap:\n\nA pure function must return a single output for a given input\nIts output should only depend on its input\nA pure function shouldn’t change any internal or external state\n\nGiven that a functional program is just a composition of pure functions, and that state changes are often what affects the real-world outcome of our function calls (more on these side-effects later), the last property effectively means that pure functions don’t mutate state at all. This is also the reason state mutation is often frowned upon, in general, in functional programming. This presents unique challenges, as you might expect, given that so many operations in the traditional paradigm of programming used within countless real-world codebases do mutate state (in fact, in some cases, they have to – for anything useful to ever happen). But some state mutation is definitely avoidable.Take, for instance, a for loop which increments its index on every iteration. We will see how functional programming languages attempt to bypass for loops, or iteration in general, in favor of function composition (syntactic sugars, in your choice of language, will become indispensable here!).\n\n“Nice” consequences of working with pure functions\nWorking with pure functions conveys some great benefits. For instance, properties (1) and (2) make pure functions interchangeable with their output (just as, say, \\(f(2)\\) given \\(f(x)=x^2\\) can reliably be substituted for the number \\(4\\) in math). This allows us to pass in pure functions as arguments into other pure functions (as well as return them as output) with entirely predictable results. If a function, by contrast, printed something to the console, along with evaluating the square, we would consider that an effectful function (and therefore it would be considered impure). Such a function cannot be reliably substituted by its output because it also affects an external state, producing an effect that the output alone does not capture. This benefit, to reliably substitute the representation of a value for the value itself, yields nice benefits. In mathematics, for example in the field of Deep Learning, we are able to cheaply compute the gradient of a loss-function using the back-propagation algorithm which is simply storing intermediate values during the forward-pass so that, during the backwards-pass (or what’s known as back-propagation), we avoid redundant calculations… But this simple substitution relies on the fact that the function evaluated at input is the same as the output. By contrast, if functions in math affected external state somewhere, or produced other such side-effects (more of which we will see in the section dedicated to Side-effects), it would be a lot more difficult to come up with the optimization known as back-propagation in ML. So, when we use pure functions, we gain mathematical insights about our programs. In effect, we’re just modeling the solution as a chain of pure function calls and storing the results as immutable state. This also imposes a certain component of horizontal eye-movement when it comes to reading functional code.\nIn the next section we look at the differences between declarative and imperative styles of writing software and why functional programming prefers the former style."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#declarative-vs-imperative-styles",
    "href": "posts/functional_programming/functional_programming.html#declarative-vs-imperative-styles",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "Declarative vs imperative styles",
    "text": "Declarative vs imperative styles\nAt a basic level, an imperative style of programming can be likened to cooking at home with a cookbook. Imperative languages look more like a list of commands directed at the computer. Declarative writing, by contrast, can be compared to dining at a restaurant. We aren’t issuing commands at a grueling level of granularity (e.g. iterating over an array manually, or appending to a list). Instead, we’re specifying the desired outcome without the implementation details like we would in mathematics when we, for instance, write \\(f(x)=x^2\\) succinctly (implying to square every feature of the input vector \\(x\\)). We prefer declarative code to imperative in FP partly because imperative code involves a lot of state mutation and partly because writing pure functions facilitates writing declarative code.\nIn the imperative style, we’re saying “step through the list, read each item, square it and append it to a new list.” In the declarative style we’re saying “just square every element of this list.” These differences are mostly semantic and, in real life, software contains a mix of both styles. The distinction is also not really black or white, and is often dependent on the implementation of the given language.\nAn example is worth a thousand words and, since Python provides a good enough playground for showcasing these styles, here is an example in Python.\nImperative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = []\nfor num in numbers:\n    squared.append(num ** 2)\n\nprint(squared)\n\n[1, 4, 9, 16, 25]\n\n\nDeclarative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = map(lambda x: x**2, numbers)\n\nprint(list(squared))\n\n[1, 4, 9, 16, 25]\n\n\nNotice how in the declarative style we merely instructed our function to square each feature, but we didn’t tell the program how to do it in grueling detail and we avoided the use of a for loop (which means we avoided mutating the state of the index of the loop). Also, more lines in the declarative style return a value, rather than just carrying our instructions (we will see the difference between mere instructions, or statements and pure expressions later on). However, since there’s printing to the console at the end, even the declarative program would not be considered functional.\n\nSide-effects\nFunctions which violate any of the three afforementioned properties are said to produce side effects (or simply effects). The most common side effect is when a function modifies a state (i.e. a chunk of computer memory) outside itself (violating property (3)). Examples of side effects include:\n\n\n\n\n\n\n\nEffect\nFunctional Programming Way\n\n\n\n\nA function directly modifying a variable defined in the global scope.\nThe FP approach is to pass the global variable as input instead, and have the function return a modified copy of the input.\n\n\nA function writing to an external database.\nThis is an example of an unavoidable side effect in practice. The FP approach is to mitigate. Specifics are language dependent, but usually the strategy involves gathering all such unavoidable side effects into one impure corner of the code, and keeping the rest of the code pure.\n\n\nA function like the built-in functions of printing to the console, retrieving system time, or a random number generator (or those functions which use them)\nYet more examples of unavoidable side effects. Such functions are inherently dependent on external or hidden state such as the time of day in the real world and, in general, things other than their input.\n\n\n\nAlthough some side effects are unavoidable, we should minimize their use in our code. Functional programming languages offer just that ability.\n\n\nInstructions (statements) vs expressions\nIn functional programming, we distinguish between mere instructions to the computer (which are also sometimes known as statements) and expressions (or pure expressions). This distinction is similar to that between functions, in the programming sense, and pure functions in the mathematical sense – Expressions, like functions, must always return a value. Contrast this with instructions like the traditional if/else statements or loops like while which control the flow of execution, but don’t evaluate to anything.\nAs mentioned earlier, such effects are unavoidable at times. However, functional languages have different strategies of mitigating these impurities and writing pure code anyway. Usually they aim to gather the impurities together at the top or bottom of the code. Some languages (such as Scala which is a blend of OOP and FP), go to great lengths to minimize side effects by enforcing the return requirement of its syntactic structures. Even though Scala has the traditional for loop as an instruction, it favors the use of for-comprehensions which are essentially syntactic sugar (enabled by monadic types that capture effects, more on these later). Each line of a for-comprehension in Scala evaluates to a value.\nThe idea is to use a clever type system to capture effects. If side effects must exist, they should be known to Scala. To achieve this, Scala has a monadic type known as Unit which can hold only () as its value. This is its designated side effect type. So, functional programming languages elevate instructions or statements, which normally don’t return anything, the status of pure expressions by returning a dedicated side effect type. In practice there are many types for different side effects (for example, an IO monadic type captures side effects produced by operations like console logging). Let’s see some examples of how Scala does away with traditional for loops and if/else statements and uses pure expressions instead.\n\n\nControl flow: conditional statements and loops\nIn Scala, if statements are implemented as expressions similar to the familiar ternary expressions in Python. Scala has traditional if statements too, but the if pure expression is what’s preferred. Here are some examples that demonstrate difference the difference in both Python and Scala:\nPython:\nx = 1 if condition == True else 0\nScala:\nval x = if (condition) 1 else 0\nIn this example, x necessarily evaluates to a value: one of possible two. This if expression will not produce a side effect as would an open-ended if statement. Inside an if statement, the programmer might just do something crazy and unheard of like accessing a database, or printing a line to the console (both considered side effects).\nThis brings us to an important point. It’s not that if statements would necessarily result in side effects, it’s just that functional programming discourages the use of language constructs that lend themselves to producing side effects more easily. Syntactic choices like this are a common theme in FP. For instance, Scala’s choice to treat () as a returnable value (of type Unit) rather than just a piece of syntax is very deliberate. Let’s see why by examining a Scala, for-comprehension.\nScala For Comprehension:\nval result = \n    for {\n        _ &lt;- print(\"Hello\")\n        _ &lt;- print(\"World!\")\n    } yield ()\nIt may not look like it, but the code snippet above (showing a for-comprehension) is one of the ways in which Scala actually chains many potentially side-effect producing operations together via function composition (which is what’s going on in the background). Notice three things about it:\n\nThe for comprehension returns a value captured by result.\nThe print statement produces a side effect which is discarded as _.\nAt the end we simply say yield ()… If we wished to return a value instead we would do so inside the () however, because Scala associates a type with (), what’s actually returned is the side-effect captured as a Unit (so the for comprehension returns something)\n\n\n\nFunction composition vs iteration and higher-order functions (HOFs)\nBecause FP frowns upon the use of if/else and for/while statements, it prefers function composition to iteration. In fact, the for-comprehension above is just cleverly disguised function composition.\nTake, for example, a while loop that runs until a key press (or any other user input). Of course, this may be an unavoidable side effect in the real world. The FP approach would, then, just be to contain this impurity somewhere with the rest of its kind.\nIn general, instead of iteration, function composition is preferred (mathematical readers will understand that recursion, which we’re used to solving problems with in CS, is just a type of function composition). Functional programming prefers this approach most of the time. This may sound tedious and almost like having to re-learn how to program, at first, but syntactic sugars and other abstractions exist to make this pattern more readable (like for-comprehension in Scala, or do-notation in Haskell). But also, it does involve thinking abour programming in new ways, and that’s very much the point!\nThere are already a few familiar examples of function composition that have been adopted by popular languages like Python, and are very intuitive (especially when dealing with applications in data pipelines). Some famous examples are the map and the filter functions in Python. We already saw an example of map in the declarative code snippet above, so we won’t dive into its specifics here. But both map and filter are examples of higher-order functions (HoFs) – functions which take other functions as input and/or themselves output other functions. Functions map and filter show that function composition can be very readable and intuitive… Furthermore, neither map nor filter modify their input in-place. Rather, they return a modified copy of the input to avoid external state mutation which is considered poor practice in FP. Later on, we shall see that a related method called flatMap exists that turns out to play a key role in allowing functional programmers to write code by chaining multiple, potentially side effect-producing, functions together. This is due to flatMap’s unique function signature (one which flattens arrays of arrays back into a one-dimensional array), as we will see.\n\nHigher-order functions\nTo pipe functions into other functions (as in function composition), we need functions that take other functions as input and can also output functions. When we treat functions this way, we basically treat them as first-class values which means like any other value, they can be passed and returned around. There is a mathematically-inspired reason, other than designing software as function composition, for using HoFs.\nWhen mathematicians write:\n\\[\n\\sum_{x=a}^{b} f(x)\n\\]\nwhere, say \\(a,b \\in \\mathcal{Z}\\), they understand that \\(f\\) stands for some general function. So there’s no need to write a separate expression for summing the integers, one for summing the squares of the integers, and one for summing the factorials of integers between \\([a,b]\\).\nLet’s write a function in Scala that sums the integers:\ndef sumInts(a: Int, b: Int): Int = \n    if a &gt; b then 0 else a + sumInts(a + 1, b)\nTo get the sum of squares, we’d need to define another function:\ndef square(x: Int): Int = x * x\n\ndef sumSquares(a: Int, b: Int): Int = \n    if a &gt; b then 0 else square(a) + sumSquares(a + 1, b)\nBut there’s clearly some repetition here, so we can factor out a common pattern. What if we changed the signature of sumInts to take a function as argument?\ndef sum(f: Int =&gt; Int, a: Int, b: Int): Int =\n    if a &gt; b then 0 else f(a) + sum(f, a + 1, b)\nNow we can write:\ndef sumInts(a: Int, b: Int) = sum(id, a, b) \nwhere id is the identity function:\ndef id(x: Int): Int = x\nand:\ndef sumSquares(a: int, b: Int) = sum(square, a, b)\nAdmittedly this doesn’t look great because we’re creating a lot of boilerplate functions. To get rid of this boilerplate, Scala was the first language to introduce the notion of anonymous functions (in Python these are known by the lambda keyword). We can think of anonymous functions as literals. Similar to how we can do println(\"hello world!\") without having to name the string literal \"hello world!\" using a variable, we can declare functions as literals. Using anonymous functions, the id and square functions above can be written respectively as:\n(x: Int): Int =&gt; x\n(x: Int): Int =&gt; x * x\nThis reduces our sumInts and sumSquares to:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b) // Types can be omitted if they can be inferred from context\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nAnonymous functions are syntactic sugar. That is, they aren’t necessary but make life easier.\n\n\n\nCurrying\nBut so far we’ve only used the HoF’s ability to accept functions as input. Let’s also use their ability to output functions. The pattern we are about to learn is called currying (after Haskell Curry), and it’s useful for, among other things, dependency injection.\nNote, again, the functions:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b)\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nBoth a and b are passed into each unchanged. Is there a common pattern we can extract? We can use currying which is just partial application of the function. Here’s the curried version of sum:\ndef sum(f: Int =&gt; Int): (Int, Int) =&gt; Int =\n    def sumFn(a: Int, b: Int): Int =\n        if a &gt; b then 0 else f(a) + sumF(a + 1, b)\n    sumF\nOur sum functions now constructs and returns a new function which takes the rest of input the (a and b). This is called partial-application. We’ve split the sum into two parts: the first part accepts only the argument f as input, the second one accepts the rest of the arguments.\nNow we can define sumInts and sumSquares respectively as just:\ndef sumInts = sum(x =&gt; x)\ndef sumSquares = sum(x =&gt; x*x)\nNote that when we call sum, we get back a function with signature (Int, Int) =&gt; Int (which is exactly the signature we want for sumInts and sumSquares). We can now use better readable syntax like sum(cube)(1,5) + sum(squares)(5,10) doing away with sumInts and sumSquares (using which we’d have to write the above as: sumInts(1,5) + sumSquares(5,10)).\nSince it can get quite clumsy to write curried functions, Scala provides a shorthand. This is equivalent to the curried sum written above.\ndef sum(f: Int =&gt; Int)(a: Int, b: Int): Int = \n    if a &gt; b then 0 else f(a) sum(f)(a + 1, b)\nIn Python, there’s support for curried functions in the functools library (functools.partial, which implements a curried version of a function you pass to it (by itself currying the input).\nCurrying, in general, can be applied \\(n\\) times to an \\(n\\)-dimensional function, each outer function returning an (anonymous) inner function which partially applies the rest of the parameters. This means languages need not have support for functions with parameters, as long as they have support for anonymous functions. In fact, the most minimal programming language, a lambda calculus, does away with parameters by relying on currying alone.\n\n\nMonads\nMonads are a term borrowed from Category Theory. They have a strict definition in mathematics, but for our purposes they’re useful black boxes that provide the following benefits (some already mentioned before in passing). Monads provide a way to compose potentially side-effect producing functions together, and in general they make function composition lend itself to being abstracted behind syntactic sugars (like do notation in Haskell) which make functional programs more readable and more useful for in an applied sense. As a sidenote, to the readers who are familiar with JavaScript Promises, these are, in essence, the same as the side-effect type Unit in Scala (or, rather, more like the more specific IO side-effect). The main point is that a Promise also a monad, which makes it possible to come up with nice, syntactic sugars like async await. In fact, do notation in Haskell is actually the generalized version of this type of async await pattern that works with any monad and not just a Promise.\nMonads, as far as we’re concerned, are abstract classes that implement flatMap (also map, also an identity map, but the big picture is lost in the details). Yes, this is the same flatMap we discussed earlier in the context of it being one of the higher-order functions that are popularly used in more mainstream languages like Python. It turns out, chaining two side-effect producing operations by function compositi produces nested side-effects (and if we’re using a language that uses types to capture side-effects, then we quickly generate some nasty, nested types). For example, chaining two IO operations that prompt the user for two strings may produce something like IO(IO(String)). So what’s flatMap’s role in all of this? Our friend, flatMap, with its unique capacity to flatten, is exactly the thing that’s needed to get an IO(String)) back! We will discuss this flattening property of flatMap in more detail later on. For now, it’s important to re-iterate that having an implementation of flatMap, which may go by other names in other languages (e.g. Bind in Haskell), is, along with a few other key properties, what qualifies an abstract class to be a monad.\nFor more on Monads, and flatMap, check out this excellently visualized blog post by Matt Gllagher.\nSo, it’s no wonder that in a functional programming language we want our side effect-producing expressions to return a monadic type, so that we can chain two or more of such expressions together (using function composition) without generating a nested mess."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#benefits-of-functional-programming",
    "href": "posts/functional_programming/functional_programming.html#benefits-of-functional-programming",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "Benefits of functional programming",
    "text": "Benefits of functional programming\n\nParallelization\nFP confers some benefit in terms of parallelization because:\n\nA common challenge in parallel programming is to avoid mutating data while another thread is using it. Due to state immutability principles in FP, this problem is eliminated\nFP avoids writing functions which rely on hidden state (i.e. any state that’s not a direct input), so functions can be executed in parallel without the concern of synchronizing access to some shared state.\nFP can make it easier to identify opportunities for parallelization\nLanguages which are built around FP have powerful parallelization libraries that offer parallelized versions of common operations like map"
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#functional-programming-hazards",
    "href": "posts/functional_programming/functional_programming.html#functional-programming-hazards",
    "title": "A Beginner’s Introduction to Concepts in Functional Programming",
    "section": "Functional programming hazards",
    "text": "Functional programming hazards\n\nTail recursion: avoiding stack overflow\nIf we’re going to favor the use of recursion (or, in general function composition) over the more imperative style of coding, we ought to tread carefully as to not cause stack overflow (which, as we know, is when the system runs out of working memory). Tail recursion optimization (similar to other techniques like memoization) helps us drastically cut the amount of stack memory used. It takes a constant amount of memory on the stack, instead of linear or worse. Read more about tail recursive optimization here."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html",
    "href": "posts/optimization/introduction_to_optimization.html",
    "title": "Notes: Introduction to Optimization",
    "section": "",
    "text": "An optimization problem can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e. minimize or maximize) some objective function. The objective function can be almost anything — cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are basically the same problem. Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\), the reflected objective function.\n\n\nThis post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, let’s define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods.\n\n\n\nFeel free to read about check out my posts about some of the more advanced concepts in optimization like linear programs and their geometry, optimization algorithms, or duality theory. For a fun problem solvable using linear programs, check out the post on modelling discrete failures in any given system."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Notes: Introduction to Optimization",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "title": "Notes: Introduction to Optimization",
    "section": "",
    "text": "First, let’s define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#where-to-go-from-here",
    "href": "posts/optimization/introduction_to_optimization.html#where-to-go-from-here",
    "title": "Notes: Introduction to Optimization",
    "section": "",
    "text": "Feel free to read about check out my posts about some of the more advanced concepts in optimization like linear programs and their geometry, optimization algorithms, or duality theory. For a fun problem solvable using linear programs, check out the post on modelling discrete failures in any given system."
  },
  {
    "objectID": "posts/optimization/duality_theory.html",
    "href": "posts/optimization/duality_theory.html",
    "title": "Notes: Duality Theory",
    "section": "",
    "text": "Every convex optimization problem, designated as the primal, has a related problem called its dual which can be colloquially thought of as its evil twin. The primal and the dual represent two different perspectives on the same problem.\nIn the most general case, if the primal is a minimization problem, its dual is a maximization problem. In the case of constrained optimization, if the primal is minimization in \\(n\\) variables and \\(m\\) constraints then its dual is a maximization in \\(m\\) variables and \\(n\\) constraints.\nFurthermore, any feasible value of the dual is a lower-bound for all feasible values of the primal. In particular, should they both exist, the dual optimum is a lower bound for the primal optimum. This property, called weak duality, lies at the core of duality theory. The utility of formulating a problem whose solution obtains, at least, a lower-bound for the primal optimum and, in the special case, the primal optimum itself should be self-evident.\nIn the best case scenario a problem exhibits a property called strong duality, which guarantees that the primal and the dual optima agree. Such problems are called strongly dual problems and include, but are not limited to, all linear programs (LPs) and a category of convex non-linear optimization problems. For strongly dual problems, solving the dual guarantees that we’ve also solved the primal. Furthermore, as we shall see, taking the dual of the dual gives back the primal. So this relationship is true in the converse — if we’ve solved the primal then we’ve also solved its dual.\nThis is what makes duality theory so useful in practice. Having a related, usually easier, optimization problem gives applied scientists a huge computational advantage. However, even if the dual does not turn out to be any easier to solve and/or strong duality fails to hold, we still stand to gain structural insight about the primal problem itself.\nIn this post we show how the dual of a problem arises, we examine its relationship with the primal, and list all possible primal-dual outcomes. In doing so, we look at duality in the general case of constrained optimization, in the specific case of linear programs, and in a category of unconstrained problems."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#the-lagrangian-dual-variables-and-the-dual-function",
    "href": "posts/optimization/duality_theory.html#the-lagrangian-dual-variables-and-the-dual-function",
    "title": "Notes: Duality Theory",
    "section": "The Lagrangian, Dual Variables, and the Dual Function",
    "text": "The Lagrangian, Dual Variables, and the Dual Function\nThe Lagrangian linear relaxation, sometimes simply referred to as the Lagrangian, is:\n\\[\\mathcal{L}(x,\\lambda,\\mu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\mu_i h_i(x)\\] \\[\\textrm{where} \\ \\lambda \\geq 0\\]\nWe call the \\(\\lambda_i\\)’s the Lagrange multipliers corresponding to the inequality constraints, and the \\(\\mu_i\\)’s those corresponding to the equality constraints. The vectors \\(\\lambda\\) and \\(\\mu\\), composed of these Lagrange multipliers, are called the Lagrange multiplier vectors or, for reasons that will soon become apparent, the dual variables.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn some sources, the Lagrangian is simply stated as \\(\\mathcal{L}(x,\\lambda) = f_0(x) + \\sum_{i=1}^n \\lambda_i f_i(x)\\). Indeed, by separating the equality constraints \\(h_i(x) = 0\\) into \\(h_i(x) \\leq 0\\) and \\(-h_i(x) \\leq 0\\), we can transform a problem with equality constraints into one with only inequality constraints. So, this formulation of the Lagrangian is still general enough to account for problems with equality constraints."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#a-lagrangian-lower-bound",
    "href": "posts/optimization/duality_theory.html#a-lagrangian-lower-bound",
    "title": "Notes: Duality Theory",
    "section": "A Lagrangian Lower-Bound",
    "text": "A Lagrangian Lower-Bound\nNot only does the Lagrangian (\\(\\mathcal{L}\\)) relax the unconstrained problem, it also plays a natural role in the formulation of the dual problem.\nThe first thing to note about the Lagrangian is that the coordinate-wise \\(\\lambda \\geq 0\\) condition is crucial. This is because, in the event that an inequality constraint is violated, say \\(f_i(x) &gt; 0\\), the corresponding \\(\\lambda_i\\) must be non-negative in order to apply a positive penalty to the minimization. On the other hand, \\(\\mu\\) is free to assume any value since the equality constraints can be violated in either direction and both scenarios must be positively penalized.\nThe second thing to note about the Lagrangian is that, even though it applies a positive penalty that scales linearly in the severity of the violation, this penalty is, nevertheless, not as severe as the infinite penalty applied in \\(\\mathcal{J}\\). Also, in the Lagrangian, we may actually be rewarding feasible choices of \\(x\\) that have margin. That is, in the event that \\(f_i(x) &lt; 0\\), \\(\\lambda_if_i(x)\\) is a non-positive reward for the minimization problem.\nAll of this is to say that \\(\\mathcal{L}\\) is a point-wise lower-bound on \\(\\mathcal{J}\\). That is, the following inequality holds:\n\\[\\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu \\tag{3.1}\\]\nThis fact is also obvious by plotting each of the \\(m + p\\) linear penalties, superimposing them against the plots of the corresponding infinitely hard penalty functions, and noticing that in each case \\(\\lambda_i f_i(x) \\leq \\mathbb{1}_-(f_i(x))\\) and \\(\\mu_i h_i(x) \\leq \\mathbb{1}_0(h_i(x))\\).\nTaking \\(\\min\\) w.r.t. \\(x\\) of the LHS in \\((3.1)\\) we get:\n\\[\\min_x \\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu\\]\nFurthermore, restricting \\(x\\) to the primal feasible set \\(\\mathcal{X}\\) on which \\(J(x) = f_0(x)\\), we obtain something interesting:\n\\[\\min_x \\mathcal{L}(x,\\lambda,\\mu) \\leq f_0(x) \\ \\ \\forall x \\in \\mathcal{X}, \\lambda \\geq 0, \\mu \\tag{3.2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Lagrangian may not attain its \\(\\min\\) w.r.t. \\(x\\), in which case the LHS is simply \\(-\\infty\\). We shall see later, once we define the dual function and the duality gap, that this corresponds to the dual function being \\(-\\infty\\) \\(\\forall \\lambda \\geq0, \\mu\\) and the duality gap being \\(\\infty\\). In a sense, this is a useless lower bound. So, for now, we assume the interesting case in which the minimum is attained and thus \\(\\inf_x \\mathcal{L}(x, \\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\).\n\n\n\nDesignating the original problem as the primal, we call \\(g(\\lambda, \\mu) := \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\) the dual function because it exhibits the aforementioned property of weak duality. That is, per \\((3.2)\\), any feasible value of \\(g(\\lambda, \\mu)\\) is a lower-bound for any feasible value of the primal.\nTaking min of the other side, we have a more specific flavor of weak duality:\n\\[g(\\lambda,\\mu) \\leq \\min_x f_0(x) \\ \\ \\forall \\lambda \\geq 0, \\mu\\]\nOr simply:\n\\[g(\\lambda,\\mu) \\leq f_0(x^*) \\ \\ \\forall \\lambda \\geq 0, \\mu \\tag{3.3}\\]\nThat is, any feasible value of the dual is a lower-bound for the primal optimum.\nMaximizing both sides of \\((3.3)\\) by noticing that the RHS is a constant, and by assuming the LHS attains its \\(\\max\\) we get an even more specific flavor of weak duality:\n\\[\\max_{\\lambda \\geq 0, \\mu} g(\\lambda,\\mu) \\leq f_0(x^*)\\]\nOr simply, assuming \\(\\lambda^*\\) and \\(\\mu^*\\) to be dual-optimal:\n\\[g(\\lambda^*, \\mu^*) \\leq f_0(x^*) \\tag{3.4}\\]\nThat is, the dual optimum is a lower-bound for the primal optimum.\nFrom here we move, quite naturally, to defining the dual problem."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#the-lagrange-dual-problem",
    "href": "posts/optimization/duality_theory.html#the-lagrange-dual-problem",
    "title": "Notes: Duality Theory",
    "section": "The Lagrange Dual Problem",
    "text": "The Lagrange Dual Problem\nIt’s natural, to ask what the tightest lower bound on the primal optimal value \\(f_0(x^*)\\) is. This amounts to finding the values \\(\\lambda^* \\geq 0\\), and \\(\\mu^*\\) for which \\(g(\\lambda^*, \\mu^*)\\) is maximized. We call this the Lagrange dual problem or, simply, the dual problem.\nIt can be stated as:\n\\[\n\\begin{aligned}\n\\max_{\\lambda, \\mu} &: g(\\lambda, \\mu)\n\\\\\ns.t. &: \\lambda \\geq  0\n\\end{aligned}\n\\]\nLooking at the above, it becomes immediately clear why we were motivated to call \\(\\lambda\\), and \\(\\mu\\) the dual variables: they are the variables of the dual problem."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#the-max-min-inequality",
    "href": "posts/optimization/duality_theory.html#the-max-min-inequality",
    "title": "Notes: Duality Theory",
    "section": "The Max-Min Inequality",
    "text": "The Max-Min Inequality\nThe inequality expressed as \\((3.5)\\) is, in fact, a general result in mathematics called the Max-Min Inequality. To summarize: the Max-Min Inequality makes no assumptions about the function, it’s true for all functions of the form \\(f: X \\times Y \\rightarrow \\mathbb{R}\\) and asserts that:\n\\[\\sup_{x\\in X} \\left\\{ \\inf_{y\\in Y} f(x,y) \\right\\} \\leq \\inf_{y\\in Y} \\left\\{ \\sup_{x\\in X} f(x,y) \\right\\}\\]\nSince no assumption is made on \\(f\\), the inequality also holds for the Lagrangian, \\(\\mathcal{L}\\). And, since we’re in the special case where the optimal values of the primal and the dual are assumed to exist, the functions do attain the respective optima. That is, we can replace \\(\\sup\\) and \\(\\inf\\) in the above inequality with \\(\\max\\) and \\(\\min\\) which obtains the symmetric formulation of weak duality as in \\((3.5)\\).\nWe can now prove weak duality through a non-optimization lens by proving the Max-Min Inequality.\nFor any \\(f\\), and \\(x \\in X\\), \\(y \\in Y\\) we have:\n\\[f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x\\]\nThe right-hand side is now only a function of \\(x\\), so minimizing both sides w.r.t. \\(x\\) yields:\n\\[ \\inf_x f(x,y) \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\} \\ \\ \\forall y\\]\nThe right-hand side is now a constant, so maximizing both sides w.r.t. \\(y\\) results in the desired conclusion.\n\\[\\sup_y \\left\\{ \\inf_x f(x,y) \\right\\} \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Max-Min Inequality proof should remind us of the steps taken to obtain \\((3.2)\\) through \\((3.4)\\) from \\((3.1)\\). In fact, \\((3.1)\\) is of form \\(f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x\\), since \\(J(x)\\) is, as shown earlier, equivalent to \\(\\max_{\\lambda \\geq 0, \\mu} L(x, \\lambda, \\mu)\\).\n\n\n\n\nGame-Theoretic Interpretation\nThe Max-Min Inequality is perhaps best understood intuitively as a game between two adversarial players (the optimizers above).\nLet’s represent the game using a value tree. The nodes are the final scores, and the junctures represent player choices. The first juncture represents Player 1’s turn, and the second that of Player 2.\n\n\n\n\n\n   graph TD;\n      A[Turn 1] --&gt; B[Turn 2];\n      A--&gt;C[Turn 2];\n      B--&gt;D[2];\n      B--&gt;E[7];\n      C--&gt;F[1];\n      C--&gt;G[8];\n\n\n\n\n\n\nTo the minimizer, as Player 1, the game tree above might as well be an imaginary tree shrouded in the mist of uncertainty because the true min of \\(1\\) is unattainable. Player 2 has final say, and if the minimizer chooses \\(1\\)’s subtree, the maximizer will choose \\(8\\). The minimizer must be more pragmatic and choose not the subtree that contains the true min, but rather that which restricts the maximizer’s choice most to the compromise: \\(7\\). That means, we can compute a hidden value tree for our minimizer (giving us a glimpse into one frame of the recursive Minimax algorithm which is used to maximize the minimum gain or minimize the maximum loss).\n\n\n\n\n\n   graph TD;\n      A[Turn 1] --&gt; B[7];\n      A--&gt;C[8];\n\n\n\n\n\n\nThe outcome of this game is determinedly \\(7\\). That’s a score Player 2, the maximizer, is very satisfied with. Note that if we reverse the turns, the score of the game will favor the minimizer. That’s exactly what the max-min inequality is saying: a game that’s rigged in favor of Player 2 will always result in a better outcome for Player 2 than if the turns were reversed."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#slaters-condition---sufficient-condition-for-strong-duality",
    "href": "posts/optimization/duality_theory.html#slaters-condition---sufficient-condition-for-strong-duality",
    "title": "Notes: Duality Theory",
    "section": "Slater’s Condition - Sufficient Condition for Strong Duality",
    "text": "Slater’s Condition - Sufficient Condition for Strong Duality\nWhile the rare non-convex problem could exhibit the property, strong duality is mostly enjoyed by convex problems. However, not all convex problems are strongly dual. There are many results that establish conditions on the problem, beyond convexity and existence of a primal-optimal, under which strong duality holds. These conditions are called constraint qualifications. In this section we will explore such conditions for convex problems and discuss them in the specific case of linear programs.\nOne of these constraint qualification conditions is Slater’s condition.\n\nSlater’s Condition:   \\(\\exists \\ \\hat x\\) s.t. \\(f_i(\\hat x) &lt; 0\\), and \\(h_i(\\hat x) = 0\\) \\(\\forall i\\).\n\nInformally, Slater’s condition says that the existence of a feasible point which has margin w.r.t. all the inequality constraints is needed in addition to convexity. In even simpler terms, the feasible region must have an interior point.\nThe sufficient condition for strong duality in convex problems is then:\n\nSufficient Condition for Strong Duality:   Any convex optimization problem satisfying Slater’s condition has zero duality gap.\n\nThe proof of this is beyond what we’re trying to accomplish in this post.\nA weaker constraint qualification condition guarantees strong duality in the case of linear constraints. If \\(k\\) of the \\(m\\) inequality constraints are linear then the condition becomes:\n\\[\n\\begin{aligned}f_i(\\hat x) &\\leq 0, \\ i = 1,...,k, \\\\\nf_i(\\hat x) &&lt; 0, \\ i = k+1,...,m, \\\\\nh_i(\\hat x) &= 0, \\ i = 1,...,p\n\\end{aligned}\n\\]\nIn other words, the linear constraints need not have margin.\nNote that if all the constraints are linear, which is the case in linear programming, the above constraint qualification condition simply reduces to feasibility.\nSo, while a sufficient condition of strong duality in non-linear convex programs is, both, the existence of a feasible interior point and a primal optimal, the situation is remarkably simpler in linear programs. Since a primal optimal for a linear program is also feasible, it satisfies the weaker constraint qualification condition. Thus, for a linear program to be strongly dual the existence of a primal optimal is sufficient."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#the-max-min-equality",
    "href": "posts/optimization/duality_theory.html#the-max-min-equality",
    "title": "Notes: Duality Theory",
    "section": "The Max-Min Equality",
    "text": "The Max-Min Equality\nJust as weak duality is the Max-Min Inequality in disguise, strong duality is the Minimax Theorem in disguise. The Minimax Theorem is about the special case of the Max-Min Inequality in which the LHS and the RHS are strictly equal. It holds for any function \\(f: X \\times Y \\rightarrow \\mathbb{R}\\) that has some additional structure. Roughly speaking, when \\(f\\) is saddle-shaped, convex in one variable and concave in the other, the Max-Min Inequality holds with strict equality.\nThe following theorem, which is offered without proof, translates this result into the setting of optimization.\n\nSaddle Point Theorem:   If \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal and dual optimal solutions for a convex problem which satisfies Slater’s condition, they form a saddle point of the associated Lagrangian. Furthermore, if \\((x^*, (\\lambda^*, \\mu^*))\\) is a saddle point of a Lagrangian, then \\(x^*\\) is primal optimal and \\((\\lambda^*, \\mu^*)\\) is dual optimal for the associated problem, and the duality gap is zero.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis theorem should not be taken as a certificate of strong duality. If the Lagrangian is saddle-shaped then the associated problem is strongly dual, however the converse is not true. Since not all strongly dual problems are convex problems which satisfy Slater’s condition, if a problem is strongly dual it is not guaranteed that its Lagrangian is saddle-shaped.\n\n\n\n\nGame-Theoretic Interpretation\nIn keeping with the game theoretic intuition developed in the section on weak duality, one can imagine a game in which the first player’s optimal choice is independent of the second player’s actions. In such a game, both players are free to play their best strategies and, consequently, the order of play is not important."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#an-easier-dual-problem",
    "href": "posts/optimization/duality_theory.html#an-easier-dual-problem",
    "title": "Notes: Duality Theory",
    "section": "An Easier Dual Problem",
    "text": "An Easier Dual Problem\nLet’s further qualify what we mean when we say strong duality gives an equivalent, usually easier, problem to solve.\nAt the start of this post we considered a general convex program. However, everything we’ve discussed about Lagrangian duality applies to non-convex problems as well. Suppose the primal problem is non-convex. The task is that of finding the primal optimum:\n\\[f_0(x^*) = \\min_x \\left\\{ \\max_{\\lambda \\geq 0, \\mu} \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nBut maximizing the Lagrangian over \\(\\lambda \\geq 0\\) and \\(\\mu\\) for a fixed \\(x\\), recovers \\(\\mathcal{J}(x)\\): a non-differentiable objective. So, we cannot use the unconstrained optimality condition in finding the stationary points of \\(\\mathcal{J}(x)\\) which is what’s required in the next step.\nMeanwhile, the dual problem is that of finding the dual optimum:\n\\[g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nMinimizing the Lagrangian over \\(x\\) for fixed \\(\\lambda \\geq 0\\) and \\(\\mu\\) may still be a difficult problem but, at least, it lends itself to using the method of unconstrained optimization. Moreover, the resulting dual function \\(g(\\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\) is a point-wise minimum of linear functions in \\(\\lambda\\) and \\(\\mu\\), so its always concave in those variables. Additionally, the constraint \\(\\lambda \\geq 0\\) is a simple, convex (linear in fact), constraint. So, the dual problem is a convex optimization problem regardless of the convexity of the primal.\nSolving a convex dual problem is usually easier that solving a non-convex primal problem. However, even if the primal is a convex problem to begin with, the dual may still be easier to solve. The primal could have more variables than constraints in which case its dual has more constraints than variables. This is yet another way in which the dual can be an easier problem to solve than the primal."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#farkas-lemma",
    "href": "posts/optimization/duality_theory.html#farkas-lemma",
    "title": "Notes: Duality Theory",
    "section": "Farkas’ Lemma",
    "text": "Farkas’ Lemma\nFarkas’ Lemma simply states that a given vector \\(c\\) is either a conic combination of some vectors \\(a_i\\)’s (for \\(i \\in I\\)), or it’s entirely separated from their cone by some hyperplane.\nWe state Farkas’ Lemma without offering proof since it has such an obvious geometric interpretation.\n\nFarkas’ Lemma:   For any vector \\(c\\) and \\(a_i \\ \\ (i \\in I)\\) either the first or the second statement holds:\n\n\\(\\exists p \\geq 0\\) s.t. \\(c = \\sum_{i \\in I} a_ip_i\\)\n\\(\\exists\\) vector \\(d\\) s.t. \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I\\) but \\(d^Tc &lt; 0\\)"
  },
  {
    "objectID": "posts/optimization/duality_theory.html#proving-a-theorem-of-the-alternative",
    "href": "posts/optimization/duality_theory.html#proving-a-theorem-of-the-alternative",
    "title": "Notes: Duality Theory",
    "section": "Proving a Theorem of the Alternative",
    "text": "Proving a Theorem of the Alternative\nTo see how we can prove a Theorem of the Alternative, it helps to state one.\n\nTheorem:   Exactly one of the following two statements most hold for a given matrix A.\n\n\\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\)\n\\(\\exists p\\) s.t. \\(p^TA &gt; 0\\)\n\n\n\nProof using a Separation Argument\n\nPrelude\nAt the heart of separation arguments lies this simple fact.\n\nSeparating Hyperplane Theorem:   For any convex set \\(C\\), if a point \\(\\omega \\notin C\\) then there exists a hyperplane separating \\(\\omega\\) and \\(C\\).\n\nFarkas’ Lemma, for instance, is proved by a separation argument that uses, as its convex set, the conic combination of the \\(a_i\\)’s. The conclusion is immediate since in Farkas’ Lemma the first statement plainly says that a vector belongs to the convex set, and the second statement plainly says there exists a separating hyperplane between the two.\nThis is the pattern all separation arguments must follow. However, in general, it may take a bit of work to define the problem-specific convex set and also to show that the two statements are really talking about belonging to this set, and separation from it. However, once these components are in place, the proof is complete.\nUsing this idea, let’s give a proof of the above theorem using a separation argument.\n\n\nProof\nFirst order of business is to come up with a convex set.\nLet’s take \\(C = \\{ z : z = Ay, \\sum_i y_i = 1, y \\geq 0 \\}\\) to be the convex hull of the columns of \\(A\\).\nThe first statement in the theorem was that \\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\).\nSince \\(x \\ne 0\\) and \\(x \\geq 0\\) we can scale as \\(x\\) as \\(y = \\alpha x\\) until \\(\\sum_i y_i = 1\\).\nSo, the first statement is equivalent to saying the origin belongs to the convex hull \\(C\\) (i.e. \\(0 \\in C\\))\nThe second statement was that \\(\\exists p\\) s.t. \\(p^TA &gt; 0\\). This is equivalent to saying that all the columns of \\(A\\) lie to one side of the separating hyperplane introduced by \\(p\\).\nBut all \\(z \\in C\\) are convex combinations of \\(A\\)’s columns. In particular since they’re a convex combination they’re also a conic combination, so all \\(z \\in C\\) also lie on the same side of the hyperplane. That is \\(p^Tz &gt; 0 \\ \\ \\forall z \\in C\\).\nBut, of course, \\(p^T0 = 0\\) (not \\(&gt; 0\\)). So, according to the second statement, the origin is separated from \\(C\\).\nThis concludes the proof since the two statements must be mutually exclusive.\n\n\n\nProof using Strong Duality\nTo prove the theorem we need to show two things. First, we need to show \\(1 \\implies \\neg 2\\), then we need to show \\(\\neg 1 \\implies 2\\).\nThe \\(1 \\implies \\neg 2\\) direction is simple.\nSuppose \\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\).\nThen \\(\\forall p \\ \\ (p^TA)x = p^T(Ax) = p^T0 = 0\\) (not \\(&gt; 0\\)).\nWe tackle the \\(\\neg 1 \\implies 2\\) direction using duality.\nThe strategy is to construct an LP based on \\(\\neg 1\\) such that the feasibility of its dual implies \\(2\\).\nWe can express \\(\\neg 1\\) as ‘\\(\\forall x \\ne 0\\), either \\(Ax \\ne 0\\) or \\(x &lt; 0\\).’ Equivalently, ‘\\(x \\ne 0 \\implies Ax \\ne 0\\) or \\(x &lt; 0\\).’ Taking the contrapositive, statement \\(1\\) becomes ‘\\(Ax = 0\\) and \\(x \\geq 0 \\implies x = 0\\).’\nSo, we form the LP as:\n\\[\n\\begin{aligned}\n&\\max_x: \\textbf{1}^Tx\n\\\\\n&s.t.: \\begin{aligned} &Ax = 0\n\\\\\n&x \\geq 0\n\\end{aligned}\n\\end{aligned}\n\\]\nNote that \\(x = 0\\) is a feasible solution to the LP. Furthermore, assuming statement \\(1\\) guarantees that \\(x = 0\\) is the only feasible solution. Thus, the LP is feasible and bounded.\nBy strong duality, its dual exists and is also feasible and bounded.\nThe dual is:\n\\[\n\\begin{aligned}\n&\\min_p: \\textbf{0}^Tp\n\\\\\n&s.t.: p^TA \\geq \\textbf{1}\n\\end{aligned}\n\\]\nSince the dual is feasible, \\(\\exists p\\) s.t. \\(p^TA \\geq 1 &gt; 0\\) which demonstrates the truth of statement \\(2\\) and, in doing so, completes the proof."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#stationarity-condition",
    "href": "posts/optimization/duality_theory.html#stationarity-condition",
    "title": "Notes: Duality Theory",
    "section": "Stationarity Condition",
    "text": "Stationarity Condition\nIn the section titled an easier dual problem we mentioned that the dual problem is that of finding the dual optimal value:\n\\[g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nIf strong duality holds, this dual optimum agrees with the primal optimum. That is:\n\\[g(\\lambda^*, \\mu^*) = f_0(x^*)\\]\nTurns out in case of strong duality there’s even more to be said. As we saw earlier optimizing the unconstrained objective \\(\\mathcal{J}(x)\\) not only resulted in the primal optimum \\(f_0(x^*)\\) for some optimal \\(x^*\\) of the constrained problem, the very same point \\(x^*\\) itself turned out to be an optimizer of \\(\\mathcal{J}(x)\\). Similarly, we can show that the primal optimum \\(x^*\\) for some primal-dual optimal pair \\((x^*, (\\lambda^*, \\mu^*))\\) optimizes \\(\\mathcal{L}(x, \\lambda^*, \\mu^*)\\). In other words, the primal optimum \\(x^*\\) is a stationary point of the Lagrangian at the dual optimum \\((\\lambda^*,\\mu^*)\\).\nThat is:\n\\[x^* = \\arg \\min_x \\mathcal{L} (x, \\lambda^*, \\mu^*) \\tag{6.1}\\]\nOr, equivalently:\n\\[\\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) = \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\tag{6.2}\\]\nWe can think of \\((6.1)\\) and \\((6.2)\\) as the analogs of \\((2.1)\\) and \\((2.2)\\) for the Lagrangian (\\(\\mathcal{L}\\)). This is exactly what we’ve been working towards. Recall that the original motivation in augmenting the constrained problem into the unconstrained \\(\\mathcal{J}\\) was to find the former’s optimizer using methods of unconstrained optimization on \\(\\mathcal{J}\\). Once found, \\((2.1)\\) or \\((2.2)\\) would guarantee that an optimizer of \\(\\mathcal{J}\\) was, itself, an optimizer of the original problem. Failing that, we relaxed \\(\\mathcal{J}\\) into \\(\\mathcal{L}\\) hoping we could still accomplish the same. \\((6.1)\\) and \\((6.2)\\) are the results which guarantee precisely that. They say that the optimizer \\(x^*\\) of the original problem can be found by optimizing the unconstrained objective \\(\\mathcal{L}\\). And, since \\(\\mathcal{L}\\) is everywhere differentiable w.r.t. \\(x\\), we can now proceed.\nIn practice, however, \\((6.1)\\) and \\((6.2)\\) only give us a way to solve for a primal-optimal \\(x^*\\) directly if a dual-optimal \\((\\lambda^*, \\mu^*)\\) is already known. That is, any time the dual problem is easier to solve than the primal.\nMore generally, this fact gives us the next best thing. It gives us a way to check if a given pair \\((x^*,(\\lambda^*,\\mu^*))\\) is primal-dual optimal – an optimality condition known as stationarity condition.\n\nStationarity Condition:   Suppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem. Then: \\[\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\]\n\nThe stationary condition is obtained simply by an application of the unconstrained optimality condition to \\(\\mathcal{L}(x, \\lambda^*, \\mu^*)\\):\n\\[\\nabla_x \\mathcal{L} (x^*, \\lambda^*, \\mu^*) = 0\\]\nExpanding the LHS gives:\n\\[\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\]\nFor the sake of completeness, since we stated them without offering a proof, let’s prove the equivalent claims \\((6.1)\\) and \\((6.2)\\) from which stationarity condition ultimately follows.\n\nProof of Claims (6.1) and (6.2)\nSuppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem.\nThe following point-wise inequality holds in general since its LHS is a minimization over \\(x\\) and its RHS is a maximization over \\((\\lambda, \\mu)\\) of the Lagrangian.\n\\[g(\\lambda, \\mu) \\leq \\mathcal{L}(x, \\lambda, \\mu) \\leq \\mathcal{J}(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu\\]\nIt is also, in particular, true for the primal-dual optimal pair. That is:\n\\[g(\\lambda^*, \\mu^*) \\leq \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\leq \\mathcal{J}(x^*) \\tag{7.1}\\]\nHowever, \\(\\mathcal{J}(x^*) = f_0(x^*)\\) and, by strong duality, \\(g(\\lambda^*, \\mu^*) = f_0(x^*)\\). Hence, \\(g(\\lambda^*, \\mu^*) = \\mathcal{J}(x^*)\\) and \\((7.1)\\) is actually the equality.\n\\[\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = g(\\lambda^*, \\mu^*) \\tag{7.2}\\]\nSubstituting, the definition of the dual function for the RHS of \\((7.2)\\), we get:\n\\[\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*)\\]\nWhich is exactly \\((6.2)\\) and, by equivalence, also \\((6.1)\\)."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#complementary-slackness",
    "href": "posts/optimization/duality_theory.html#complementary-slackness",
    "title": "Notes: Duality Theory",
    "section": "Complementary Slackness",
    "text": "Complementary Slackness\nStrong duality also obtains another optimality condition known as complementary slackness (CS).\n\nComplementary Slackness (CS):   Suppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem. Then: \\[\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i\\]\n\nInformally, if a primal constraint at an optimal \\(x^*\\) is loose, that is \\(f_i(x^*) \\ne 0\\), then its corresponding dual variable \\(\\lambda^*_i\\) in the dual optimal \\(\\lambda^*\\) must be zero. Conversely, if the dual variable \\(\\lambda_i^*\\) is positive then the corresponding constraint must be tight.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf a primal constraint is tight at \\(x^*\\), complementary slackness tells us nothing about its corresponding dual variable.\n\n\n\n\nProof of Complementary Slackness\nSuppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem.\nExpanding the RHS we obtain:\n \\[\n\\begin{aligned}\nf_0(x^*) &= g(\\lambda^*, \\mu^*) \\\\\n&= \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) \\\\\n&= \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\\\\n&=  f_0(x^*) + \\sum_{i=1}^m \\lambda_i^* f_i(x) + \\sum_{i=1}^p \\mu_i^* h_i(x^*) \\\\\n&\\leq f_0(x^*)\n\\end{aligned} \\tag{8.1}\n\\] \nThe first equality holds by strong duality, the second holds by the definition of the dual function, the third equality holds by \\((6.2)\\), and the fourth is true by the expansion of \\(\\mathcal{L}(x^*, \\lambda^*, \\mu^*)\\).\nTo see why the last inequality holds, note that:\n\\[\\sum_{i=1}^p \\mu_i^* h_i(x^*) = 0\\]\nsince, by feasibility of \\(x^*\\), \\(h_i(x^*) = 0 \\ \\ \\forall i\\). Then again, by feasibility of \\(x^*\\), we have:\n\\[f_i(x^*) \\leq 0  \\ \\ \\forall i \\tag{8.2}\\]\nFurthermore, by construction of the Lagrangian, \\(\\lambda \\geq 0\\). So, together with \\((8.2)\\), we have:\n\\[\\sum_{i=1}^m \\lambda^*_i f_i(x^*) \\leq 0\\]\nBut taken altogether \\((8.1)\\) says \\(f_0(x^*) \\leq f_0(x^*)\\) which can only hold through strict equality.\nThen it must be the case that \\(\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0\\)\nBeing a sum of non-positive terms, \\(\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0\\) if and only if\n\\[\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i \\tag{8.3}\\]\nwhich concludes the proof of complementary slackness."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#karush-kuhn-tucker-kkt-conditions",
    "href": "posts/optimization/duality_theory.html#karush-kuhn-tucker-kkt-conditions",
    "title": "Notes: Duality Theory",
    "section": "Karush-Kuhn-Tucker (KKT) Conditions",
    "text": "Karush-Kuhn-Tucker (KKT) Conditions\nComplementary slackness and stationarity condition are often bundled into the KKT Conditions.\nIn the absence of strong duality the KKT Conditions are necessary but insufficient for optimality. However, for problems which are strongly dual the KKT Conditions become a certificate of optimality. That is, they are both necessary and sufficient.\n\nKKT Conditions:   The primal-dual pair \\((x^*, (\\lambda^*, \\mu^*))\\) satisfies the KKT conditions if the following hold:\n\n\\(\\nabla_x f_0(x^*) + \\sum_{i=1}^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\)\n\\(\\lambda^*_if_i(x^*) = 0 \\ \\ \\forall i\\)\n\\(g_i(x^*) \\leq 0 \\ \\ \\forall i\\)\n\\(h_i(x^*) = 0 \\ \\ \\forall i\\)\n\\(\\lambda^* \\geq 0\\)\n\n\nWe recognize KKT-1 as the stationarity condition, and KKT-2 as complementary slackness. KKT-3 through KKT-5 simply ensure primal-dual feasibility.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThese conditions only apply to problems with differentiable objective and constraints. For the case in which one or more of the objective or constraints is non-differentiable, there is an easy generalization of the KKT conditions using sub-differentials. However, sub-differentials are beyond the scope of this post.\n\n\n\nPrimal-dual pairs which satisfy the KKT Conditions are called KKT pairs.\n\nGeneralization of Unconstrained Optimization\nThe KKT conditions represent a strict generalization of the unconstrained optimality condition for use in constrained problems.\nTo see this, note that if there are no constraints then the KKT conditions simply reduce to the familiar unconstrained optimality condition:\n \\[\\nabla_x f_0(x^*) = 0\\] \nIn order to discuss optimality in constrained problems, we must first define a feasible direction.\n\nFeasible Direction:   A unit vector \\(d\\) is called a feasible direction at any \\(x\\) if \\(x + \\epsilon d\\) remains feasible for \\(\\epsilon &gt; 0\\) small enough.\n\nWe are now in a position to generalize the unconstrained optimality condition into a constrained optimality condition.\nBy using Taylor expansion, for small enough \\(\\epsilon &gt; 0\\) and any feasible \\(d\\), we can estimate \\(f_0(x^* + \\epsilon d)\\) by its linear approximation as:\n \\[f_0(x^* + \\epsilon d) = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td\\] \nBut since \\(x^*\\) is optimal, we have:\n \\[\n\\begin{aligned}\nf_0(x^*) &\\leq f_0(x^* + \\epsilon d) \\\\\n& = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td\n\\end{aligned}\n\\] \nWhich necessitates that \\(\\nabla f_0(x^*)^Td \\geq 0\\). Since \\(d\\) was just an arbitrary feasible direction, this result must hold for all feasible directions. Hence, the constrained optimality condition can be given as:\n\nConstrained Optimality Condition:   If \\(x^*\\) is an optimizer of \\(f_0\\) over some constraint set then, for any feasible direction \\(d\\) at \\(x^*\\), \\(\\nabla f_0(x^*)^Td \\geq 0\\).\n\nNote that \\(\\nabla f_0(x^*)^Td\\) is simply the directional derivative of \\(f_0\\) in the direction \\(d\\). So, in plain words, the constrained optimality condition says that the directional derivative of the objective function in any feasible direction at an optimizer should be non-negative. This ensures that moving in any feasible direction does not minimize the objective any further.\n\n\nCertificate of Optimality\nAs promised, the KKT Conditions together with strong duality obtain a certificate of optimality.\n\nCertificate of Optimality:   If strong duality holds, then \\(x^*, (\\lambda^*, \\mu^*)\\) are primal-dual optimal if and only if they are a KKT pair.\n\n\nProof of Certificate of Optimality\nWe have already shown one direction of the certificate in the sections on stationarity condition and complementary slackness, where we proved that being a primal-dual optimal pair in a strongly convex problem guarantees \\((x^*, (\\lambda^*, \\mu^*))\\) is also a KKT pair.\nShowing the other direction provides us with an interesting geometric viewpoint of the KKT conditions. Incidentally, Farkas’ Lemma is the key theoretical result that underpins this proof.\nLet’s begin the proof.\nIf a particular constraint is loose at \\(x^*\\) then taking a small enough step in any direction from \\(x^*\\) does not violate it. Formally, if \\(f_i(x^*) &lt; 0\\), then \\(f_i(x^* + \\epsilon d) \\leq 0\\) \\(\\forall d\\) and for some \\(\\epsilon &gt;0\\). So, loose constraints do not pose any restrictions on the set of feasible directions.\nHowever, if a constraint is tight at \\(x^*\\), that is \\(f_i(x^*) = 0\\), then we must be careful not to violate it. Suppose the set of indices of all the tight constraints at \\(x^*\\) is given by \\(I_{x^*}\\). For small enough \\(\\epsilon &gt; 0\\), we can estimate \\(f_i(x^* + \\epsilon d)\\) by its linear Taylor expansion as:\n\\[f_i(x^* + \\epsilon d) = f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td \\ \\ \\forall i \\in I_{x^*}\\]\nFor feasibility, we want \\(f_i(x^* + \\epsilon d) \\leq 0\\). So, we require:\n\\[f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*}\\]\nBut since \\(f_i\\) is tight at \\(x^*\\), \\(f_i(x^*) = 0\\), which simply leaves us with:\n\\[\\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*}\\]\nWith the above restriction of \\(d\\), the feasible directions can now be stated as:\n\\[d \\ \\textrm{s.t.} \\ \\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*} \\tag{8.1}\\]\nOr, equivalently:\n\\[d \\ \\textrm{s.t.} \\ - \\nabla f_i(x^*)^Td \\geq 0 \\ \\ \\forall i \\in I_{x^*} \\tag{8.2}\\]\nBut, since \\(x^*\\) is optimal, by the constrained optimality condition we have:\n\\[\\nabla f_0(x^*)^Td \\geq 0 \\ \\ \\forall \\ \\textrm{feasible} \\ d \\tag{8.3}\\]\nThat is, for all \\(d\\) as in \\((8.2)\\).\nPut together, \\((8.2)\\) and \\((8.3)\\) say that \\(\\not \\exists \\ d\\) which defines a separating hyperplane between \\(\\nabla f_0(x^*)\\) and \\(-\\nabla f_i(x^*)\\) for all binding constraints at \\(x^*\\). By Farka’s Lemma, this means that the only other alternative scenario must be true — it must be the case that \\(\\nabla f_0(x^*)\\) lies in the cone of the \\(-\\nabla f_i(x^*)\\)’s.\nFormally, \\(\\exists \\ \\lambda^* \\geq 0\\) s.t.\n\\[\\nabla f_0(x^*) + \\sum_{i \\in I_{x^*}} \\lambda^*_i f_i(x^*) = 0 \\tag{8.4}\\]\nUpon closer examination, \\((8.4)\\) is exactly KKT-1, KKT- 2, and KKT-5 all rolled into one condition. The remaining conditions, KKT-3 and KKT-4 simply follow from the assumed feasibility of \\(x^*\\).\nThus, we have shown that if \\(x^*\\) is primal-optimal, its KKT pair \\((x^*, (\\lambda^*, \\mu^*))\\) exists. Furthermore, as proved earlier, if strong duality holds then any KKT pair is primal-dual optimal. Hence, if strong duality holds, the \\((\\lambda^*, \\mu^*)\\) obtained through the above procedure is also dual-optimal."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#proof-of-strong-duality-in-lps",
    "href": "posts/optimization/duality_theory.html#proof-of-strong-duality-in-lps",
    "title": "Notes: Duality Theory",
    "section": "Proof of Strong Duality in LP’s",
    "text": "Proof of Strong Duality in LP’s\n\nPrelude\nAs in the general case, we construct a KKT pair through the use of Farkas’ Lemma. Then, by a structural property of LPs, we notice that the dual and the primal optima agree. This concludes the proof of LP strong duality.\n\n\nProof\nSuppose \\(x^*\\) is primal-optimal. Let the set \\(I_{x^*} = \\{ i : a_i^Tx^* = b_i\\}\\) be the set of the indices of the active constraints at \\(x^*\\). Our goal is to construct a dual optimal solution \\(p^*\\) s.t. \\(c^Tx^* = b^Tp^*\\).\nLet \\(d\\) be any vector that satisfies \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}\\). That is, \\(d\\) is a feasible direction w.r.t. to all the active constraints.\nBy the assumption that \\(x^*\\) is optimal, we have \\(c^Tx^* \\leq c^T(x^* + \\epsilon d) = c^Tx^* + \\epsilon c^Td\\). Thus, \\(c^Td = d^Tc \\geq 0\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(d^Tc\\) is nothing but the directional derivative at the minimizer \\(x^*\\). So, this also follows from the optimality of \\(x^*\\) using the constrained optimality condition.\n\n\n\nBut, since \\(d\\) is a vector s.t. \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}\\) and \\(d^Tc \\geq 0\\), \\(d\\) does not separate \\(c\\) from the cone of the \\(a_i\\)’s. And, since \\(d\\) was arbitrary, this puts us in the setting of Farkas’ Lemma. Namely, there exist no vectors \\(d\\) that separate \\(c\\) from the cone. This means the alternative must be true — \\(c\\) must a conic combination of the \\(a_i\\)’s that are active at the minimizer. In other words, \\(\\exists p \\geq 0\\) s.t. \\(c = \\sum_{i \\in I_{x^*}} p_ia_i\\).\nBut \\(p\\) has dimension equal to only the number of active constraints at \\(x^*\\). To be a dual variable at all, it must have dimension equal to the number of all primal constraints. We extend \\(p\\) to \\(p^*\\) by setting all the entries that do not correspond to the active constraints at \\(x^*\\) to be zero.\nThat is \\(p^*_i = \\begin{cases} p_i \\ \\ \\textrm{if} \\ \\  i \\in I_{x^*} \\\\ 0   \\ \\ \\textrm{if} \\ \\  i \\notin I_{x^*} \\end{cases}\\).\nNow \\(A^Tp^*  = \\sum_{i} p^*_ia_i = c\\), so any feasibility condition in the dual, whether it be \\(A^Tp \\leq c\\), \\(A^Tp \\geq c\\), or \\(A^Tp = c\\), is satisfied by \\(p^*\\).\nFurthermore, the dual objective at \\(p^*\\) agrees with the primal objective at \\(x^*\\).\n\\[b^Tp^* = \\sum_{i} b_ip_i^* = \\sum_{i \\in I_{x^*}} b_ip_i^* + \\sum_{i \\notin I_{x^*}} b_ip_i^* = \\sum_{i \\in I_{x^*}} a_i^Tx^*p_i^* = (\\sum_{i \\in I_{x^*}} p_ia_i^T)x^* = c^Tx^* \\]\nThis concludes the proof."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#fl-transform---a-convex-operation",
    "href": "posts/optimization/duality_theory.html#fl-transform---a-convex-operation",
    "title": "Notes: Duality Theory",
    "section": "FL Transform - a Convex Operation",
    "text": "FL Transform - a Convex Operation\nThe FL Transform \\(f^*\\) is always convex regardless of the convexity of \\(f\\).\nThat’s because, for a fixed \\(x\\), \\(y^Tx - f(x)\\) is a linear function in \\(y\\). So, \\(f^*\\) is a point-wise supremum of linear functions, making it convex."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#the-case-of-involution",
    "href": "posts/optimization/duality_theory.html#the-case-of-involution",
    "title": "Notes: Duality Theory",
    "section": "The Case of Involution",
    "text": "The Case of Involution\nThe double FL Transform \\(f^{**}\\) does not always recover \\(f\\). To see this fact note that, as an FL Transform of the some function (namely, \\(f^*\\)), \\(f^{**}\\) is always convex. Therefore, \\(f^{**} \\ne f\\) if \\(f\\) is non-convex.\nBut convexity alone is not enough to guarantee involution. We need an additional condition on \\(f\\), namely that its sub-level sets must be closed, to ensure \\(f^{**} = f\\)."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#inverse-gradients",
    "href": "posts/optimization/duality_theory.html#inverse-gradients",
    "title": "Notes: Duality Theory",
    "section": "Inverse Gradients",
    "text": "Inverse Gradients\nIf \\(f\\) has closed sub-level sets and is convex then the gradients of \\(f\\) and \\(f^*\\) are inverses. That is, assuming both \\(f\\) and \\(f^*\\) are differentiable:\n\\[y = \\nabla f(x) \\iff x = \\nabla f^*(y)\\]\nLet’s first prove the \\(\\implies\\) direction.\nSuppose \\(y = \\nabla f(x)\\). By \\(f\\)’s convexity:\n\\[f(\\hat x) \\geq f(x) + y^T(\\hat x - x) \\ \\ \\forall \\hat x\\]\nAnd so:\n\\[y^T \\hat x - f(\\hat x) \\leq y^T x - f(x) \\ \\ \\forall \\hat x\\]\nBy taking supremum over \\(x\\) and by noting that, since the sub-level sets are closed, the supremum is attained, we obtain:\n\\[f^*(y) = y^T x - f(x)\\]\nThe desired result follows by taking the gradient of both sides w.r.t. \\(y\\). That is:\n\\[\\nabla f^*(y) = x\\]\nThe \\(\\impliedby\\) direction is similar. We start from the assumption that \\(x = \\nabla f^*(y)\\) and get the desired result by using the involution property \\(f^{**} = f\\)."
  },
  {
    "objectID": "posts/optimization/duality_theory.html#fl-duality",
    "href": "posts/optimization/duality_theory.html#fl-duality",
    "title": "Notes: Duality Theory",
    "section": "FL Duality",
    "text": "FL Duality\nAs mentioned, the FL Transform has a natural role in duality.\nSuppose the unconstrained optimization problem is:\n\\[\\min_x : f(x) + h(Ax)\\]\nWhere \\(f\\) and \\(h\\) are convex functions, and \\(A\\) is a matrix representing a bounded linear transformation.\nWe introduce a dummy variable \\(y\\) and form the artificial constraint \\(y = Ax\\). The problem becomes:\n\\[\n\\begin{aligned}\n\\min_{x,y} &: f(x) + h(y) \\\\\ns.t. &: Ax = y\n\\end{aligned}\n\\]\nForming the Lagrangian gives us:\n\\[\\mathcal{L}(x,y,z) = f(x) + h(y) + z^T(Ax - y)\\]\nThen, the dual function is the following FL Transform:\n\\[\n\\begin{aligned}\ng(z) &= \\min_{x,y} \\mathcal{L}(x,y,z) \\\\\n&= \\min_{x,y} f(x) + h(y) + z^T(Ax - y) \\\\\n&= \\min_{x,y} (A^Tz)^Tx + f(x) - z^Ty + h(y) \\\\\n&= \\min_x \\left\\{ (A^Tz)^Tx + f(x) \\right\\} + \\min_y \\left\\{ -z^Ty + h(y) \\right\\} \\\\\n&= \\min_x \\left\\{ -\\left((-A^Tz)^Tx - f(x)\\right) \\right\\} + \\min_y \\left\\{ -\\left(z^Ty - h(y)\\right) \\right\\} \\\\\n&= - \\max_x \\left\\{ (-A^Tz)^Tx - f(x) \\right\\} - \\max_y \\left\\{ z^Ty - h(y) \\right\\} \\\\\n&= - f^*(-A^Tz) - h^*(z)\n\\end{aligned}\n\\]\nAnd, consequently, the dual problem is:\n\\[\\max_z: - f^*(-A^Tz) - h^*(z)\\]\nTo convince ourselves of the utility of this dual, note that the dual is, indeed, an easier problem. This is because the negative of an FL Transform is always concave regardless of the convexity of \\(f\\) and \\(h\\). So, the dual problem is a maximization of a concave function which is, in general, an easy optimization problem."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html",
    "href": "posts/leetcode/lc11_container_with_most_water.html",
    "title": "LC11: Container with Most Water",
    "section": "",
    "text": "We are given an integer array height of length n. Each element of the array corresponds to a vertical line (of n total lines) drawn from the horizontal axis such that the i-th line’s two endpoints are (i, 0) and (i, height[i]) (see the figure below).\nFind two lines which, together with the x-axis, form a container which can hold the most water. Return the maximum amount of water this container can store.\nExample\n\n\n\nContainer with most water\n\n\nInput: height = [1,8,6,2,5,4,8,3,7]\nOutput: 49\nExplanation\nThe above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of the water (highlighted section) the container can contain is 49 (in units of area)."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#optimal-sub-structure",
    "href": "posts/leetcode/lc11_container_with_most_water.html#optimal-sub-structure",
    "title": "LC11: Container with Most Water",
    "section": "Optimal Sub-Structure",
    "text": "Optimal Sub-Structure\n\nLet \\(h(i)\\) denote the height of the \\(i\\)-th vertical line.\nLet \\(a(i,j)\\) denote the area of the container formed by the pair of vertical lines \\((i,j)\\).\nLet \\(maxArea(i,j)\\) denote the maximum area formed by the lines \\({i,...,j}\\) – that is the output of the procedure on the subarray height[i:j].\n\nSuppose, without loss of generality, \\(h(1) \\leq h(n)\\) (that is, the first line is shorter than the last). The \\(w.l.o.g.\\) there will boil down to having to keep track of two pointers at the implementation stage (in the iterative approach) needed to determine which height is less at any particular iteration.\nNow, we can state a top-down optimal sub-structure for the problem:\n\\[\nmaxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\n\\]\nThis top-down expression hints at a recursive solution (which is left as an exercise to the reader). In this post, we will offer a bottom-up, iterative solution (with very basic tabulation – just the running maximum water variable, really) – one that’s often called the Two-Pointer Solution (which is apt, because most of what we have to do is keep track of two pointers pointing to the vertical lines).\nWe have explored dynamic programming before in some detail in a similar post about finding the Maximum Subarray, and have even proven the correctness of another, similar, DP solution to the problem of finding the Best Time to Buy and Sell Stock. This problem is yet another, very basic, example of DP. As we know, DP solutions are often proven by proving the underlying optimal sub-structure. Let’s look at such a proof.\n\nProof of The Optimal Sub-Structure\nFor the initial pair \\((1,n)\\) where \\(h(1) \\leq h(n)\\) we have \\(a(1,n) &gt; a(1,k)  \\ \\ \\forall k\\). This is because we’re starting out from the widest container formed by \\({(1,n)}\\) and considering containers of decreasing width formed by the pairs \\({(1, n-1), (1, n-2), ..., (1,2)}\\).\nIn case \\({h(k) &gt; h(1)}\\) for some \\({n \\geq k &gt; 1}\\) the area of the container formed by \\({(1,k)}\\) is still determined by \\({h(1)}\\), except now it’s less wide. Whereas if \\({h(k) &lt; h(1)}\\) the area of the container decreases not only in width but also in height.\nIn both cases we have \\({a(1,n) &gt; a(1,k)}\\) which means in general \\({a(1,n) &gt; a(1, k) \\ \\ \\forall k}\\).\nTherefore, we may omit the first vertical line from consideration and consider the sub-problem on the indices \\({2,...,n}\\). The final, optimal solution will then be \\(maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\) as was the claim."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#the-two-pointers-algorithm-pseudo-code",
    "href": "posts/leetcode/lc11_container_with_most_water.html#the-two-pointers-algorithm-pseudo-code",
    "title": "LC11: Container with Most Water",
    "section": "The Two Pointers Algorithm (Pseudo-Code)",
    "text": "The Two Pointers Algorithm (Pseudo-Code)\nAt this point, the Two-Pointers Algorithm is all but trivial to see:\n\nInitialize two pointers, ‘left’ and ‘right’, at the first and last index respectively.\nWhile the pointers do not intersect:\n\nCalculate the area of the container formed by the pointers and determine if it’s the maximum area encountered so far\nKeep the position of the pointer of the vertical line that’s longer fixed\nAdvance the pointer of vertical line that’s shorter towards the fixed pointer"
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#the-solution",
    "href": "posts/leetcode/lc11_container_with_most_water.html#the-solution",
    "title": "LC11: Container with Most Water",
    "section": "The Solution",
    "text": "The Solution\n\ndef maxArea(height) -&gt; int:\n    i, j = 0, len(height) - 1\n    water = 0\n    while i &lt; j:\n        water = max(water, (j - i) * min(height[i], height[j]))\n        if height[i] &lt; height[j]:\n            i += 1\n        else:\n            j -= 1\n    return water\n\n# Example input\nheights = [1,8,6,2,5,4,8,3,7]\nprint(f'The container with most water has area: {maxArea(heights)}')\n\nThe container with most water has area: 49"
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html",
    "title": "LC: Connected Sinks and Sources",
    "section": "",
    "text": "We are given a pipe system represented by a 2D rectangular grid of cells. There are three different types of items located in the cells within the grid, with each having either no items or 1 item:\n\nSource: There is 1 source in the system. It is represented by the asterisk character *.\nSinks: There are an arbitrary number of sinks in the system. They are each represented by a different uppercase letter (A, B, C, etc.).\nPipes: There are 10 different shapes of pipes, represented by the following characters: ═, ║, ╔, ╗, ╚, ╝, ╠, ╣, ╦, ╩\n\nNote that:\n\nEach pipe has openings on 2 or 3 sides of its cell.\nTwo adjacent cells are connected if both have a pipe opening at their shared edge.\nWe should treat the source and sinks as having pipe openings at all of their edges. For example, the two cells A╦ are connected through their shared edge, but the two cells B╔ are not.\nA sink may be connected to the source through another sink. For example, in the simple pipe system *A═B═C, all three sinks are connected to the source.\n\nOur objective is to write a function that determines which sinks are connected to the source in a given pipe system.\nAs an example, consider the following pipe system.\nFormatted input\n*╗ ╦═A\n ╠═╝\n C ╚═B\nIn this pipe system, the source * is connected to sinks A and C but not B.\nA system is specified by an input text file in the following un-formated way.\nUnformatted input\n*02\nC10\n╠11\n╗12\n═21\n╚30\n╝31\n╦32\n═40\n═42\nB50\nA52\nNote that each item is followed by its coordinate in the grid (the origin of the coordinate system is taken to be the lower left corner (0,0) corresponding to an empty space in this system)."
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#problem-statement",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#problem-statement",
    "title": "LC: Connected Sinks and Sources",
    "section": "",
    "text": "We are given a pipe system represented by a 2D rectangular grid of cells. There are three different types of items located in the cells within the grid, with each having either no items or 1 item:\n\nSource: There is 1 source in the system. It is represented by the asterisk character *.\nSinks: There are an arbitrary number of sinks in the system. They are each represented by a different uppercase letter (A, B, C, etc.).\nPipes: There are 10 different shapes of pipes, represented by the following characters: ═, ║, ╔, ╗, ╚, ╝, ╠, ╣, ╦, ╩\n\nNote that:\n\nEach pipe has openings on 2 or 3 sides of its cell.\nTwo adjacent cells are connected if both have a pipe opening at their shared edge.\nWe should treat the source and sinks as having pipe openings at all of their edges. For example, the two cells A╦ are connected through their shared edge, but the two cells B╔ are not.\nA sink may be connected to the source through another sink. For example, in the simple pipe system *A═B═C, all three sinks are connected to the source.\n\nOur objective is to write a function that determines which sinks are connected to the source in a given pipe system.\nAs an example, consider the following pipe system.\nFormatted input\n*╗ ╦═A\n ╠═╝\n C ╚═B\nIn this pipe system, the source * is connected to sinks A and C but not B.\nA system is specified by an input text file in the following un-formated way.\nUnformatted input\n*02\nC10\n╠11\n╗12\n═21\n╚30\n╝31\n╦32\n═40\n═42\nB50\nA52\nNote that each item is followed by its coordinate in the grid (the origin of the coordinate system is taken to be the lower left corner (0,0) corresponding to an empty space in this system)."
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#solution",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#solution",
    "title": "LC: Connected Sinks and Sources",
    "section": "Solution",
    "text": "Solution\n\nParsing the input\nLet’s define a function load_file that loads the input file and returns the contents as a list of strings.\n\ndef load_file(filePath: str) -&gt; list[str]:\n    with open(filePath, 'r') as file:\n        return file.readlines()\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use the keyword with rather than try-finally to leverage built in Python file IO safety features.\n\n\n\nLet’s save the output of load_file as the input we’ll provide to parse_input (a function we’ll define later). Running load_file for the example file above obtains:\n\ninput = load_file(\"./problem_assets/connected_sinks_and_sources/example1.txt\")\nprint(input[:5]) # Printing only the five leading entries of the input for brevity\n\n['*02\\n', 'C10\\n', '╠11\\n', '╗12\\n', '═21\\n']\n\n\nAs we can see the lines include the newline character, We can further clean up input by using map and rstrip which strips the right-end of each line of any undesired characters. We can modify the definition of load_file to return the cleaned up input.\n\ndef load_file(filePath: str) -&gt; list[str]:\n    with open(filePath, 'r') as file:\n        return [line.rstrip(\"\\n\") for line in file.readlines()]\n\nLet’s run this re-defined load_file for the example file again to see the difference.\n\ninput_ex1 = load_file(\"./problem_assets/connected_sinks_and_sources/example1.txt\")\nprint(input_ex1[:5])\n\n['*02', 'C10', '╠11', '╗12', '═21']\n\n\n\n\nDefining an Item object\nBut we’re still dealing with simple string representations… That means it’s not immediately clear how to obtain the open edges of, say, the ╚ character programmatically. Let’s parse this list down into a data structure we will call Item.\nItem will have Boolean instance variables:\n\nleft\nright\ntop\nbottom\n\nWhich will indicate whether the corresponding edge is open or closed (True or False respectively).\nIt will also have instance variables x and y for the item’s coordinates. An additional type instance variable with possible values in \\(\\{\\)Source,Pipe,Sink\\(\\}\\). This may be helpful for checking edge conditions later on.\nHere’s the implementation of the Item class. Since it’s quite long and trivial, I’ve collapsed the definition. Feel free to expand and examine…\n\n\nCode\nclass Item:\n    def __init__(\n            self, \n            type: str = \"Source\",\n            ascii: str = \"*\",\n            coords: tuple[int, int] = (0,0),\n            left: bool = True,\n            right: bool = True,\n            top: bool = True,\n            bottom: bool = True,\n        ):\n        self.type = type\n        self.ascii = ascii\n        self.coords = coords\n        self.left = left\n        self.right = right\n        self.top = top\n        self.bottom = bottom\n    \n    def __repr__(self): # This is a custom representation method for the Item class\n        # Prepare all fields\n        fields = [\n            f\"type: {self.type}\",\n            f\"ascii: {self.ascii}\",\n            f\"coordinates: {self.coords}\",\n            \"edges:\",\n            f\"  left: {self.left}\",\n            f\"  right: {self.right}\",\n            f\"  top: {self.top}\",\n            f\"  bottom: {self.bottom}\"\n        ]\n        # Find the longest line\n        max_length = max(len(line) for line in fields)\n        # Add extra space for the class name\n        title = f\" {type(self).__name__} \"\n        max_length = max(max_length, len(title))\n        # Create the box\n        bottom = \"+\" + \"-\" * (max_length + 2) + \"+\"\n        title_line = f\"+{title:-^{max_length + 2}}+\"\n        # Build the output\n        lines = [title_line]\n        # Add each field with padding\n        for field in fields:\n            padding = \" \" * (max_length - len(field))\n            lines.append(f\"| {field}{padding} |\")\n        lines.append(bottom)\n        return \"\\n\".join(lines)\n\n\nUnfortunately, there’s no way to get around the hard-coding of the ASCII characters into Item-s. Here’s an implementation of map_to_items. It’s quite long and trivial, but feel free to expand and examine the implementation…\n\n\nCode\ndef map_to_items(input: list[str]) -&gt; list[Item]: # Converts the text input to a list of Item objects\n    def to_item(line: str) -&gt; Item: # Converts a single line in the input to an Item\n        objectToReturn = Item() # Initialize an empty object\n        objectToReturn.ascii = line # We will also print the unconverted ASCII representation of the object for debugging purposes\n        objectToReturn.coords = tuple([int(coord) for coord in line[1:]]) # Extract the coordinates as a tuple of integers\n        objectToReturn.left = True # Initialize all edges to open\n        objectToReturn.right = True\n        objectToReturn.top = True\n        objectToReturn.bottom = True\n        # Match the first character of the line to determine the type of object\n        if line[0] == \"*\": # Match the first character of the line to determine the type of object\n            objectToReturn.ascii = \"*\"\n            # The default object is a source, '*', at (0,0), so nothing else needs to be changed in this case...\n        elif line[0] == \"═\":\n            # Note: edges are ordered [top, right, bottom, left]\n            objectToReturn.ascii = \"═\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.top = False\n            objectToReturn.bottom = False\n        elif line[0] == \"║\":\n            objectToReturn.ascii = \"║\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.left = False\n            objectToReturn.right = False\n        elif line[0] ==  \"╔\":\n            objectToReturn.ascii = \"╔\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.top = False\n            objectToReturn.left = False\n        elif line[0] ==  \"╗\":\n            objectToReturn.ascii = \"╗\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.top = False\n            objectToReturn.right = False\n        elif line[0] == \"╚\":\n            objectToReturn.ascii = \"╚\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.left = False\n            objectToReturn.bottom = False\n        elif line[0] == \"╝\":\n            objectToReturn.ascii = \"╝\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.right = False\n            objectToReturn.bottom = False\n        elif line[0] == \"╠\":\n            objectToReturn.ascii = \"╠\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.left = False\n        elif line[0] == \"╣\":\n            objectToReturn.ascii = \"╣\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.right = False\n        elif line[0] == \"╦\":\n            objectToReturn.ascii = \"╦\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.top = False\n        elif line[0] == \"╩\":\n            objectToReturn.ascii = \"╩\"\n            objectToReturn.type = \"Pipe\"\n            objectToReturn.bottom = False\n        else: # The case of a sink\n            if not line[0].isalpha(): # Check if the first character is a letter at all\n                raise ValueError(\"The first character of a sink must be a letter.\")\n            objectToReturn.ascii = line[0]\n            objectToReturn.type = \"Sink\"\n        return objectToReturn\n            \n    return list(map(to_item, input))\n\n\nLet’s run input through map_to_items to obtain the programmatic representation of the items in our pipe system. We show only the first five from the output in the interest of brevity.\n\nparsed_input_ex1 = map_to_items(input_ex1)\nfor obj in parsed_input_ex1[:3]:\n    print(obj)\n\n+------- Item --------+\n| type: Source        |\n| ascii: *            |\n| coordinates: (0, 2) |\n| edges:              |\n|   left: True        |\n|   right: True       |\n|   top: True         |\n|   bottom: True      |\n+---------------------+\n+------- Item --------+\n| type: Sink          |\n| ascii: C            |\n| coordinates: (1, 0) |\n| edges:              |\n|   left: True        |\n|   right: True       |\n|   top: True         |\n|   bottom: True      |\n+---------------------+\n+------- Item --------+\n| type: Pipe          |\n| ascii: ╠            |\n| coordinates: (1, 1) |\n| edges:              |\n|   left: False       |\n|   right: True       |\n|   top: True         |\n|   bottom: True      |\n+---------------------+"
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#path-finding-dfs",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#path-finding-dfs",
    "title": "LC: Connected Sinks and Sources",
    "section": "Path finding (DFS)",
    "text": "Path finding (DFS)\nThis is a path finding problem, so we’re likely going to use either BFS or DFS. Since it’s easier to represent the given data as a 2D array, instead of converting it into a graph representation, we will implement DFS on the 2D array itself.\nHere’s the graph representation of the pipe system above, for equivalency’s sake (although we won’t be using graphs).\n\n\n\n\n\n   graph TD;\n      A[\"* (0,2)\"] --&gt; B[\"╗ (1,2)   \"];\n      B--&gt;C[\"╠ (1,1)\"];\n      C--&gt;D[\"C (1,0)\"];\n      C--&gt;E[\"═ (2,1)\"];\n      E--&gt;F[\"╝ (3,1)\"];\n      F--&gt;G[\"╦ (3,2)\"];\n      G--&gt;H[\"═ (4,2)\"];\n      H--&gt;K[\"A (5,0)\"];\n      L[\"╚ (3,0)\"]--&gt;M[\"═ (4,0)\"];\n      M--&gt;N[\"B (5,0)\"];\n\n\n\n\n\n\n\nAs we can see, we have two disjoint acyclic graphs representing the example pipe system above (ignore the edge directions, Mermaid is cool but pretty limited).\nIn general, because BFS traverses a graph one level at a time, we tend to use BFS when looking for the shortest path between two nodes. The first time BFS lands on the target node constitutes the shortest path to that node (or one of the multiple such paths if there’s a tie between several paths). DFS is better suited for finding any valid path between the source * and a sink, and not necessarily the shortest such path, we can use DFS for this problem.\n\nExpanding the input into a 2D matrix\nBFS can be implemented on a 2D array just as it can on a graph. Graphs are nothing more, really, than adjacency lists. But first, we need to expand the input into such a 2D matrix of Items. To maintain align the directions of the neighbors of a given item to the formatted input, we need to expand the input in a particular way. We want grid[0][2] to refer to the source * (0,2).\nThat means the grid should be something like:\n[[None, None, *],[C,╠,╗], ...]\nThen the 1st argument to grid will point to the 1st subarray, and the 2nd argument will point to the sink itself. In other words, the first argument refers to the columns of the formatted input, and the second to the rows. The empty space above * (0, 2), in the formatted input, will then be referenced by grid[0][3].\n\ndef expand_parsed_input(input: list[Item]) -&gt; list[list[Item]]:\n    # Find the maximum x and y coordinates occuring the given input\n    max_rows  = 0\n    max_cols = 0\n    for item in input:\n        if item.coords[0] &gt; max_cols:\n            max_cols = item.coords[0]\n        if item.coords[1] &gt; max_rows:\n            max_rows = item.coords[1]\n    # Initialize the 2D matrix with 'None' types\n    grid = [[None for _ in range(max_rows + 1)] for _ in range(max_cols + 1)]\n    # Populate the 2D matrix with the 'Item'-s\n    for item in input:\n        grid[item.coords[0]][item.coords[1]] = item\n    return grid\n\nexpanded_parsed_input_ex1 = expand_parsed_input(parsed_input_ex1)\n\nThe output of expand_input contains Item types and None types (for empty cells).\nBefore we implement DFS, let’s define a few helper functions that check if two Item-s are connected by the relevant edge.\n\n\nHelper functions to determine connectedness at an edge\n\ndef connected_at_top(item1: Item, item2: Item) -&gt; bool:\n    return item1.top and item2.bottom\n\ndef connected_at_bottom(item1: Item, item2: Item) -&gt; bool:\n    return item1.bottom and item2.top\n\ndef connected_at_left(item1: Item, item2: Item) -&gt; bool:\n    return item1.left and item2.right\n\ndef connected_at_right(item1: Item, item2: Item) -&gt; bool:\n    return item1.right and item2.left\n\nWe’re going to need these helper functions to check if two Item-s are connected by an edge before we add them to the DFS stack (to visit).\n\n\nDFS Implementation\nFinally, let’s implement iterative DFS using a stack.\n\n# Iterative version using a stack\ndef dfs_iterative(grid, start_x, start_y):\n    # Obtain the dimensions of the grid\n    # Note: grid = [[None, None, *],[C,╠,╗], ...] \n    # 1st index spans the subarrays \n    # 2nd index spans the items themselves \n    dim_cols, dim_rows = len(grid), len(grid[0])\n\n    # Initialize the visited set\n    visited = set()\n    # Initialize the stack with the starting position\n    stack = [(start_x, start_y)]\n    \n    while stack:\n        x, y = stack.pop()\n        \n        if (x, y) in visited:\n            continue\n        if grid[x][y].type == \"Sink\":\n            print(f\"Found connected sink: {grid[x][y].ascii}\")\n\n        visited.add((x, y))\n        # print(f\"Visiting: {grid[x][y].ascii} ({x}, {y})\") &lt;-- Enable for debugging\n        \n        # Add only the eligible neighbors to the stack \n        # Eligible neighbors are those that:\n        # 1. Are not outside the grid \n        # 2. Are not 'None' \n        # 3. Are connected to the current item by a corresponding edge\n        # Check the top neighbor\n        if y + 1 &gt;= 0 and y + 1 &lt;= dim_rows - 1 and grid[x][y + 1] is not None:\n            if connected_at_top(grid[x][y], grid[x][y + 1]):\n                stack.append((x, y + 1))\n        # Check the bottom neighbor\n        if y - 1 &gt;= 0 and y - 1 &lt;= dim_rows - 1 and grid[x][y - 1] is not None: \n            if connected_at_bottom(grid[x][y], grid[x][y - 1]):\n                stack.append((x, y - 1))\n        # Check the right neighbor\n        if x + 1 &gt;= 0 and x + 1 &lt;= dim_cols - 1 and grid[x + 1][y] is not None:\n            if connected_at_right(grid[x][y], grid[x + 1][y]):\n                stack.append((x + 1, y))\n        # Check the left neighbor\n        if x - 1 &gt;= 0 and x - 1 &lt;=  dim_cols - 1 and grid[x - 1][y] is not None:\n            if connected_at_left(grid[x][y], grid[x - 1][y]):\n                stack.append((x - 1, y))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen doing iterative graph traversal with a heap or a stack, it’s always a good idea to print the associated data structure (namely the heap or the stack) at the beginning of each iteration as a debugging tactic.\n\n\n\nFinally, let’s run the DFS on the expanded input, supplying the coordinates of the source * as the starting point.\n\ndfs_iterative(expanded_parsed_input_ex1, 0, 2)\n\nFound connected sink: A\nFound connected sink: C\n\n\nThere we have it!"
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#verifying-solution",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#verifying-solution",
    "title": "LC: Connected Sinks and Sources",
    "section": "Verifying solution",
    "text": "Verifying solution\nNow let’s try it on a slightly altered input. This time, we have redirected the pipes so that only B and C are connected to the source.\nFormatted input\n*╗ ╦═A\n ╠═╦\n C ╚═B\nUnformatted input\n*02\nC10\n╠11\n╗12\n═21\n╚30\n╦31\n╦32\n═40\n═42\nB50\nA52\n\ninput_ex2 = load_file(\"./problem_assets/connected_sinks_and_sources/example2.txt\")\nparsed_input_ex2 = map_to_items(input_ex2)\nexpanded_parsed_input_ex2 = expand_parsed_input(parsed_input_ex2)\ndfs_iterative(expanded_parsed_input_ex2, 0, 2)\n\nFound connected sink: B\nFound connected sink: C"
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#visualization-of-the-dfs-algorithm-using-manim",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#visualization-of-the-dfs-algorithm-using-manim",
    "title": "LC: Connected Sinks and Sources",
    "section": "Visualization of the DFS algorithm using Manim",
    "text": "Visualization of the DFS algorithm using Manim\nLet’s use Manim (a mathematical animation library), to visualize the DFS algorithm on the matrix for fun!\nWe start by importing the cell magic %%manim, then modify and insert our dfs_iterative implementation into a Manim scene. The Manim code itself is collapsed, as it’s not the focus of this post.\n\nimport manim\n\n\n\nCode\n%%manim -qh ConnectedSinksAndSourcesDemo\n\nfrom manim import *\n\nclass ConnectedSinksAndSourcesDemo(Scene):\n    def construct(self):    \n        # Create table\n        sinks_and_sources=[[\"*\",\"╗\",\"\",\"╦\",\"═\",\"A\"],[\"\",\"╠\",\"═\",\"╝\",\"\",\"\"],[\"\",\"C\",\"\",\"╚\",\"═\",\"B\"]]\n        table = Table(sinks_and_sources, include_outer_lines=True)\n        self.play(Write(table))\n        # The DFS algorithm still requires the original `expanded_parsed_input_ex1` to work\n        self.dfs_iterative(expanded_parsed_input_ex1, table, 0, 2)\n        self.wait(5)\n\n\n    # Custom Manim cell highlighter...\n    def highlight_cell(self, manim_table, row, col, color=GREEN):\n        cell = manim_table.get_cell((row, col))\n        background = Rectangle(\n            width=cell.width,\n            height=cell.height,\n            fill_color=color,\n            fill_opacity=0.5,\n            stroke_width=0\n        )\n        background.move_to(cell)\n        self.play(FadeIn(background))\n\n    # Same DFS algorthm as before, but modified to inject Manim effects...\n    def dfs_iterative(self, grid, manim_table, start_x, start_y):\n        dim_cols, dim_rows = len(grid), len(grid[0])\n\n        # Initialize the visited set\n        visited = set()\n        # Initialize the stack with the starting position\n        stack = [(start_x, start_y)]\n        \n        while stack:\n            x, y = stack.pop()\n            \n            if (x, y) in visited:\n                continue\n\n            visited.add((x, y))\n\n            # In Manim, tables are indexed from 1. \n            # (x,y) = (0,2) - corresponds to -&gt; maninm_table(1,1)              \n            self.highlight_cell(manim_table, 3-y, x+1)\n                    \n            if y + 1 &gt;= 0 and y + 1 &lt;= dim_rows - 1 and grid[x][y + 1] is not None:\n                if connected_at_top(grid[x][y], grid[x][y + 1]):\n                    stack.append((x, y + 1))\n            # Check the bottom neighbor\n            if y - 1 &gt;= 0 and y - 1 &lt;= dim_rows - 1 and grid[x][y - 1] is not None: \n                if connected_at_bottom(grid[x][y], grid[x][y - 1]):\n                    stack.append((x, y - 1))\n            # Check the right neighbor\n            if x + 1 &gt;= 0 and x + 1 &lt;= dim_cols - 1 and grid[x + 1][y] is not None:\n                if connected_at_right(grid[x][y], grid[x + 1][y]):\n                    stack.append((x + 1, y))\n            # Check the left neighbor\n            if x - 1 &gt;= 0 and x - 1 &lt;=  dim_cols - 1 and grid[x - 1][y] is not None:\n                if connected_at_left(grid[x][y], grid[x - 1][y]):\n                    stack.append((x - 1, y))\n\n\nHere’s DFS on the grid visualized using Manim.\n\n\n\n\n\nVideo\nConnected Sinks and Sources Demo\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "posts/leetcode/lc_connected_sinks_and_sources.html#conclusion",
    "href": "posts/leetcode/lc_connected_sinks_and_sources.html#conclusion",
    "title": "LC: Connected Sinks and Sources",
    "section": "Conclusion",
    "text": "Conclusion\nWe explored how to use iterative DFS on a matrix to solve the connected sinks and sources problem. From here, we can explore other graph problems or learn about game development and, perhaps, turn this rudimentary algorithm into a fun game mechanic?"
  },
  {
    "objectID": "posts/game_development/unreal_engine_cheat_sheet.html",
    "href": "posts/game_development/unreal_engine_cheat_sheet.html",
    "title": "Unreal Engine: Cheat Sheet",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time. You will need to download the latest version of Xcode.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local\nFollow the README to compile and run the Unreal Editor. The Editor uses a version of the Unreal Engine which can be downloaded by running a few installation scripts in the UnrealEngine project that download the engine and game files.\n\n\n\n\nWe can also install Unreal Engine through the Unreal Launcher.\n\n\n\nBefore starting a project, make sure to:\n\nLoad static content\nEnable raytracing\n\nFeel free to pick a template project from the project browser, such as a first-person or a third-person game. Once the new project opens a .uproject file is created in the default ~/Documents/Unreal Projects directory. We can click this file directly to open the project to continue where we left it off in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut\nFunction\nOptional explanation\n\n\n\n\nRMB\nRotate camera\n\n\n\nRMB + W or A/S/D\nMovement front/left/back/right\n\n\n\nRMB + E or Q\nMovement up or down\n\n\n\nScroll Wheel Up/Down + RMB\nAdjust camera speed\n\n\n\nLMB\nSelect object in world\n\n\n\nQ\nObject selection mode\n\n\n\nW\nMovement translation mode\n\n\n\nE\nRotation translation mode\n\n\n\nR\nScale translation mode\n\n\n\nDel\nDelete selected object\n\n\n\nH\nHide selected object\n\n\n\nCtrl + D\nDuplicate selected object\nAnother shortcut is holding Opt and dragging the selected object\n\n\nHold Shift + LMB\nSelect multiple objects\n\n\n\nHold Ctrl + LMB\nDeselect multiple objects from a multi-object selection\n\n\n\nF\nFocus on selected object\nIf we’re lost in our scene we can always select an object in the Outliner window and press F to re-focus on the object\n\n\nG\nGame view mode\nHides widgets, see the world as if in-game\n\n\nCmd + F11\nImmersive mode\nHides all windows and shows the world in full screen (note: we may also need to press Fn)\n\n\nCtrl +  Space \nOpen Content Drawer\nThe Content Drawer is what contains all of our in-game assets (3D/2D assets, code, etc.)\n\n\nOpt + P\nPlay game\n\n\n\nEsc\nExit game and go back to Edit mode\n\n\n\nShift + 1 or 2/3/…\nChange Unreal Editor mode\nThe default is Selection Mode. Other modes include Landsaping Mode, Foliage Mode, etc. These bring up various tools to edit landscapes and make foliage (as their names suggest)\n\n\nHold L + LMB\nRotate sun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView mode\nFunction\nOptional explanation\n\n\n\n\nWireframe\nShows only the wireframe mesh\n\n\n\nUnlit\nShows the world disregarding lighting information\nUseful for visibility when editing in dark environments\n\n\nLight only\nThe world with lighting alone (i.e. without material information)\n\n\n\n\n\n\n\nThe translation modes and snap tools are available in the top-right corner of the viewport.\n\n\n\n\n\n\nFigure 1: Translation and snap tools\n\n\n\nFrom left to right, we have:\n\nObject selection mode\nTranslation mode\nRotation mode\nScale mode\nToggle for coordinate system (global vs local) relative to which the transform gizmo is shown\nSurface snapping options (controls how objects snap to existing surfaces)\nTranslation snapping\nRotation snapping\nScale snapping\nCamera speed adjustment\nMinimize/maximize current viewport\n\nThe default viewport includes a grid, this can be disabled in Show menu in the top-left corner of the viewport by un-checking Grid.\n\n\n\nThere are two ways to add objects to the scene. The first is through Add Object in the top-left corner.\nClick \nThe second way is by using the Content Drawer in the bottom-left corner.\nClick  or use shortcut Ctrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nThe latter method is for adding custom game objects (blueprints as well as 2D/3D assets and more).\nWe often need to see where a particular game asset is located in the Content Drawer. The shortcut to find any object inside the Content Drawer is Ctrl + B while having the object selected.\nAll the assets that make up our game, such as custom 3D assets, custom blueprints (code), etc are stored within the /Content folder of the project.\n\n\n\n\n\nThe Details window looks as follows.\n\n\n\n\n\n\nFigure 2: Details window\n\n\n\nIt contains all the details about a selected object such as:\n\nTransformation information (coordinates, angle of rotation, and scale)\nMaterial information\nPhysics and many more…\n\n\n\n\nThe Outliner window looks as follows.\n\n\n\n\n\n\nFigure 3: Outliner window\n\n\n\nIt contains all the objects that make up the scene, as well as options to show/hide, save, and pin objects:\n\n\n\n\nAssuming we’ve selected the third person template, go into the Content Browser and go to /Content/ThirdPerson/Blueprints to access the blueprints of the third person character.\n\n\n\n\n\n\nFigure 4: Content Drawer - Third person blueprints\n\n\n\nClick on the humanoid. Feel free to dock the newly opened window (as a new tab) next to the open viewport in the Unreal Editor.\nA blueprint consists of three views:\n\nEventGraph\nConstruction Script\nViewport\n\n\n\nContains most of the blueprint’s logic.\n\n\n\n\n\n\nFigure 5: Blueprints - Event graph\n\n\n\n\n\n\nMore on this later…\n\n\n\n\n\n\n\n\n\nFigure 6: Blueprints - Viewport\n\n\n\nThis view shows everything within the selected blueprint as an object. Objects in a blueprint are called components. The Components window is the equivalent of the Outliner window but for blueprints. Rather than showing every object in the world, it shows every component in the blueprint.\n\n\n\n\n\n\nFigure 7: Blueprints - Components window\n\n\n\nThe My Blueprint window contains all the nodes and variables contained in the blueprint. We can click on the EventGraph in the Graphs tab of this window to go into the EventGraph view directly.\n\n\n\n\n\n\nFigure 8: Blueprints - My Blueprint window"
  },
  {
    "objectID": "posts/game_development/unreal_engine_cheat_sheet.html#option-1-cloning-unrealengine-project-from-github",
    "href": "posts/game_development/unreal_engine_cheat_sheet.html#option-1-cloning-unrealengine-project-from-github",
    "title": "Unreal Engine: Cheat Sheet",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time. You will need to download the latest version of Xcode.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local\nFollow the README to compile and run the Unreal Editor. The Editor uses a version of the Unreal Engine which can be downloaded by running a few installation scripts in the UnrealEngine project that download the engine and game files."
  },
  {
    "objectID": "posts/game_development/unreal_engine_cheat_sheet.html#option-2-install-using-unreal-launcher",
    "href": "posts/game_development/unreal_engine_cheat_sheet.html#option-2-install-using-unreal-launcher",
    "title": "Unreal Engine: Cheat Sheet",
    "section": "",
    "text": "We can also install Unreal Engine through the Unreal Launcher."
  },
  {
    "objectID": "posts/game_development/unreal_engine_cheat_sheet.html#starting-a-project",
    "href": "posts/game_development/unreal_engine_cheat_sheet.html#starting-a-project",
    "title": "Unreal Engine: Cheat Sheet",
    "section": "",
    "text": "Before starting a project, make sure to:\n\nLoad static content\nEnable raytracing\n\nFeel free to pick a template project from the project browser, such as a first-person or a third-person game. Once the new project opens a .uproject file is created in the default ~/Documents/Unreal Projects directory. We can click this file directly to open the project to continue where we left it off in the future."
  },
  {
    "objectID": "posts/game_development/unreal_engine_cheat_sheet.html#unreal-cheat-sheet",
    "href": "posts/game_development/unreal_engine_cheat_sheet.html#unreal-cheat-sheet",
    "title": "Unreal Engine: Cheat Sheet",
    "section": "",
    "text": "Shortcut\nFunction\nOptional explanation\n\n\n\n\nRMB\nRotate camera\n\n\n\nRMB + W or A/S/D\nMovement front/left/back/right\n\n\n\nRMB + E or Q\nMovement up or down\n\n\n\nScroll Wheel Up/Down + RMB\nAdjust camera speed\n\n\n\nLMB\nSelect object in world\n\n\n\nQ\nObject selection mode\n\n\n\nW\nMovement translation mode\n\n\n\nE\nRotation translation mode\n\n\n\nR\nScale translation mode\n\n\n\nDel\nDelete selected object\n\n\n\nH\nHide selected object\n\n\n\nCtrl + D\nDuplicate selected object\nAnother shortcut is holding Opt and dragging the selected object\n\n\nHold Shift + LMB\nSelect multiple objects\n\n\n\nHold Ctrl + LMB\nDeselect multiple objects from a multi-object selection\n\n\n\nF\nFocus on selected object\nIf we’re lost in our scene we can always select an object in the Outliner window and press F to re-focus on the object\n\n\nG\nGame view mode\nHides widgets, see the world as if in-game\n\n\nCmd + F11\nImmersive mode\nHides all windows and shows the world in full screen (note: we may also need to press Fn)\n\n\nCtrl +  Space \nOpen Content Drawer\nThe Content Drawer is what contains all of our in-game assets (3D/2D assets, code, etc.)\n\n\nOpt + P\nPlay game\n\n\n\nEsc\nExit game and go back to Edit mode\n\n\n\nShift + 1 or 2/3/…\nChange Unreal Editor mode\nThe default is Selection Mode. Other modes include Landsaping Mode, Foliage Mode, etc. These bring up various tools to edit landscapes and make foliage (as their names suggest)\n\n\nHold L + LMB\nRotate sun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView mode\nFunction\nOptional explanation\n\n\n\n\nWireframe\nShows only the wireframe mesh\n\n\n\nUnlit\nShows the world disregarding lighting information\nUseful for visibility when editing in dark environments\n\n\nLight only\nThe world with lighting alone (i.e. without material information)\n\n\n\n\n\n\n\nThe translation modes and snap tools are available in the top-right corner of the viewport.\n\n\n\n\n\n\nFigure 1: Translation and snap tools\n\n\n\nFrom left to right, we have:\n\nObject selection mode\nTranslation mode\nRotation mode\nScale mode\nToggle for coordinate system (global vs local) relative to which the transform gizmo is shown\nSurface snapping options (controls how objects snap to existing surfaces)\nTranslation snapping\nRotation snapping\nScale snapping\nCamera speed adjustment\nMinimize/maximize current viewport\n\nThe default viewport includes a grid, this can be disabled in Show menu in the top-left corner of the viewport by un-checking Grid.\n\n\n\nThere are two ways to add objects to the scene. The first is through Add Object in the top-left corner.\nClick \nThe second way is by using the Content Drawer in the bottom-left corner.\nClick  or use shortcut Ctrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nThe latter method is for adding custom game objects (blueprints as well as 2D/3D assets and more).\nWe often need to see where a particular game asset is located in the Content Drawer. The shortcut to find any object inside the Content Drawer is Ctrl + B while having the object selected.\nAll the assets that make up our game, such as custom 3D assets, custom blueprints (code), etc are stored within the /Content folder of the project.\n\n\n\n\n\nThe Details window looks as follows.\n\n\n\n\n\n\nFigure 2: Details window\n\n\n\nIt contains all the details about a selected object such as:\n\nTransformation information (coordinates, angle of rotation, and scale)\nMaterial information\nPhysics and many more…\n\n\n\n\nThe Outliner window looks as follows.\n\n\n\n\n\n\nFigure 3: Outliner window\n\n\n\nIt contains all the objects that make up the scene, as well as options to show/hide, save, and pin objects:\n\n\n\n\nAssuming we’ve selected the third person template, go into the Content Browser and go to /Content/ThirdPerson/Blueprints to access the blueprints of the third person character.\n\n\n\n\n\n\nFigure 4: Content Drawer - Third person blueprints\n\n\n\nClick on the humanoid. Feel free to dock the newly opened window (as a new tab) next to the open viewport in the Unreal Editor.\nA blueprint consists of three views:\n\nEventGraph\nConstruction Script\nViewport\n\n\n\nContains most of the blueprint’s logic.\n\n\n\n\n\n\nFigure 5: Blueprints - Event graph\n\n\n\n\n\n\nMore on this later…\n\n\n\n\n\n\n\n\n\nFigure 6: Blueprints - Viewport\n\n\n\nThis view shows everything within the selected blueprint as an object. Objects in a blueprint are called components. The Components window is the equivalent of the Outliner window but for blueprints. Rather than showing every object in the world, it shows every component in the blueprint.\n\n\n\n\n\n\nFigure 7: Blueprints - Components window\n\n\n\nThe My Blueprint window contains all the nodes and variables contained in the blueprint. We can click on the EventGraph in the Graphs tab of this window to go into the EventGraph view directly.\n\n\n\n\n\n\nFigure 8: Blueprints - My Blueprint window"
  },
  {
    "objectID": "python_environments/leetcode-sandbox/lib/python3.13/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "python_environments/leetcode-sandbox/lib/python3.13/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html",
    "title": "LC121: Best Time to Buy and Sell Stock",
    "section": "",
    "text": "We are given an array of prices where prices[i] is the price of a given stock on the i-th day.\nWe want to maximize our profit by choosing a single day to buy one stock and choosing a different day in the future to sell that stock. Return the maximum profit you can achieve from this transaction. If you cannot achieve any profit, return 0.\nExample 1\nInput: prices = [7,1,5,3,6,4]\nOutput: 5\nExplanation\nBuy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Note that buying on day 2 and selling on day 1 is not allowed because you must buy before you sell\nExample 2:\nInput: prices = [7,6,4,3,1]\nOutput: 0\nExplanation\nIn this case, no transactions are done and the max profit = 0."
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#proof-of-invariance-of-max_profit",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#proof-of-invariance-of-max_profit",
    "title": "LC121: Best Time to Buy and Sell Stock",
    "section": "Proof of Invariance of max_profit",
    "text": "Proof of Invariance of max_profit\nThe loop above does one of only two things at some iteration k: either it updates min_price or it doesn’t. These are, obviously, exclusive scenarios.\nSuppose it doesn’t update min_price and let’s label this case as Case 1. In this case prices[k] &gt; min_price. The first loop invariant, min_price holds the lowest price dip up to, and including, the index k (we’ll leave the proof of this to the reader). In this first case, the difference of prices[k] and min_price is calculated and max_profit is updated if and only if the difference is greater than the value of max_profit at the end of the previous iteration. This guarantees that max_profit still holds the maximum profit up to, and including, index k at the end of the current iteration.\nIn the other opposite case (Case 2), when the loop does update min_price at the current iteration, it proceeds to the subsequent iteration with max_profit still holding the maximum profit up to, and including, only index k (not k+1). If prices[k+1] is, again, less than min_price (which is just prices[k] at that point) then the loop just goes on updating min_price until it encounters an uptick in the price. Note that if the prices just keep decreasing until the very end, then the proof is complete. Since the prices just keep decreasing monotonically, max_profit (which holds the answer up to index k) is actually the final answer – The perfect time to sell, if the stock just keeps crashing after some index k, would be at index k. So, suppose we’re in the interesting case when there is no monotonic crash till the very end. Then, at some future iteration corresponding to index j (where j &gt; k+2 because the k+1-th index which, by assumption, represents a price dip corresponds to iteration k+2), we have prices[j] &gt; min_price. But notice that this puts us, again, in the familiar Case 1 whereby min_price does get updated. We’ve already shown that max_profit holds the maximum profit up to, and including, the current index in that case.\nHence, max_profit is a loop invariant in both of the cases above (which constitute the set of all possible cases). Therefore, at the last iteration of the loop, max_profit holds the maximum profit up to, and including, the last index n-1 (for an n-sized problem). In other words, it holds the final solution."
  },
  {
    "objectID": "posts/leetcode/lc53_maximum_subarray.html",
    "href": "posts/leetcode/lc53_maximum_subarray.html",
    "title": "LC53: Maximum Subarray",
    "section": "",
    "text": "Given an integer array nums, find the subarray with the largest sum, and return its sum.\nExample 1\nInput: nums = [-2,1,-3,4,-1,2,1,-5,4]\nOutput: 6\nExplanation\nThe subarray [4,-1,2,1] has the largest sum 6.\nExample 2\nInput: nums = [1]\nOutput: 1\nExplanation\nThe subarray [1] has the largest sum 1."
  },
  {
    "objectID": "posts/leetcode/lc53_maximum_subarray.html#similarity-to-other-subarray-problems",
    "href": "posts/leetcode/lc53_maximum_subarray.html#similarity-to-other-subarray-problems",
    "title": "LC53: Maximum Subarray",
    "section": "Similarity to Other Subarray Problems",
    "text": "Similarity to Other Subarray Problems\nThe Maximum Subarray problem is similar to the Best Time to Buy and Sell Stock problem (see post) and a number of other subarray problems. All of them lends themselves to an iterative, tabulated DP solution. In fact, the Best Time to Buy and Sell Stock solution closely resembles Kadane’s algorithm (even on sight). The similarities between these two problems are due to the fact that both problems are concerned with finding some optimal score over a contiguous subarray. Whereas Kadane’s is concerned with the subarray having maximum sum, the (nameless) algorithm that solves Best Time to Buy and Sell Stock is concerned with maximum profit (which is defined as the difference between the last element of the optimal subarray and the first one). The score function (how it determines optimality), and the contiguity requirement are what give rise to the key optimal sub-structures in both problems.\nWe can prove the correctness of Kadane’s algorithm by proving the optimal sub-structures (which are simply stated in this post), and by using loop invariants to show that the overall answer must be correct. But, we will omit the proof here. Instead, in this post, we will give the intuition for how one might derive the algorithm from scratch. To see an example of a proof, check out the Best Time to Buy and Sell Stock post, where we don’t spend any time solving the problem and, instead, offer its proof of correctness. The proof of correctness of Kadane’s algorithm will be very similar to the proof offered in that complementary post (and it is left as an exercise)."
  },
  {
    "objectID": "posts/leetcode/lc53_maximum_subarray.html#kadanes-algorithm",
    "href": "posts/leetcode/lc53_maximum_subarray.html#kadanes-algorithm",
    "title": "LC53: Maximum Subarray",
    "section": "Kadane’s Algorithm",
    "text": "Kadane’s Algorithm\n\nDynamic Programming - The Key Idea\nThe key idea behind dynamic programming is to solve sub-problems once, store their results (one way or another), and use them to solve the larger problem. Usually there’s some optimal sub-structure to DP problems that make this type of optimization possible – optimal sub-structure that may reveal itself in the process of walking through an example. Optimal sub-structures are just computationally cheap steps to obtain the solution to the larger problem from that of its sub-problems.\n\nTwo Approaches\n\n\n\n\n\n\n\nApproach\nImplementation details\n\n\n\n\nTop-Down (Memoization)\nUses recursion to solve the problem, and memoization to store the solutions to the sub-problems\n\n\nBottom-Up (Tabulation):\nIteratively solves sub-problems, in a specific order, eliminating the need for recursion (this technique is also known as tabulation)\n\n\n\n\n\n\nApplying DP to the Maximum Subarray Problem\nKadane’s algorithm, which solves this problem in a single pass, relies on two top-down, optimal sub-structures that exist within this problem. Once uncovered, these will give us immediate clues into how to write a recursive solution (but this will be left as exercise). In this post we will give the iterative, bottom-up, and tabulated algorithm known as Kadane’s Algorithm. It’s worth noting that one of the optimal sub-structures in this problem stems from the contiguity requirement (of subarrays), which explains why a set of algorithms like Kadane’s solve a variety of other subarray-related problems (for example, Best Time to Buy and Sell Stock). These optiomal sub-structures, ultimately, lead to a quadratic reduction in the total number of passes required over subarrays when computing the answer.\nFirst, let’s break the overall problem down into its constituent sub-problems. The sub-problems will be solved in a particular order such that the solution of one, stored in memory, obtains the solution to the next sub-problem (and so on). We shall see that, in fact, this approach ends up not requiring us to do any sums over any subarrays at all! There are several ways of breaking a problem down array problems into sub-problems using clever indexing.\nOne way is to observe that any subarray ends (or begins) at some index k. We may define a pure abstraction: The solution to the problem of size-k+1 (i.e. index 0 corresponds to a problem of size 1). Note that this definition isn’t pulled out of thin air, it’s the global maximum for the k+1-sized sub-problem (i.e. the solution to the overall problem were it to have size k+1). So, let’s call it by its name: global_max[k] (where k is an index, not an iteration).\nWe can define another such useful abstraction: The solution to the problem, if the optimal subarray was constrained to those subarrays which end on index k. Note that this, also, isn’t pulled out of thin air. It’s the local maximum at index k (i.e. the best guess for the answer while lacking global context). So, we call it local_max[k].\nNote that global_max[k] and local_max[k] aren’t the same thing. It’s easy to get lost in trying to solve this problem by conflating local_max with global_max, but they’re not the same. They’re also the only variables needed to implement a single-pass solution, as we’ll soon see. Let’s see how these two definitions differ.\nThere may be an input array for which the best local_max[k] can achieve are negative sums. For instance, take k=3 in the following Manim animation (which demonstrates such a case). First, let’s import manim…\n\nfrom manim import *\n\nThe following is the Manim code that generates the animation below. It’s collapsed, because it’s not the focus of this post.\n\n\nCode\n%%manim -qh KadaneAlgoDemo\n\nclass KadaneAlgoDemo(Scene):\n    def construct(self):    \n        # Create table\n        array=[[1,-2,3,-5,2,3]]\n        table = IntegerTable(array, include_outer_lines=True)\n        self.play(Write(table))\n        # Get table out of the way...\n        self.wait(1)\n        self.play(table.animate.scale(0.25)) \n        self.play(table.animate.to_corner(UP + LEFT))\n\n        # Positioning of subarrays\n        h_shift = 6.45\n        v_shift = 0.5\n\n        # Print subarray(s) ending at index 0\n        index = 0\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT + 2.5 * UP))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i]    \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n        self.wait(1)\n\n        # Positioning of subarrays\n        h_shift -= 3 + 1*0.20\n        v_shift = 0.5 # Reset vertical shift\n\n        # Print subarray(s) ending at index 1\n        index = 1\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT + 2.5 * UP))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i]           \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n            for idx in range(index - left):\n                subtable.add_highlighted_cell((1,idx + 1), color=YELLOW_D)\n                subtable.get_entries((1, idx + 1)).set_color(BLACK)  # Darken the color of the text against the lighter background\n            subtable.add_highlighted_cell((1, index - left + 1), color=BLUE_E)\n        self.wait(1)\n\n        # Positioning of subarrays\n        h_shift -= 3 + 2*0.20\n        v_shift = 0.5 # Reset vertical shift\n\n        # Print subarray(s) ending at index 2\n        index = 2\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT + 2.5 * UP))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i] \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n            for idx in range(index - left):\n                subtable.add_highlighted_cell((1,idx + 1), color=YELLOW_D)\n                subtable.get_entries((1, idx + 1)).set_color(BLACK)  # Darken the color of the text against the lighter background\n            subtable.add_highlighted_cell((1, index - left + 1), color=BLUE_E)\n        self.wait(1)\n\n        # Positioning of subarrays\n        h_shift -= 3 + 3*0.20\n        v_shift = 0.5 # Reset vertical shift\n\n        # Print subarray(s) ending at index 3\n        index = 3\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT + 2.5 * UP))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i] \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n            for idx in range(index - left):\n                subtable.add_highlighted_cell((1,idx + 1), color=YELLOW_D)\n                subtable.get_entries((1, idx + 1)).set_color(BLACK)  # Darken the color of the text against the lighter background\n            subtable.add_highlighted_cell((1, index - left + 1), color=BLUE_E)\n        self.wait(1)\n\n        # Positioning of subarrays\n        h_shift = 5 # Reset horizontal shift\n        v_shift = 3 # Reset vertical shift\n\n        # Print subarray(s) ending at index 4\n        index = 4\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i] \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n            for idx in range(index - left):\n                subtable.add_highlighted_cell((1,idx + 1), color=YELLOW_D)\n                subtable.get_entries((1, idx + 1)).set_color(BLACK)  # Darken the color of the text against the lighter background\n            subtable.add_highlighted_cell((1, index - left + 1), color=BLUE_E)\n        self.wait(1)\n\n        # Positioning of subarrays\n        h_shift -= 5 + 4*0.20\n        v_shift = 3 # Reset vertical shift\n\n        # Print subarray(s) ending at index 4\n        index = 5\n        index_text = Text(f\"i = {index}\")\n        self.play(Write(index_text))\n        self.play(index_text.animate.scale(0.35))\n        self.play(index_text.animate.shift(h_shift * LEFT))\n        for left in range(index+1):\n            subarray=[array[0][left:index+1]]\n            subtable = IntegerTable(subarray, include_outer_lines=True)\n            self.play(Write(subtable))\n            self.play(subtable.animate.scale(0.25)) \n            self.play(subtable.animate.shift(h_shift * LEFT + 2.5 * UP + v_shift * DOWN))\n            subarray_sum = sum(subarray[0]) # This is local_max[i] \n            subarray_sum_text = Text(f\"Sum: {subarray_sum}\")\n            self.play(Write(subarray_sum_text))\n            self.play(subarray_sum_text.animate.scale(0.35))\n            self.play(subarray_sum_text.animate.next_to(subtable, RIGHT))\n            h_shift -= 0.20\n            v_shift += 0.5\n            for idx in range(index - left):\n                subtable.add_highlighted_cell((1,idx + 1), color=YELLOW_D)\n                subtable.get_entries((1, idx + 1)).set_color(BLACK)  # Darken the color of the text against the lighter background\n            subtable.add_highlighted_cell((1, index - left + 1), color=BLUE_E)\n        self.wait(5)\n\n\n\n\n\n\n\nVideo\nKadane’s Demo\n\n\n\n\nFigure 1\n\n\n\nIgnore the highlighted cells for now, we will explain what the colors designate later.\nIn the case above, global_max[k] actually retains its previous value (global_max[k-1]), totally omitting the k-th cell from consideration. But the word Sum in the video is closely related to local_max[k]. For a given index k, local_max[k] is just the greatest of the Sum values for that index. This spells out the following optimal sub-structure:\nglobal_max[k] = max(global_max[k-1], global_max[k-1] + local_max[k])\nNote that this is a greedy-choice update for global_max, because it asserts that taking the local_max step at each turn yields the answer. Note also that it stems from the optimality (i.e. scoring) function being the sum in this problem. In other, similar, problems (like Best Time to Buy and Sell Stock the scoring function of that problem may yield another optimal sub-structure that can be exploited.\nNotice that this optimal sub-structure gives us a shortcut by which to update global_max[k], if we already have local_max[k] for free (by some dark magic). If we only had a way to go from local_max[k-1] to local_max[k], we’d get to the final solution very quickly. We could start with global_max[0], the solution to the base sub-problem which is trivially known, and substitute in this way, in a single pass, until we got to global_max[n] (for some n-sized problem). Luckily for us, there is such a cheap rule that obtains local_max[k] from local_max[k-1]. It is, again, the result of an optimal sub-structure stemming from the contiguity requirement.\nFor any given index k, to obtain local_max[k], we need not compute the sums of all the subarrays ending at index k. Since we know local_max[k-1] from solving the previous sub-problem, the update is simply:\nlocal_max[k] = max(local_max[k-1] + nums[k], nums[k])\nTo convince ourselves of this, let’s watch the Manim video again and let’s pay close attention to the yellow and blue cells. The video is nothing but the brute-force approach laid out bare. The cell highlights in the video expose the above optimal sub-structure. In general, sub-structures tend to reveal themselves during the brute-force stage so always start algorithmic problems by working through an example manually. In the video, you’ll notice that any subarray ending at index k can be divided into two parts, a subarray ending at index k-1 (highlighted in yellow) and the single-element subarray nums[k] (in blue). Since we know local_max[k-1], to find out local_max[k] we no longer need to compute the sums of all the possible subarrays ending at k-1 – That’s a problem we’ve solved already. Rather, local_max[k] will always be either nums[k] itself, or the sum nums[k] + local_max[k-1]. So we can do away with much of the summation from the brute force approach.\nLet’s walk through a potential solution that uses these two optimal sub-structures. Because we know nums[0] and local_max[0] (trivially), we can find out local_max[1] by using the expression for local_max[k]. And, since we have global_max[0] (again, trivially), we can find global_max[1] by using the expression for global_max[k]. It’s easy to see that proceeding iteratively in this manner, by first updating local_max[k] then global_max[k], we can arrive at the solution to the overall n-sized problem (i.e. global_max[n]) without the need to compute sums over subarrays at all!"
  },
  {
    "objectID": "posts/leetcode/lc53_maximum_subarray.html#final-solution",
    "href": "posts/leetcode/lc53_maximum_subarray.html#final-solution",
    "title": "LC53: Maximum Subarray",
    "section": "Final Solution",
    "text": "Final Solution\nNow we just need to implement a loop that updates local_max and global_max in this particular manner. For more involved DP problems, we may need a matrix (or some other higher-dimensional data structure) to store the solutions to the sub-problems. But for this problem, two loop invariants (local_max and global_max) will suffice. Let’s write the solution.\n\nnums = [-2,1,-3,4,-1,2,1,-5,4]\n\n\nlocal_max = float('-inf')\nglobal_max = float('-inf')\n\nfor i, num in enumerate(nums):\n    if num &gt; local_max + num:\n        local_max = num\n    else:\n        local_max = local_max + num\n    if global_max &lt; local_max:\n        global_max = local_max\n\nprint(global_max)\n\n6"
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "",
    "text": "A linear program is a special type of convex optimization problem in \\(n\\)-dimensions that has a linear objective and a constraint set that’s a polytope. That is, its constraint set is an intersection of \\(n\\)-dimensional linear inequalities (halfspaces) and linear equalities (hyperplanes).\nIn matrix form, it may be stated as\n\\[\n\\begin{cases}\n\\min_x: c^Tx\n\\\\\ns.t.: \\begin{aligned} &A_1x \\leq b_1\n\\\\\n&A_2x \\geq b_2\n\\\\\n&A_3x = b_3\n\\end{aligned}\n\\end{cases}\n\\]\nwhere \\(c \\in \\mathbb{R}^n\\) is the cost vector of the objective function, \\(x \\in \\mathbb{R^n}\\) is the decision variable, \\(A_1 \\in \\mathbb{R}^{m \\times n}\\), \\(b_1 \\in \\mathbb{R}^m\\) and \\(A_2 \\in \\mathbb{R}^{p \\times n}\\), \\(b_2 \\in \\mathbb{R}^p\\) together define the collection of linear inequality constraints, and \\(A_3 \\in \\mathbb{R}^{q \\times n}\\) and \\(b_3 \\in \\mathbb{R}^q\\) define the collection of linear equality constraints.\nAs we will shortly prove, an LP in any form such as the one above can be converted into its standard form\n\\[\n\\begin{cases}\n\\min_x: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax = b\n\\\\\n&x \\geq 0\n\\end{aligned}\n\\end{cases} \\dagger\n\\]\n\n\nLinear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they’re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP’s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc.\nThe reason LP’s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster."
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html#applications",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html#applications",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "",
    "text": "Linear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they’re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP’s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc.\nThe reason LP’s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster."
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html#introducing-slack-variables",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html#introducing-slack-variables",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "Introducing Slack Variables",
    "text": "Introducing Slack Variables\nThe inequality constraint \\(A_1x \\leq b_1\\) has slack. Formally, we can define vector \\(s \\geq 0\\) (component-wise), that bridges the gap between \\(A_1x\\) and \\(b_1\\), that is s.t. \\(A_1x + s = b_1\\).\nSince this introduces new variables, we have to represent those in the objective and the equality constraints in a way that doesn’t affect the optimization outcome.\nThe LP becomes\n\\[\n\\begin{cases}\n\\min_x: c^Tx + \\mathbf{0}^Ts\n\\\\\ns.t.: \\begin{aligned} &A_1x + s = b_1\n\\\\\n&A_2x + 0s = b_2\n\\\\\n&s \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\nThis LP is equivalent to the one before. Namely, if the previous optimizer was \\(x^*\\), the optimizer in the new LP is the concatenation \\([x^*, b_1 - A_1x]\\) which gives the same optimal value in the objective function.\nThis is almost in standard form, an LP with only equality constraints, and non-negativity constraints. However, the decision variable of this LP is the concatenation \\([x,s]^T\\), whereas the non-negativity applies to \\(s\\) alone.\nThe next step is to decompose \\(x\\) as \\(x = x^+ - x^-\\) where \\(x^+,x^- \\geq 0\\) respectively contain only the positive and only the negative entries of \\(x\\). That is, \\(x^+\\), and \\(x^-\\) have entries \\(x_i^+ = \\max\\{0, x_i\\}\\) and \\(x_i^- = -\\min\\{0, x_i\\}\\).\nWith this substitution we get\n\\[\n\\begin{cases}\n\\min_x: c^Tx^+ - c^Tx^- + \\mathbf{0}^Ts\n\\\\\ns.t.: \\begin{aligned} &A_1x^+ - A_1x^- + s = b_1\n\\\\\n&A_2x^+ - A_2x^- + 0s = b_2\n\\\\\n&x^+, x^-, s \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\nWhich is an LP in standard form \\(\\dagger\\)."
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html#extreme-points---geometric-definitions",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html#extreme-points---geometric-definitions",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "Extreme Points - Geometric Definitions",
    "text": "Extreme Points - Geometric Definitions\nFirst, let’s give a couple of geometric definitions of an extreme point.\n\nDefinition 1:   A point \\(x\\) is an extreme point of a polytope \\(P\\) if it is not the convex combination of any other two points in the polytope.\n\nThat is, if \\(\\exists y,z \\in P\\) and \\(\\lambda \\in [0,1]\\) s.t. \\(x = \\lambda y + (1- \\lambda)z\\) then \\(x\\) is not an extreme point of \\(P\\).\n\nDefinition 2:   A point \\(x\\) is an extreme point of a polytope \\(P\\) if it is the unique optimum for some cost vector \\(c\\).\n\nThat is, if \\(\\exists c \\in \\mathbb{R}^n\\) s.t. \\(c^Tx &lt; c^Ty \\ \\ \\forall y \\in P\\) then \\(x\\) is an extreme point."
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html#extreme-points---algebraic-definition",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html#extreme-points---algebraic-definition",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "Extreme Points - Algebraic Definition",
    "text": "Extreme Points - Algebraic Definition\nIt’s useful to define an extreme point algebraically. To that end, let’s define the concept of a basic feasible solution (BFS).\nSuppose we have the polytope \\(\\{x : Ax \\leq b, Dx = f\\}\\).\n\nDefinition:   An active constraint at \\(x\\) is a constraint that’s satisfied through strict equality.\n\nThat is, the \\(i\\)-th constraint is said to be active at x if \\(a_i^Tx = b_i\\).\nThis can be thought of as \\(x\\) being on the edge of the halfspace defined by \\(a_i^Tx \\leq b_i\\).\nWe can also define the active set at \\(x\\) as the set of all active constraints at \\(x\\).\nSo, the active set at \\(x\\) is \\(\\mathcal{A}_x = \\{ a_i : a_i^Tx = b_i \\} \\cup \\{ d_i : d_i^Tx = f_i \\}\\), where \\(\\{ d_i : d_i^Tx = f_i \\}\\) is included for completeness.\n\nBasic Feasible Solution\nWe are now ready to define what it means for a point \\(x\\) to be a basic feasible solution of a linear program.\n\nDefinition:   The point \\(x\\) is a basic feasible solution (BFS) of the linear program if its active set \\(\\mathcal{A}_x\\) contains exactly \\(n\\) linearly independent vectors where \\(n\\) is the dimension of \\(x\\).\n\nLet’s ponder the BFS definition for a minute.\nImagine a closed polytope in 2D. Each of its vertices are defined by, at least, two intersecting lines. It’s possible that a vertex is the result of the intersection of three or more lines, but deleting all but two of those lines will still retain the vertex. In other words, two linearly independent (i.e. non-parallel) constraints define an extreme point in 2D.\nThe BFS definition is simply a generalization of this insight to \\(n\\)-dimensions.\nAs we will prove shortly, BFS and extreme point are synonymous. In fact, the following are equivalent:\n\n\\(x\\) is an extreme point by Definition 1.\n\\(x\\) is an extreme point by Definition 2.\n\\(x\\) is a basic feasible solution.\n\n\n\nMatrix-Vector Formulation of Basic Feasible Solutions\nTaking as our starting point an LP in standard form \\(\\dagger\\) we can characterize basic feasible solutions in matrix-vector form.\nTake the standard constraint set \\(\\Omega = \\{ Ax = b, x \\geq 0 \\}\\) and let’s make a few simplifying assumptions.\n\n\\(A\\) is \\(m \\times n\\) with \\(m \\leq n\\).\n\\(A\\) is full-rank\n\\(b \\geq 0\\)\n\\(A\\) has form \\(A = [B,D]\\) where \\(B\\) is an \\(m \\times m\\) full-rank matrix and \\(D\\) is the rest of \\(A\\).\n\nSome of these assumptions impose restrictions on \\(\\Omega\\), whereas others are without loss of generality.\nAssumption 1 is simply there to make the problem interesting. Were \\(n &gt; m\\), the system of equalities would be over-determined and the constraint set would either be empty or contain a single point, which itself would be the optimum. It is, therefore an assumption which is not done without loss of generality.\nAssumption 2 is equivalent to saying \\(rank(A) = m\\). That is to say, all \\(m\\) rows of \\(A\\), as well as \\(m\\) of the \\(n\\) columns of \\(A\\), are linearly independent. This assumption is also not done without loss of generality. Having less linearly independent rows corresponds to having less non-redundant constraints which clearly affects the constraint set \\(\\Omega\\).\nAssumption 3 is done W.L.O.G. since the signs of \\(A\\)’s row entries can always be flipped.\nAssumption 4 is also done W.L.O.G. because if \\(A\\) contains a full-rank \\(m \\times m\\) submatrix per Assumption 2, then \\(A = [B,D]\\) is a re-ordering of \\(A\\) which adds no further restrictions on \\(\\Omega\\).\nFor \\(\\Omega\\) that satisfies Assumptions 1-4, the basic feasible solutions can be reformulated as follows.\n\nDefinition:   Let \\(x_B\\) be such that. \\(Bx_B = b\\). Then the concatenation \\(x = [x_B, 0]^T\\) is a solution to \\(Ax = b\\). Such solutions are called feasible solutions. Furthermore, if \\(x_B \\geq 0\\), such solutions are called basic feasible solutions.\n\nNote that, for the case we’re in, this is consistent with the earlier definition of a BFS.\nLet \\(x\\) be a BFS according to this definition. \\(Ax = b\\) poses a set of \\(m\\) linearly independent constraints since \\(rank(A) = m\\), whereas \\(x \\geq 0\\) poses a set of \\(n\\). But \\(x = [x_B, 0]^T\\) is a vector at which all \\(m\\) of the equality constraints \\(Ax = b\\) are active, and \\(n-m\\) of the inequality constraints \\(x \\geq 0\\) are also active. So in total \\(n\\) linearly independent constraints are active at a BFS, which is consistent with the earlier definition.\n\n\nBasic Feasible Solutions and Extreme Points are Equivalent\nTo formally prove that basic feasible solutions are extreme points in the geometric sense, consider the following theorem and its proof.\n\nTheorem:   The point \\(x\\) is an extreme point of \\(\\Omega = \\{ Ax =b, x \\geq 0 \\}\\) if and only if it is a basic feasible solution.\n\n\nProof\nSufficiency \\(\\implies\\):\nLet \\(x\\) be an extreme point of \\(\\Omega\\). Since it’s in \\(\\Omega\\), \\(x \\geq 0\\) and \\(Ax = b\\).\nEquivalently, \\(\\sum_{i=1}^n x_ia_i = b\\) where the \\(a_i\\)’s are the column vectors of \\(A\\).\nNote that \\(x\\) must contain zero entries, since it takes \\(n\\) linearly independent active constraints to be an extreme point and only \\(m\\) come from the equality constraints \\(Ax = b\\).\nBy Assumption 1, \\(m\\) of \\(A\\)’s columns are linearly independent. We’d like to claim that these \\(m\\) are exactly those corresponding to the non-zero \\(x_i\\) entries.\nIf this claim turns out to be true, then the full-rank \\(m \\times m\\) submatrix \\(B\\) will contain exactly those \\(m\\) columns. And \\(x = [x_B, 0]^T\\), where \\(x_B\\) are the non-zero entries of \\(x\\), would be a BFS. \\(\\ast\\)\nSo, let’s prove the linear independence claim using a contradiction argument.\nWithout loss of generality, through rearrangement, let the first \\(m\\) elements be the nonzero entries. That is, \\(x_1, ..., x_m &gt; 0\\), and \\(x_{m+1}, ... ,x_n = 0\\).\nThen \\(\\sum_{i=1}^n x_ia_i = \\sum_{i=1}^m x_ia_i = b\\).\nTowards contradiction, assume \\(a_1, ..., a_m\\) are linearly dependent. Then \\(\\exists y_1, ..., y_m \\in \\mathbb{R}\\) not all zero s.t. \\(y_1a_1 + ... + y_ma_m = 0\\)\nTake \\(\\epsilon &gt; 0\\) to be very small. Small enough so that \\(x_i \\pm \\epsilon y_i &gt; 0 \\ \\ \\forall i = 1,...,m\\).\nDefine two points as\n\\(z^1 = [x_1 - \\epsilon y_1, ..., x_p - \\epsilon y_p, 0, ..., 0]^T\\) and, \\(z^2 = [x_1 + \\epsilon y_1, ..., x_p + \\epsilon y_p, 0, ..., 0]^T\\).\nThese points clearly satisfy \\(z_1,z_2 \\geq 0\\), so they they satisfy one of \\(\\Omega\\)’s constraints.\nFurthermore,\n\\(Az^1 = \\sum_{i=1}^m z^1_ia_i = \\sum_{i=1}^m x_ia_i - \\epsilon \\sum_{i=1}^m y_ia_i = b\\) since \\(\\sum_{i=1}^m y_ia_i = 0\\).\nand similarly \\(Az^2 = b\\).\nSo, \\(z^1\\), and \\(z^2\\) are indeed in \\(\\Omega\\).\nBut note that \\(x = \\frac{z^1 + z^2}{2}\\) is a convex combination of two points in \\(\\Omega\\), which contradicts the assumption that it’s an extreme point.\nHence, \\(a_1,...,a_m\\) must be linearly independent. This concludes the proof by \\(\\ast\\).\nNecessity \\(\\impliedby\\):\nSuppose \\(x\\) is a BFS and assume, towards contradiction, that it’s not and extreme point of \\(\\Omega\\).\nThen \\(\\exists y,z \\in \\Omega\\) with \\(y \\ne z\\) s.t. \\(x = \\alpha y + (1- \\alpha) z\\) for some \\(\\alpha \\in (0,1)\\).\nBut since \\(y,z \\in \\Omega\\) they satisfy \\(Ay = Az = b\\), so \\(Ay - Az = A(y - z) = 0\\).\nThat is \\((y_1 - z_1)a_1 + ... + (y_m - z_m)a_m = 0\\).\nBut since \\(y \\ne z\\), not all \\((y_i - z_i) = 0\\). So, \\(a_1,..., a_m\\) are linearly dependent. This contradicts the assumption that \\(x\\) was a BFS."
  },
  {
    "objectID": "posts/optimization/linear_programs_and_lp_geometry.html#the-extreme-point-theorem",
    "href": "posts/optimization/linear_programs_and_lp_geometry.html#the-extreme-point-theorem",
    "title": "Notes: Linear Programs and LP Geometry",
    "section": "The Extreme Point Theorem",
    "text": "The Extreme Point Theorem\nWhy devote so much time defining extreme points geometrically, and then again algebraically? As hinted earlier and as shall be proved shortly, optima of linear programs occur at the extreme points. This is the reason LP’s are a class of easy convex optimization problems — the search space for their optima can be reduced to a finite number of extreme points.\n\nThe Extreme Point Theorem:   If a linear program has a finite optimum, and its constraint polytope has at least one extreme point, then there is an extreme point which is optimal.\n\nSo, if we want to solve linear programs we need only consider the extreme points.\nLet’s prove the theorem through induction on the dimension.\n\nProof\nTake the following general LP and assume it has a finite optimum. Assume also that its constraint polytope has, at least, one extreme point.\n\\[\n\\begin{cases}\n\\min_{x}: c^Tx\n\\\\\ns.t.: x \\in \\mathcal{P}\n\\end{cases}\n\\]\nAssume the theorem is true for this LP with an \\((n-1)\\)-dimensional constraint polytope. The objective is to show that it’s also true for the same LP with an \\(n\\)-dimensional constraint polytope.\nLet \\(v\\) be the optimal value of the LP.\nLet \\(Q = P \\cap \\{ x : c^Tx = v \\}\\) be the intersection of the constraint polytope with the level set of the objective function at the optimal value.\nSince \\(Q\\) is the intersection of an \\(n\\)-dimensional polytope \\(P\\) with an additional linear constraint (a hyperplane), it is \\((n-1)\\)-dimensional.\nBy the inductive hypothesis, there is an extreme point \\(x^* \\in Q\\) that’s optimal for the LP.\nBy a contradiction argument, \\(x^*\\) is also an extreme point in \\(P\\).\nSuppose it is not an extreme point in \\(P\\). Then by Definition 1 of extreme point, \\(x^*\\) is a convex combination of two points in \\(P\\). That is, \\(\\exists y,z \\in P\\) s.t. \\(\\lambda y + (1- \\lambda)z = x^*\\) for some \\(\\lambda \\in [0,1].\\)\nBut then \\(\\lambda c^Ty + (1- \\lambda)c^Tz = c^Tx^* = v\\), since \\(x^*\\) is optimal. But the left hand side is a convex combination of scalars, so \\(c^Ty = c^Tz = v\\). This means \\(y,z \\in Q\\), which contradicts the fact that \\(x^*\\) is an extreme point in \\(Q\\).\nHence, \\(x^*\\) must be an extreme point in \\(P\\).\n\n\nSketch for an Alternate Proof\nWe can also prove the Extreme Point Theorem using a recursive argument. Recall that a continuous \\(1\\)-dimensional function \\(f: \\mathcal{D} \\rightarrow \\mathbb{R}\\) on a closed interval \\(\\mathcal{D} \\subset \\mathbb{R}\\) necessarily achieves a min/max either on the endpoints of \\(\\mathcal{D}\\) or somewhere inside. If we additionally stipulate that \\(f\\) is linear, the only possibilities are the endpoints. Extending this logic to linear programs in \\(n\\)-dimensions which have finite optimal solutions, we conclude that the optimal solution cannot occur at any interior point of the constraint polytope \\(\\mathcal{P}\\) and, instead, must occur somewhere on its boundary. But now we can consider the \\((n-1)\\)-dimensional polytopes forming \\(\\mathcal{P}\\)’s boundary separately and apply the same logic to each one recursively. In the base case, we reach the conclusion that the optimal solution must occur at an endpoint of a \\(1\\)-dimensional polytope — a line segment such as \\(\\mathcal{D}\\). Such a point is an extreme point of the constraint polytope."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "",
    "text": "Gradient Descent (GD) is a powerful, yet incredibly simple, iterative optimization algorithm. We can think of it as a greedy algorithm in the setting of continuous optimization. That is, one step of GD is our best attempt at local optimization given limited information about the objective \\(f(x)\\), and having limited computational power at our disposal.\nWe can further qualify what we mean by ‘limited information’ about the objective by introducing a categorization on optimization algorithms – the Oracle Access Model (OAM). In this model, the objective is abstracted into a black box. For each input \\(x\\), the black box gives the algorithm access to the value of the objective, \\(f(x)\\), and, optionally, global information in the form of higher order behavior such as \\(\\nabla f(x)\\), \\(\\nabla^2 f(x)\\), etc. GD is what’s known as a first-order oracle because it’s only allowed access to \\(f(x)\\) and first-order information in the form of \\(\\nabla f(x)\\).\nIt’s important to note that the OAM is not all-inclusive, there are a number of optimization algorithms, such as composite optimization algorithms, that utilize structural information about the objective that goes beyond \\(n\\)-th order behavior. An example of such an algorithm is Proximal Gradient Descent (PGD) which, in addition to \\(f(x)\\) and \\(\\nabla f(x)\\), also has access to the prox operator: \\(Prox_{h,\\eta}(x)\\).\nWe will cover these composite optimization algorithms in later posts. Many of the composite optimization algorithms, such as PGD, are simple modifications of vanilla GD. The modification done in PGD, for example, make it suitable for constrained optimization. For now, however, we focus on the case of unconstrained optimization with oracles, particularly on Gradient Descent, in order to develop the key algorithmic intuition."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#idea-1---greedy-choice-of-direction",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#idea-1---greedy-choice-of-direction",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Idea 1 - Greedy Choice of Direction",
    "text": "Idea 1 - Greedy Choice of Direction\nLet \\(x\\) be the initial iterate, and let the update be given by:\n\\[x^+ = x + \\eta d\\]\nfor some directional unit-vector \\(d\\) and step-size parameter \\(\\eta &gt; 0\\).\nWe base the algorithm on the assumption that the linear approximation of the objective at a the next iterate \\(x^+\\) is a good-enough estimate of its true value at \\(x^+\\).\nThat is:\n\\[f(x^+) = f(x + \\eta d) \\approx f(x) + \\eta \\nabla f(x)^T d \\ \\ \\forall d \\tag{1.1}\\]\nImmediately, a locally optimal choice presents itself to us. Since we wish to minimize \\(f(x)\\), it would be wise to insist that the objective at \\(x^+\\) improves or, at least, does not worsen.\nThat is, we insist:\n\\[f(x^+) \\approx f(x) + \\eta \\nabla f(x)^T d \\leq f(x) \\tag{1.3}\\]\nAnd, since we are greedy in our approach, we wish to make \\(f(x^+)\\) as small as possible. Since, on the RHS, \\(f(x)\\) is fixed and \\(\\eta &gt; 0\\), this amounts to minimizing the scaled inner-product \\(\\nabla f(x)^Td\\). To that end, we choose \\(d\\) opposite and parallel to the gradient, i.e. \\(d = - \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}\\).\nThe update step becomes:\n\\[x^+ = x - \\eta \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}\\]\nBy re-labeling, \\(\\eta\\) can absorb the normalization constant. This obtains the gradient descent update step as it’s often introduced in the textbooks – a step in the negative gradient direction:\n\\[x^+ = x - \\eta \\nabla f(x) \\tag{1.4}\\]\nThis makes intuitive sense because the negative gradient direction is the direction in which the objective decreases most. So, it’s only natural that the update should take us in this most enticing direction."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#idea-2---greedy-choice-of-next-iterate",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#idea-2---greedy-choice-of-next-iterate",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Idea 2 - Greedy Choice of Next Iterate",
    "text": "Idea 2 - Greedy Choice of Next Iterate\nInstead of defining the update step \\(x^+ = x + \\eta d\\) and then choosing the locally optimal direction \\(d\\) greedily, we can choose the update step and the direction, both, in one fell swoop.\nStarting from the linear approximation:\n\\[\nf(y) \\approx f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y \\tag{2.1}\n\\]\nWe can now insist, in a greedy fashion, that the next iterate \\(x^+\\) be the minimizer of the linear approximation. That is, we insist:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) \\tag{2.2}\n\\]\nBut the linear approximation is only local, so it would be wise to distrust it for points far away from the current iterate. In this case, since the linear approximation is, in fact, unbounded below, \\((2.2)\\) would obtain \\(x^+ = \\pm \\infty\\). To avoid this problem, we introduce a parametrized penalty term that prevents \\(x^+\\) from venturing too far from the current iterate \\(x\\). That is:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) + \\eta ||y - x||_2^2 \\tag{2.3}\n\\]\nNow, since the RHS is a a simple quadratic in \\(y\\), it has a unique minimizer which can be found by using the unconstrained optimality condition. This just boils down to taking the gradient of the RHS w.r.t. the optimization variable \\(y\\), setting it to zero, and then solving for the unique root. This procedure obtains:\n\\[x^+ = x - \\frac{1}{2 \\eta} \\nabla f(x)\\]\nBy re-labeling, we, once again, get the canonical form of the GD update step as in \\((1.3)\\) – a step in the negative gradient direction."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#initialization",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#initialization",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Initialization",
    "text": "Initialization\nFrom this point on, we will limit our discussion to convex objectives in order to eliminate the possibility of strictly local optimizers (i.e. non-global optimizers) and inflection points, both of which GD gets stuck at if initialized poorly. This ensures the only stationary points, points at which \\(\\nabla f(x) = 0\\) and the GD update makes no further progress, are global minimizers. On such convex functions, as we will soon discover, GD has a convergence guarantee for all step-sizes independently of initialization."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#fixed-step-size-gd",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#fixed-step-size-gd",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Fixed Step-Size GD",
    "text": "Fixed Step-Size GD\nTo kickstart our analysis of GD, we consider the fixed step-size algorithm first. Let’s take two quintessential convex problems in \\(\\mathbb{R}\\), \\(f(x) = x^2\\) and \\(h(x) = |x|\\), and analyze GD’s performance on them.\n\nSimple Analysis of Fixed Step-Size GD\n\nFirst, let’s run the algorithm on \\(h(x) = |x|\\) for \\(x \\in \\mathbb{R}\\):\nSince \\(|x|\\) is non-differentiable at \\(x = 0\\), the gradient has a discontinuity at \\(x = 0\\). Non-differentiability, such as this, will eventually force us to introduce the notion of sub-gradients, but for now we can get away with it simply by avoiding the gradient’s behavior at \\(0\\). So:\n\\[\nh'(x) =\n\\begin{cases}\n\\begin{aligned}\n-1 \\ &\\textrm{if $x &lt; 0$} \\\\\n1 \\ &\\textrm{if $x &gt; 0$}\n\\end{aligned}\n\\end{cases}\n\\]\nThen, for a fixed \\(\\eta &gt; 0\\), the update step is:\n\\[x^+ = x \\pm \\eta\\]\nwhere the sign of \\(\\eta\\) depends on where the previous iterate, \\(x\\), falls within the domain \\((-\\infty, 0) \\cup (0, \\infty)\\).\n\nNow, consider \\(f(x) = x^2\\) for \\(x \\in \\mathbb{R}\\):\nThe gradient of \\(f(x) = x^2\\) is \\(f'(x) = 2x\\), which means the fixed step-size update is:\n\\[x^+ = x - 2\\eta x\\]\n\nNote that \\(x^* = 0\\) is the unique optimizer of both \\(f(x)\\) and \\(h(x)\\). With this in mind, there are two key observations to make.\nThe first is that, for \\(x\\) far away from \\(x^* = 0\\), the update, \\(2\\eta x\\), is large (in magnitude). So, if the iterate is far from the optimizer, GD makes fast progress towards it.\nThe second observation is that, as \\(x \\rightarrow x^*\\), the update becomes small in magnitude. So, as the iterate comes close to the optimizer, GD takes smaller and smaller steps which converge to \\(0\\) in a summable way. This means, we can get the sub-optimality \\(f(x) - f(x^*)\\) to be \\(\\epsilon\\)-arbitrarily small for any fixed step-size \\(\\eta\\).\nNeither of these observations hold for GD on \\(h(x) = |x|\\) since the update \\(\\eta\\) is fixed regardless of the Euclidean distance between \\(x\\) and \\(x^* = 0\\). In particular, this means GD is not fast for \\(x\\) far away from \\(x^*\\) and does not slow down as \\(x\\) nears \\(x^*\\). Arbitrary accuracy is impossible with a fixed step-size, since the iterates eventually cycle between \\(x^T - \\eta\\) and \\(x^T + \\eta\\) where \\(x^T\\) is the last unique iterate – that is \\(x^T \\in (-\\eta, \\eta)\\). The sub-optimality, consequently, also cycles between two values which depend on the choice of \\(\\eta\\). This is to say that the sub-optimality cannot be \\(\\epsilon\\)-arbitrary small for a fixed choice of \\(\\eta\\). To be clear, there is still convergence but it’s slow and not arbitrarily accurate. Arbitrary accuracy for such problems as this can only be achieved by choosing a sequence of diminishing step-sizes \\(\\{ \\eta_t \\}_{t=1}^T\\) which reduce magnitude of the update since the gradient, itself, is constant. Of course, this sequence must be chosen with care since it’s possible to ‘run out of steam’, so to speak, before reaching the optimizer. The precise criterion is \\(\\eta_t \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) s.t. \\(\\sum_t^\\infty \\eta_t = \\infty\\).\nWe say GD on \\(f(x) = x^2\\) enjoys the self-tuning property, whereas GD on \\(h(x) = |x|\\) does not. This speaks to the fact that the self-tuning is a property of the objective functions, rather than GD itself.\nAs an overview of the theory we will soon develop, functions like \\(x^2\\) (or, more generally, any quadratic in \\(\\mathbb{R}^n\\)) will all have the self-tuning property while functions like \\(|x|\\) will not. This is what ends up introducing the taxonomy of easier-to-harder convex optimization problems mentioned in the previous section. What it means, precisely, to be like \\(x^2\\) or \\(|x|\\) will be made rigorous in the next few sections."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#self-tuning-property",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#self-tuning-property",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Self-Tuning Property",
    "text": "Self-Tuning Property\nFor a convex function that’s \\(M\\)-smooth and \\(m\\)-strongly-convex we have \\((3.1)\\) which, as a reminder, is:\n\\[m||x-y||_2 \\leq ||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y\\]\nFixing iterate \\(x\\), and replacing \\(y\\) with the optimizer \\(x^*\\) we have:\n\\[m||x-x^*||_2 \\leq ||\\nabla f(x) - \\nabla f(x^*)||_2 \\leq M||x-x^*||_2\\]\nSince \\(x^*\\) is an optimizer \\(\\nabla f(x^*) = 0\\), so the above becomes:\n\\[m||x-x^*||_2 \\leq ||\\nabla f(x)||_2 \\leq M||x-x^*||_2\\]\nThe first inequality says that the magnitude of the gradient is at least a constant multiple of the distance from the optimizer. The second inequality says that the magnitude of the gradient is at most a constant multiple of the distance from the optimizer. So, the gradient is large for \\(x\\) far from \\(x^*\\) and gets smaller as \\(x \\rightarrow x^*\\). Since the GD update depends on the magnitude of the gradient, this ensures GD has the self-tuning property. So, smoothness and strong-convexity were, indeed, the ideas needed to encapsulate the self-tuning property."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#quadratic-bounds",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#quadratic-bounds",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Quadratic Bounds",
    "text": "Quadratic Bounds\nSmoothness and strong-convexity, should they hold for a given convex function, give a universal quadratic point-wise upper and lower-bound, respectively, on the function. This is what it means to say that the function is like a quadratic. In a sense, all we’re saying is that a function is tightly asymptotically bounded by a quadratic at every point. That is, at any given point, the function should neither grow slower nor faster than quadratically.\nTo construct the upper and lower-bounds, we use the following two lemmas:\n\nLemma 1:   If \\(f\\) is convex and \\(L\\)-Lipschitz then \\(g(x) = \\frac{L}{2}x^Tx - f(x)\\) is convex.\n\n\nLemma 2:   If \\(f\\) is \\(m\\)-strongly-convex then \\(g(x) = f(x) - \\frac{m}{2}x^Tx\\) is convex.\n\nBoth lemmas are statements of comparative convexity in disguise. Lemma 1 says that \\(\\frac{L}{2}x^Tx\\) is more convex than \\(f\\), whereas Lemma 2 says that \\(f\\) is more convex than \\(\\frac{m}{2}x^Tx\\).\nIt’s not difficult to see how these lemmas can assist in sandwiching \\(f\\) between an upper-bound that’s more convex and a lower-bound that’s less convex.\nThe bounds themselves come from the quadratic approximation of \\(f\\) as:\n\\[f(y) \\approx f(x) + \\nabla f(x)^T(y-x) + \\frac{1}{2}(y-x)^T\\nabla^2 f(x)(y-x)\\ \\ \\forall y\\]\nBy replacing the hessian with its bounds \\(mI\\) and \\(MI\\) and using the above lemmas we obtain the two bounds as:\n\\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{m}{2}||y-x||_2^2 \\ \\ \\forall y\\] \\[\\textrm{and} \\tag {5.1}\\] \\[f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2 \\ \\ \\forall y\\]"
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#the-optimal-fixed-step-size",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#the-optimal-fixed-step-size",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "The Optimal Fixed Step-Size",
    "text": "The Optimal Fixed Step-Size\nWe already showed that an \\(M\\)-smooth and \\(m\\)-strongly-convex function enjoys the advantage of self-tuning. But, without knowing how to choose the step-size, we can still cause GD to make slow progress or even diverge.\nAfter all, gradient descent is based on a local linear approximation of the objective. If we take big steps, we are counting on the linear approximation to be a good-enough estimate far from the current iterate. This may be too optimistic, in which case GD will diverge. Being too pessimistic, however, is also not good. While taking small steps won’t cause GD to diverge, it will make GD painfully slow… Slow to the point of making it worthless in practice.\nLuckily, the quadratic upper-bound in \\((5.1)\\) informs our choice of step-size both in terms of a convergence guarantee and in terms of optimality. First, let’s develop the convergence guarantee.\nBy plugging the update step \\(x^+ = x - \\eta \\nabla f(x)\\) as \\(y\\) into the upper-bound in \\((5.1)\\) we obtain:\n\\[f(x^+) \\leq f(x) + \\eta(1-\\frac{M}{2}\\eta)||\\nabla f(x)||_2^2 \\tag{5.2}\\]\nAs before, it would be wise to insist \\(f(x^+) \\leq f(x)\\), which gives the convergence interval as \\(0 &lt; \\eta &lt; \\frac{2}{M}\\).\nHere, it helps to consider the simple example of quadratics.\n\nExample 1:\nConsider the quadratic form in \\(\\mathbb{R}\\) given by \\(f(x) = \\frac{1}{2}M x^2\\) where \\(x, M \\in \\mathbb{R}\\). Here we can think of \\(M\\) as the only, and therefore the largest, eigenvalue of the \\(1 \\times 1\\) matrix \\([M]\\).\nIts GD update step looks like:\n\\[x^+ = x - \\eta M x = (1 - \\eta M)x\\]\nWhich, by recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[x^T = (1- \\eta M)^Tx_0\\]\nThen, since \\(x^* = 0\\), convergence is guaranteed by ensuring \\(|1 - \\eta M| &lt; 1\\) or, equivalently, \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) as desired.\nThis generalizes to higher dimensions as we shall see in the following example.\n\nExample 2:\nConsider the quadratic form in \\(\\mathbb{R}^n\\) given by \\(f(x) = \\frac{1}{2}x^TQx\\) where \\(x \\in \\mathbb{R}^n\\) and \\(Q \\succeq 0\\).\nIts GD update step looks like:\n\\[x^+ = x - \\eta Qx = (I - \\eta Q)x\\]\nWhich, by recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[x^T = \\underbrace{(I - \\eta Q)\\ldots(I - \\eta Q)}_{\\text{$T$ times}}x_0\\]\nThe eigenvalues \\(\\tilde \\lambda_i\\) of the matrix \\(I - \\eta Q\\) are related to the eigenvalues \\(\\lambda_i\\) of \\(Q\\) according to:\n\\[\\tilde \\lambda_i = 1 - \\eta \\lambda_i\\]\nHence, if \\(\\lambda_{min} = m\\) and \\(\\lambda_{max} = M\\), then \\(\\tilde \\lambda_{min} = 1 - \\eta M\\) and \\(\\tilde \\lambda_{max} =1 - \\eta m\\).\nThe eigenvalues of \\(I - \\eta Q\\) act on the current iterate by scaling it. So, in order to ensure convergence to \\(0\\), we need \\(\\tilde \\lambda_i \\in (-1,1) \\ \\ \\forall i\\). Or, equivalently:\n\\[\\tilde \\lambda_{min} &gt; -1\\] \\[\\textrm{and}\\] \\[\\tilde \\lambda_{max} &lt; 1\\]\nBoth of these together give us \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) as desired.\n\nBut \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) is an interval, not a greedy choice. It’s just a condition that guarantees convergence. By making the greedy choice we can find the optimal step-size within this interval.\nSince the RHS of \\((5.2)\\) is strongly convex, the quadratic upper-bound is guaranteed to have a unique minimizer.\nThe idea is similar to other instances of making a greedy choice we’ve seen so far. Since the function value is upper-bounded by this quadratic, minimizing this upper-bound gives the best guarantee of smallness for the function value available to us without access to higher order information about the objective. So, the greedy choice for the next iterate is:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2\n\\]\nAs always, minimizing a quadratic is easy. After going through the motions we obtain:\n\\[x^+ = x - \\frac{1}{M}\\nabla f(x)\\]\nSo, the optimal fixed step-size is \\(\\eta = \\frac{1}{M}\\).\nWe will see this idea of minimizing a quadratic approximation of the objective, instead of the objective itself, repeat itself when we get to Newton’s Method (NM). However, NM is a second-oder oracle which has access to \\(\\nabla^2 f(x)\\), and hence the quadratic approximation at all points in the domain of the objective is accessible to NM. The remarkable thing about \\(M\\)-smoothness and \\(m\\)-strong-convexity is that they give gradient descent, a first-order oracle, access to universal quadratic bounds which eliminates the need to know \\(\\nabla^2 f(x)\\). These upper-bounds are, as we saw, what inform GD’s choice of step-size which ends up guaranteeing convergence."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#convergence-rate",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#convergence-rate",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Convergence Rate",
    "text": "Convergence Rate\nAs shown above, the theoretical best fixed step-size for an \\(M\\)-smooth objective \\(f(x)\\) is \\(\\eta = \\frac{1}{M}\\). With this choice of step-size, we can derive convergence guarantees for GD as well as its convergence rate.\n\n\\(M\\)-Smooth Objectives\nFixing the current iterate as \\(x\\), and plugging in \\(x^+ = x - \\frac{1}{M} \\nabla f(x)\\) into the upper-bound in \\((5.1)\\), we obtain the quadratic upper-bound on the next iterate in terms of the magnitude of the gradient:\n\\[f(x^+) \\leq f(x) - \\frac{1}{2M}||\\nabla f(x)||_2^2 \\tag{5.3}\\]\nFurthermore, since the underlying assumption throughout this post is that the objective is convex, we have a linear lower-bound \\(\\forall y\\), and particularly at the optimizer \\(y = x^*\\), as:\n\\[f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) \\tag{5.4}\\]\nBy reversing \\((5.4)\\) and adding it to \\((5.3)\\) we get:\n\\[f(x^+) \\leq f(x^*) + \\nabla f(x)^T(x - x^*) - \\frac{1}{2M}||\\nabla f(x)||_2^2\\]\nWith a bit of algebraic finessing, we can bring the above to the form:\n\\[f(x^+) \\leq f(x^*) + \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x - \\frac{1}{M}\\nabla f(x) - x^*||_2^2\\right]\\]\nBut \\(x - \\frac{1}{M}\\nabla f(x) = x^+\\), so we have:\n\\[f(x^+) - f(x^*) \\leq \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x^+ - x^*||_2^2\\right]\\]\nWe recognize, \\(||x^+ - x^*||_2^2\\) as the sub-optimality of the next iterate, and \\(||x - x^*||_2^2\\) as the sub-optimality of the previous iterate. When both sides are summed over \\(T\\) iterations, the RHS sum telescopes and we’re left with:\n\\[\\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2}||x_0 - x^*||_2^2\\]\nThe LHS is the sum of sub-optimalities across all \\(T\\) iterations, and the RHS is a quantity that’s proportional to the initial condition. Taking the average error across all iterations, we get:\n\\[\\frac{1}{T} \\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2\\]\nBut, we know that the algorithm with fixed step-size \\(\\eta = \\frac{1}{M}\\) has the descent property since \\(0 &lt; \\frac{1}{M} &lt; \\frac{2}{M}\\). So, the last sub-optimality \\(f(x_{T}) - f(x^*)\\) must be the smallest. In particular, it must be smaller than the average. So, we have:\n\\[f(x_{T}) - f(x^*) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2 \\tag{5.5}\\]\nSo, the error gets better with more iterations and, conversely, gets worse the larger \\(M\\), a measure of how abruptly the gradient changes anywhere on its domain, is.\nNote that \\(M\\), as well as \\(||x_0 - x^*||_2^2\\) are fixed in \\((5.5)\\). So, the convergence rate of GD on an \\(M\\)-smooth objective is \\(O(\\frac{1}{T})\\).\n\n\n\\(M\\)-Smooth and \\(m\\)-Strongly-Convex Objectives\nThe situation is drastically better if, in addition to \\(M\\)-smoothness, we also have \\(m\\)-strong-convexity. Not only do we get a much faster convergence rate, we also guarantee convergence in the iterates themselves. Note that, so far, we’ve discussed sub-optimality in objective values only. That is, the only convergence guarantee we’ve seen so far is \\(f(x^T) \\rightarrow f(x^*)\\) as \\(T \\rightarrow \\infty\\). Sometimes more is needed, we may actually want convergence of the iterates themselves. That is, we may want \\(x^T \\rightarrow x^*\\) as \\(T \\rightarrow \\infty\\)? Since strong convexity guarantees the existence of a unique optimizer \\(x^*\\), we can discuss this type of sub-optimality for \\(m\\)-strongly-convex objectives.\nIn this scenario, we have the analog of \\((5.3)\\) for the quadratic lower-bound.\nJust as \\(x - \\frac{1}{M} \\nabla f(x)\\) minimized the quadratic upper-bound, \\(x - \\frac{1}{m} \\nabla f(x)\\) minimizes the quadratic lower-bound. Plugging it into the lower-bound, we get \\((5.3)\\)’s analog as:\n\\[f(y) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\ \\ \\forall y \\tag{5.6}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis result is stronger than its analog \\((5.3)\\) because it holds \\(\\forall y\\) as opposed to \\((5.3)\\) which is only guaranteed to hold at the minimizer \\(x^+ = x - \\frac{1}{M}\\nabla f(x)\\). This is expected because \\((5.6)\\) is the result of minimizing a universal lower-bound as opposed to \\((5.3)\\) which is the result of minimizing a universal upper-bound.\n\n\n\nSince \\((5.6)\\) holds \\(\\forall y\\), it holds, in particular, at the optimizer \\(y=x^*\\):\n\\[f(x^*) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\tag{5.7}\\]\nWe can now solve for \\(||\\nabla f(x)||_2^2\\) in \\((5.7)\\) and plug the result into \\((5.3)\\). From \\((5.7)\\), we get:\n\\[||\\nabla f(x)||_2^2 \\geq 2m(f(x)-f(x^*))\\]\nWhich, when plugged into \\((5.3)\\), gives:\n\\[f(x^+) - f(x^*) \\leq \\left(1-\\frac{m}{M}\\right)(f(x)-f(x^*))\\]\nWe recognize the LHS as the sub-optimality at the next iteration, and the RHS as the sub-optimality at the current iteration. Recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[f(x_T) - f(x^*) \\leq  \\left(1-\\frac{m}{M}\\right)^T (f(x_0) - f(x^*)) \\tag{5.8}\\]\nAnd, since \\(m \\leq M\\) and both strictly positive, \\(0 &lt; \\frac{m}{M} \\leq 1\\) which guarantees convergence.\nNote that \\(m\\), \\(M\\), and the initial sub-optimality \\(f(x_0) - f(x^*)\\) are fixed quantities in \\((5.8)\\). So, the convergence rate of GD on a smooth and strongly-convex objective is \\(O(c^{-T})\\) for the constant \\(c^{-1} = 1 - \\frac{m}{M}\\). That is, the error decreases exponentially in the number of iterations. However, historically, mathematicians were concerned with the logarithm of the error, rather than the error itself, and hence this type of convergence is known as linear convergence.\nAs promised, we also have convergence of the iterates themselves. From the quadratic lower-bound in \\((5.1)\\) we can derive an upper-bound on \\(||x - x^*||_2\\) as follows. As before, letting \\(y = x^*\\) in the quadratic lower-bound gives us:\n\\[f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) + \\frac{m}{2}||x^* - x||_2^2\\]\nBut, by the Cauchy-Schwarz Inequality, we further have:\n\\[f(x^*) \\geq f(x) - ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2\\]\nBut, since \\(f(x^*) \\leq f(x)\\) by the optimality of \\(x^*\\), we must have:\n\\[- ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2 \\leq 0\\]\nFrom which it follows that:\n\\[||x - x^*||_2 \\leq \\frac{2}{m}||\\nabla f(x)||_2 \\tag{5.9}\\]\nAs GD converges to \\(f(x^*)\\), \\(\\nabla f(x) \\rightarrow \\nabla f(x^*) = 0\\) where the last equality is by optimality of \\(x^*\\). So, \\(x \\rightarrow x^*\\).\n\n\nAffine Invariance\nNot only does \\((5.8)\\) give the rate of convergence of GD, it also predicts its performance on objectives with roughly spherical vs roughly elliptical level-sets. These are the level-sets of what’s referred to as badly and well conditioned objectives respectively.\nTo say that an objective is \\(M\\)-smooth and \\(m\\)-strongly convex is to say \\((3.2)\\) which, as a reminder, is:\n\\[mI \\preceq \\nabla^2 f(x) \\preceq MI \\ \\ \\forall x\\]\nThis implies that all the eigenvalues of the hessian are bounded between \\(m\\) and \\(M\\). The eigenvalues of the hessian represent the stretch of the level sets in the principal directions. So, to say that \\(\\frac{m}{M} \\approx 1\\) is to say that \\(m \\approx M\\), which means that the level-sets are not more or less stretched in any particular direction. This implies that the level-sets are roughly spherical. As we can see from \\((5.8)\\), GD converges quite fast in such cases since \\(\\left( 1 - \\frac{m}{M} \\right)\\), the factor of decrease, is small. The opposite is true in the case when the level sets are elongated.\nThis brings us to an important property called affine invariance which GD lacks. Simply put, an affine transformation of the input space (i.e. a mere change of coordinates/basis) may affect GD’s performance drastically.\nIt helps to look at an example where the objective is a simple quadratic in \\(\\mathbb{R}^n\\).\n\nTake the quadratic objective \\(f(x) = \\frac{1}{2}x^TQx\\) where \\(x \\in \\mathbb{R}^n\\) is a coordinate vector in the standard basis. Now, consider the change of coordinates from the standard basis to a basis \\(Z\\) given by \\(Az = x\\) where \\(A\\) is a matrix whose column vectors are the basis vectors in \\(Z\\).\nThe key observation is that we can choose \\(A\\) in such a way as to make the objective in the new coordinate system have more or less elliptical level-sets which would affect GD’s performance.\nFirst, let’s come up with the same quadratic in \\(Z\\)-coordinates.\n\\[f(x) = f(Az) = \\frac{1}{2}(Az)^TQAz = z^TA^TQAz\\]\nLet \\(\\tilde Q = A^TQA\\) and define the quadratic in \\(Z\\)-coordinates as \\(\\tilde f(z) = \\frac{1}{2}z^T \\tilde Q z\\) so that \\(\\tilde f(z) = f(x)\\) for all \\(z\\) in the \\(Z\\)-coordinates corresponding to \\(x\\) in the standard basis. In particular, optimizing in either coordinate system yields the same result in terms of an optimum, that is \\(\\tilde f(z^*) = f(x^*)\\).\nThe GD factor of decrease on \\(\\tilde f\\) is \\(\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)\\), where \\(\\tilde m = \\lambda_{\\min}(\\tilde Q)\\) and \\(\\tilde M = \\lambda_{\\max}(\\tilde Q)\\). But \\(\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)\\) is under no obligation to be equal to \\(\\left(1 - \\frac{m}{M} \\right)\\), the GD factor of decrease in the standard basis, proving that GD is not affine invariant.\nAnother perspective on affine invariance comes from comparing the GD update steps in both spaces.\nThe GD update in the standard basis is:\n\\[x^+ = x - \\eta \\nabla f(x) = x - \\eta Qx\\]\nWhereas the GD update in \\(Z\\)-coordinates is:\n\\[z^+ = z - \\eta \\nabla \\tilde f(z) = z - \\eta \\tilde Qz\\]\nTo go from \\(Z\\)-coordinates back to the standard basis, we apply \\(A\\) to the LHS which necessitates its application to the RHS as well. We obtain:\n\\[Az^+ = Az - \\eta A\\tilde Qz = x - \\eta AA^TQx\\]\nWhich is not the same as \\(x - \\eta Qx\\). So, although \\(Az = x\\) the linear relationship breaks down for the next iterates produced by GD (that is \\(Az^+ \\ne x^+\\)). So, doing a step of GD in the standard basis it’s not the same as doing a step of GD in \\(Z\\)-coordinates (up to a change of basis by \\(A\\)). So, gradient descent is doing something radically different in the \\(Z\\)-coordinates compared to what it does in the standard basis.\n\n\nBest Affine Transformation\nA natural question to ask, at this point, is which choice of \\(A\\) makes GD perform faster on a quadratic objective?\nAlgebraically, the best we can hope for is \\(A\\) s.t. \\(\\tilde m \\approx \\tilde M\\). One way we can accomplish this is by forcing all of the eigenvalues of \\(\\tilde Q\\) to be the same. Particularly, letting them all be \\(1\\) by enforcing \\(\\tilde Q = A^TQA = I\\) works.\nSince \\(Q \\succeq 0\\), it has an eigendecomposition as \\(Q = PDP^T\\) where \\(D\\) is diagonal and \\(P\\) is orthonormal. Then its matrix square root exists and is given by \\(Q^{-1/2} = PD^{-1/2}P^T\\). Letting \\(A = Q^{-1/2}\\) we, indeed, have:\n\\[\n\\begin{aligned}\nA^TQA &= (PD^{-1/2}P^T)^TPDP^TPD^{-1/2}P^T \\\\\n&= PD^{-1/2}DD^{-1/2}P^T \\\\\n&= PP^T \\\\\n&= I \\\\\n\\end{aligned}\\]\nWhere we have used the fact that \\(P^TP = PP^T = I\\) since \\(P\\) is orthonormal, and the fact that diagonal matrices are raised to a power simply by raising their diagonal entries to that power.\nGeometrically, the choice of \\(\\tilde Q = I\\) forces the level-sets to be spherical. A level-set of \\(f(x) = \\frac{1}{2}x^TQx\\) in the standard coordinate system, i.e. an ellipse in \\(\\mathbb{R}^n\\), is given by \\(x^TQx = c\\) for some constant \\(c\\) which has absorbed \\(\\frac{1}{2}\\). In the \\(Z\\)-coordinates, the same level-set is given by \\(\\tilde f(z) = z^T\\tilde Qz = c\\). If \\(\\tilde Q = I\\), as is the case for the choice \\(A = Q^{-1/2}\\), then the level-set in the \\(Z\\)-coordinates becomes \\(z^Tz = c\\) which is, indeed, a sphere in \\(\\mathbb{R}^n\\).\nIn conclusion, if the objective is quadratic, we can improve GD’s convergence rate by applying the above change of basis. If the objective is not quadratic, we may still assume that it’s locally quadratic. This allows us to apply the same idea to non-quadratic objectives, but, since it requires coming up with a different matrix \\(A\\) at each iteration, the payoff from this procedure may not be worth it. So, we explore other ways of accelerating the performance of gradient descent on badly conditioned objectives by developing Accelerated Gradient Descent (AGD) which will be explained shortly."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#exact-line-search-els",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#exact-line-search-els",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Exact Line Search (ELS)",
    "text": "Exact Line Search (ELS)\nLet’s go back to the general iterative update step \\(x^+ = x + \\eta d\\).\nBy restricting the objective in the direction of the update \\(d\\) we can find the optimal step-size, \\(\\eta^*\\), at each iteration by solving the following one-dimensional, unconstrained optimization problem in \\(\\eta\\):\n\\[\\eta^* = \\arg \\min_{\\eta} f(x + \\eta d) \\tag{6.1}\\]\nWe proceed with the optimization by defining the restriction of \\(f\\) in the direction \\(d\\) as \\(\\phi(\\eta) := f(x + \\eta d)\\). Then, we can use the chain-rule to find the stationary point \\(\\eta^*\\) for which \\(\\nabla \\phi(\\eta^*) = 0\\).\nBy chain-rule:\n\\[\\nabla \\phi(\\eta) = \\nabla f(x + \\eta d)^T d \\tag{6.2}\\]\nIn the case of GD, \\(d = -\\nabla f(x)\\), so we have:\n\\[\\nabla \\phi(\\eta) = \\nabla f(x - \\eta \\nabla f(x))^T (-\\nabla f(x)) \\tag{6.3}\\]\nAn interesting geometric consequence of this is that GD with ELS takes perpendicular steps that end at a point of tangency with a level-set. Setting \\(\\nabla \\phi(\\eta) = 0\\) to find the optimal step-size obtains \\(\\eta^*\\) s.t. \\(\\nabla f(x - \\eta^* \\nabla f(x))\\) is perpendicular to \\(- \\nabla f(x)\\). In other words, GD with ELS goes in the negative gradient direction until the gradient at \\(x - \\eta^* \\nabla f(x)\\) is perpendicular to the gradient at the current iterate \\(x\\). At the next iteration, GD will take a step in the \\(- \\nabla f(x - \\eta^* \\nabla f(x))\\) direction which is still perpendicular to \\(- \\nabla f(x)\\). This means, at each iteration, GD takes a step that’s perpendicular to the step it took in the previous iteration. Furthermore, since gradients are always perpendicular to level-sets, the new iterate \\(x - \\eta^* \\nabla f(x)\\) is a point of tangency with the level-set at that point.\nAlthough this subroutine is very natural, it may be infeasible to solve an optimization problem (even a one-dimensional one) at each iteration. Hence, we introduce backtracking line search."
  },
  {
    "objectID": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#backtracking-line-search-btls",
    "href": "posts/optimization/algorithms_for_unconstrained_optimization_gradient_descent.html#backtracking-line-search-btls",
    "title": "Notes: Algorithms for Unconstrained Optimization",
    "section": "Backtracking Line Search (BTLS)",
    "text": "Backtracking Line Search (BTLS)\nAs we know a convex objective \\(f\\) is always lower-bounded by its linear approximation. That is:\n\\[f(y) \\geq f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y\\]\nPlugging \\(x^+ = x + \\eta d\\) into the above lower-bound obtains:\n\\[f(x^+) \\geq f(x) + \\eta \\nabla f(x)^T d\\]\nSo, the greatest possible reduction in value from \\(f(x)\\) to \\(f(x^+)\\) we can hope for is \\(\\eta \\nabla f(x)^T d\\) which, recall, is non-positive by a choice of \\(d\\) that guarantees descent (such as \\(d = -\\nabla f(x)\\) in gradient descent). Unless the objective is linear, in which case the above linear approximation holds with equality, we can not hope to achieve the full \\(\\eta \\nabla f(x)^T d\\) reduction in value. This is just a consequence of convexity and is, therefore, also the case when using exact line search.\nThe idea behind backtracking line search is to ensure we achieve, at least, a fraction of this maximum reduction in value by introducing a parameter \\(0 &lt; \\alpha &lt; 1\\).\nSince \\(f(x) + \\eta \\nabla f(x)^T d\\), the linear underestimate of \\(f(x^+)\\), is the tangent line to \\(f\\) at \\(x\\) in the direction \\(d\\), \\(f(x) + \\alpha \\eta \\nabla f(x)^T d\\) (for \\(0 &lt; \\alpha &lt; 1\\)) is a secant line of \\(f\\) at \\(x\\) in the direction \\(d\\). Setting \\(\\alpha = \\frac{1}{2}\\), a typical choice in practice, and finding the largest \\(\\eta\\) s.t. \\(f(x^+) = (x + \\eta d)\\) is still below this secant line ensures we get approximately half of the maximum reduction in value \\(\\eta \\nabla f(x)^T d\\).\nThis is exactly what the BTLS subroutine, detailed below, tries to achieve:\n\nThe BTLS Subroutine\n\nThe BTLS subroutine takes as input \\(0 &lt; \\alpha &lt; 1\\), and \\(0 &lt; \\beta &lt; 1\\).\nWhile \\(f(x + \\eta d) &gt; f(x) + \\alpha \\eta \\nabla f(x)^T d\\), it reduces \\(\\eta\\) by the update \\(\\eta \\leftarrow\\) \\(\\beta \\eta\\).\n\n\n\nConvergence Guarantees of GD with BTLS\nGradient descent with backtracking line search has a promising convergence guarantee for convex objectives that are also \\(M\\)-smooth.\n\nConvergence of GD with BTLS:   If \\(f\\) is \\(M\\)-smooth and convex then the step-size given by BTLS is s.t.\n\\(\\eta_{BTLS} \\geq \\frac{\\beta}{M}\\). Furthermore, \\(f(x^+) - f(x) \\leq \\frac{\\alpha \\beta}{M}||\\nabla f(x)||_2^2\\)\n\nSo, compared to the theoretical best step-size \\(\\eta = \\frac{1}{M}\\), which may not be accessible to us, we have \\(\\frac{\\beta}{M} \\leq \\eta_{BTLS} &lt; \\frac{1}{M}\\). So, \\(\\beta_{BTLS}\\) is less than the optimal step-size but, interestingly, it still keeps \\(M\\) in view despite the latter being unknown to us.\nFurthermore, for an \\(M\\)-smooth objective, the final sub-optimality after \\(T\\) iterations can be found as:\n\\[f(x_T) - f(x^*) \\leq \\frac{M}{2T \\alpha \\beta}||x_0 - x^*||_2^2 \\tag{7.1}\\]\nWhich is only a constant factor \\(\\alpha \\beta\\) worse than GD with the theoretical best fixed step-size. So, it’s still \\(O(1/T)\\).\nFor an \\(M\\)-smooth objective that’s also \\(m\\)-strongly convex, we have:\n\\[f(x_T) - f(x^*) \\leq \\left (1 - \\min \\left\\{ 2m\\alpha, \\frac{2\\alpha\\beta m}{M} \\right \\} \\right )^T||x_0 - x^*||_2^2 \\tag{7.2}\\]\nWhich is, again, comparable to GD with the theoretical best fixed step-size."
  },
  {
    "objectID": "posts/optimization/robust_lps_modelling_discrete_failures.html",
    "href": "posts/optimization/robust_lps_modelling_discrete_failures.html",
    "title": "Notes: Robust Linear Programs - Modelling Discrete Failures",
    "section": "",
    "text": "We are faced with the task of modeling a scenario in which at most \\(k\\) of the total \\(n\\) workers, machines, sensors, or other system components can fail. The task is to minimize the amount of system components needed, thereby minimizing cost, subject to certain known and robust constraints.\nWe will assume the linear cost function to be \\(c^Tx\\), and the known constraints to be \\(Ax \\leq b\\). The robust constraint is \\(a_R^Tx \\leq b_R, \\ \\ \\forall a_R \\in D_k\\) where \\(b_R\\) is a known vector, and \\(D_k\\) is the following interval uncertainty set with an additional combinatorial constraint:\n\\[\nD_k = \\{ a : a_i \\in [\\hat a_i - \\delta_i, \\hat a_i + \\delta_i] \\wedge \\textrm{at least $n-k$ of the $a_i$'s exactly equal $\\hat a_i$}\\}\n\\]\nIn \\(D_k\\), we can think of each \\(\\hat a_i\\) as the spec at which the \\(i\\)-th component should operate, and the \\(\\delta_i\\)’s as the \\(i\\)-th component’s deviation from this spec. Thus, \\(D_k\\) models the discrete failures scenario exactly…"
  },
  {
    "objectID": "posts/optimization/robust_lps_modelling_discrete_failures.html#formulating-the-inner-problem-as-a-linear-program",
    "href": "posts/optimization/robust_lps_modelling_discrete_failures.html#formulating-the-inner-problem-as-a-linear-program",
    "title": "Notes: Robust Linear Programs - Modelling Discrete Failures",
    "section": "Formulating the Inner Problem as a Linear Program",
    "text": "Formulating the Inner Problem as a Linear Program\nLet’s focus on the inner problem\n\\[\n\\begin{cases}\nmax_{a_R}: &a_R^Tx\n\\\\\ns.t.: &a_R \\in D_k\n\\end{cases} \\leq b_R\n\\]\nOur strategy now is to expand the constraint set \\(D_k\\).\nTo that end, let’s introduce slack variables \\(-1 \\leq z_i \\leq 1 \\ \\ \\forall i\\), which represent the direction of each component’s deviation from its spec. We can now rewrite the objective as:\n\\[\n\\begin{aligned}\na_R^Tx & = \\sum a_ix_i\n\\\\\n& = \\sum (\\hat a_i + z_i\\delta_i)x_i\n\\\\\n& = \\sum \\hat a_ix_i + \\sum \\delta_iz_ix_i\n\\end{aligned}\n\\]\nSo the optimization problem, which is now in the variables \\(z_i\\), becomes\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_ix_i\n\\\\\ns.t.: \\begin{aligned} &-1 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\textrm{at most $k$ of the $z_i \\ne 0$}\n\\end{aligned}\n\\end{cases}\n\\]\nWe still have the combinatorial constraint ‘at most \\(k\\) of the \\(z_i \\ne 0\\),’ which makes this into a mixed optimization problem…\nWe need to massage this problem more to bring it to standard form.\nNote that we’re dealing with a problem of maximization. In the objective, \\(\\sum \\hat a_ix_i\\) is fixed by virtue of the \\(\\hat a_i\\)’s being fixed by the given \\(D_k\\) and the \\(x_i\\)’s being fixed by the outer optimization problem. Note that the \\(\\delta_i\\)’s are also fixed by \\(D_k\\). Therefore, what would maximize the objective is each term of \\(\\sum \\delta_i z_i x_i\\) contributing positively to the sum.\nThis happens when \\(z_i\\) and \\(x_i\\) have the same sign \\(\\forall i\\). That is, their product \\(z_ix_i\\) takes values in \\([0, |x_i|]\\).\nThe remaining cases can be disposed of without changing the optimal value of the optimization problem.\nRewriting the problem, we have\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\textrm{at most $k$ of the $z_i \\ne 0$}\n\\end{aligned}\n\\end{cases} \\dagger\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that \\(|x_i|\\) does not make the objective non-linear because the \\(x_i\\)’s are fixed values, and not the decision variables.\n\n\n\n\nRelaxing the Combinatorial Constraint with a Continuous Constraint\nWe will relax the combinatorial constraint ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ by replacing it with \\(\\sum z_i \\leq k\\).\nAlthough this is a relaxation of the constraint, we will show that it makes no difference within the context of preserving the optimization problem. That is, it does not affect the optimal value of the problem.\nFirst and foremost, it’s easy to see that ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ \\(\\implies\\) \\(\\sum z_i \\leq k\\) since each \\(z_i \\in [0,1]\\).\nWe claim the converse is true as well, given that we restrict our attention to the optimal solution to the above LP \\(\\dagger\\). That is ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ \\(\\impliedby\\) \\(\\sum z_i \\leq k\\).\nThis claim is true by the geometry of linear programs. An optimal solution to the LP can only occur at an extreme point, and those are defined exactly by \\(n\\) independent active constraints.\nIn the above LP, \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\) represent a set of \\(2n\\) independent constraints, and \\(\\sum z_i \\leq k\\) is just one additional constraint.\nIf all of the \\(n\\) active constraints come from \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\), then since a given \\(z_i\\) cannot simultaneously be \\(0\\) and \\(1\\) the \\(z_i\\)’s of the optimal solution must take integral values (that is, either \\(0\\) or \\(1\\) and nothing in between).\nIn the general case, at least \\(n-1\\) constraints must come from \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\), which implies at least \\(n-1\\) of the \\(z_i\\)’s take integral values and the remaining active constraint is \\(\\sum z_i = k\\). But \\(n\\) numbers, of which \\(n-1\\) are integers, cannot add up to an integer value \\(k\\) unless the remaining number is also an integer. So, once again we have that all the \\(z_i\\)’s are integral valued.\nThen \\(\\sum z_i \\leq k\\) \\(\\implies\\) at most \\(k\\) of the \\(z_i = 1\\) \\(\\implies\\) ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ as was the claim.\nThis leaves us with the inner optimization problem\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\sum z_i \\leq k\n\\end{aligned}\n\\end{cases}\n\\]\nwhich is finally a linear program."
  },
  {
    "objectID": "posts/optimization/robust_lps_modelling_discrete_failures.html#putting-the-inner-and-outer-problems-together",
    "href": "posts/optimization/robust_lps_modelling_discrete_failures.html#putting-the-inner-and-outer-problems-together",
    "title": "Notes: Robust Linear Programs - Modelling Discrete Failures",
    "section": "Putting the Inner and Outer Problems Together",
    "text": "Putting the Inner and Outer Problems Together\nThe combined optimization problem becomes\n\\[\n\\begin{cases}\nmin: c^Tx\n\\\\\ns.t.: Ax \\leq b\n\\\\\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\sum z_i \\leq k\n\\end{aligned}\n\\end{cases} \\leq b_R\n\\end{cases}\n\\]\nThis is, of course, still not a linear program. Firstly, it’s a mixture between minimization and maximization. Secondly, since the variables are \\(x_i\\), and \\(z_i\\), the term \\(\\sum \\delta_iz_ix_i\\) is not linear in the decision variables. Thirdly, \\(|x_i|\\) is not linear in \\(x_i\\).\nWe can address these issues one by one…\n\nTaking the Dual of the Inner\nWe can turn the inner maximization problem to an inner minimization problem by taking its dual. As we know, by LP-duality (otherwise known as strong duality) this does not affect the optimal value of the problem.\nThe overall problem becomes\n\\[\n\\begin{cases}\nmin: c^Tx\n\\\\\ns.t.: Ax \\leq b\n\\\\\n\\begin{cases}\nmin_{\\lambda}: \\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k\n\\\\\ns.t.: \\begin{aligned} &\\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases} \\leq b_R\n\\end{cases}\n\\]\nFlattening the problems, since both are now minimization, we arrive at the following\n\\[\n\\begin{cases}\nmin_{x,\\lambda}: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax \\leq b\n\\\\\n&\\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k \\leq b_R\n\\\\\n&\\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\n\n\nLinearizing the Absolute Value Constraints\nThis is almost a linear program, except for the fact that \\(|x_i|\\)’s are nonlinear terms in the constraint. The last step is to split these constraints into corresponding pairs of linear constraints.\nFor each \\(i\\),\n\\[\n\\begin{aligned} \\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| &\\implies -\\lambda_0 - \\lambda_i \\leq \\delta_ix_i \\leq \\lambda_0 + \\lambda_i  \\\\ &\\implies \\begin{cases} \\lambda_0 + \\lambda_i &\\geq \\delta_ix_i \\\\ &\\textrm{and} \\\\ \\lambda_0 + \\lambda_i &\\geq -\\delta_ix_i \\end{cases}\\end{aligned}\n\\]\nSo, the final problem, which is a linear program in every right, is\n\\[\n\\begin{cases}\nmin_{x,\\lambda}: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax \\leq b\n\\\\\n&\\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k \\leq b_R\n\\\\\n&\\lambda_0 + \\lambda_i \\geq \\delta_ix_i \\ \\ \\forall i\n\\\\\n&\\lambda_0 + \\lambda_i \\geq -\\delta_ix_i \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html",
    "href": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html",
    "title": "Notes: Review of Linear Algebra for Optimization",
    "section": "",
    "text": "Let’s start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that’s commonly used in the subject."
  },
  {
    "objectID": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html#convexity",
    "href": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html#convexity",
    "title": "Notes: Review of Linear Algebra for Optimization",
    "section": "Convexity",
    "text": "Convexity\nSet convexity is defined as follows:\n\nDefinition:   A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\n\n\n\nFigure 1: Set A is convex, set B is non-convex\n\n\n\n\nOperations that Preserve Convexity\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html#examples-of-convex-sets",
    "href": "posts/linear_algebra/review_of_linear_algebra_for_optimization.html#examples-of-convex-sets",
    "title": "Notes: Review of Linear Algebra for Optimization",
    "section": "Examples of Convex Sets",
    "text": "Examples of Convex Sets\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\nConvex Hull of \\(n\\) Points\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\n\nConvex hull\n\n\n\n\nFigure 2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis’s algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\nConvex Hull of a Set\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there’s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it’s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\n\nConvex hull of a set\n\n\n\n\nFigure 3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points — it’s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\n\nAffine Combination of \\(n\\) Points\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)’s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it’s the line that passes through them, and for three points it’s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nLinear Combinations - Hyperplanes and Halfspaces\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)’s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\nHyperplanes\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere’s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x :  a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that’s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we’ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\nHalfspaces\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\nConic Combinations of \\(n\\) Points\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\nEllipses\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x  : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)’s positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\nNorm Balls\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it’s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\nPolyhedra\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\nThe Set of All Positive Semidefinite Matrices\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q :  x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "manim-sandbox/lib/python3.9/site-packages/manim/renderer/shaders/include/NOTE.html",
    "href": "manim-sandbox/lib/python3.9/site-packages/manim/renderer/shaders/include/NOTE.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "There seems to be no analog to #include in C++ for OpenGL shaders. While there are other options for sharing code between shaders, a lot of them aren’t great, especially if the goal is to have all the logic for which specific bits of code to share handled in the shader file itself. So the way manim currently works is to replace any line which looks like\n#INSERT \nwith the code from one of the files in this folder.\nThe functions in this file often include reference to uniforms which are assumed to be part of the surrounding context into which they are inserted."
  },
  {
    "objectID": "manim-sandbox/lib/python3.9/site-packages/screeninfo-0.8.1.dist-info/LICENSE.html",
    "href": "manim-sandbox/lib/python3.9/site-packages/screeninfo-0.8.1.dist-info/LICENSE.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "Screeninfo\nThe MIT License (MIT)\nCopyright (c) 2015 Marcin Kurczewski\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nDRM (Direct Rendering Manager) driver\nCopyright (c) 2011 The Chromium OS Authors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "v-poghosyan",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nLC121: Best Time to Buy and Sell Stock\n\n\nMay 23, 2024\n\n\n\n\nLC53: Maximum Subarray\n\n\nMay 23, 2024\n\n\n\n\nUnreal Engine: Cheat Sheet\n\n\nJan 13, 2023\n\n\n\n\nA Beginner’s Introduction to Concepts in Functional Programming\n\n\nJan 13, 2023\n\n\n\n\nLC: Connected Sinks and Sources\n\n\nJan 23, 2022\n\n\n\n\nLC11: Container with Most Water\n\n\nJan 23, 2022\n\n\n\n\nNotes: Linear Programs and LP Geometry\n\n\nJan 23, 2022\n\n\n\n\nNotes: Duality Theory\n\n\nJan 23, 2022\n\n\n\n\nNotes: Algorithms for Unconstrained Optimization\n\n\nJan 23, 2022\n\n\n\n\nNotes: Introduction to Optimization\n\n\nJan 23, 2022\n\n\n\n\nNotes: Robust Linear Programs - Modelling Discrete Failures\n\n\nJan 23, 2022\n\n\n\n\nNotes: Review of Linear Algebra for Optimization\n\n\nJan 23, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "unpublished_posts/software_engineering/getting_started_with_new_relic_agents.html",
    "href": "unpublished_posts/software_engineering/getting_started_with_new_relic_agents.html",
    "title": "Getting Started with New Relic and Splunk - A Complete Guide from Installation to Microservices Integration",
    "section": "",
    "text": "NewRelic and Splunk are both powerful tools for monitoring and observability, but they have some key differences:\n\nNewRelic is more focused on application performance monitoring (APM), for which it offers a wide range of features, including distributed tracing (tracing the event flow across microservices).\nSplunk is a more focused on log aggregation and analysis.\n\nHaving said that, there’s increasingly significant overlap between them as these products grow more and gain a diverse set of features. As of the time of writing this, each has its core competencies, and the two complement (rather than compete with) each other.\n\n\n\n\nA New Relic agent (which comes in many different flavors, depending on your choice of a backend language) collects metrics by either instrumenting your application’s code or attaching to its runtime environment (e.g. on the same JVM that your Java (or Scala) application is running on). Here’s how it works:\n\nInstrumentation: The agent integrates with your application at the code level or runtime environment level\n\nCode level instrumentation essentially means that the NewRelic agent is imported into the application’s source code itself, giving the developer far more visibility and control over the application’s logs\nCode level instrumentation is often called monkey patching and it’s typically used for dynamic languages like Python or JS (which are interpreted, not compiled)\nHowever, instrumenting our code is not always practical, so NewRelic offers a more cookie-cutter solution that works right out of the box – runtime environment instrumentation. This means that the agent is started with the application itself, and attaches to the running process\nThe runtime environment level of NewRelic integration is sometimes also called bytecode manipulation, and it’s usually the approach taken when dealing with languages like Java or Scala (statically typed, compiled languages). It’s a method that inserts monitoring hooks into your application which collect real-time data on its health and performance\n\nData Collection: The NewRelic agent’s hooks collect data on various aspects of your application’s performance. They either do this automatically, by detecting the frameworks used in the application and wrapping key methods and functions that handle an incoming request within a framework (e.g. an API endpoint’s logic implemented in an HTTP library of the given language) within the bytecode itself (in the runtime environment), or by the developer’s manual addition of instrumentation points (which are the places in the source code where the agent will insert its hooks). The data collected includes metrics, events, logs, and traces (also known as MELT), representing the four key datapoints used in Observability to comprehensively monitor and understand the real-time health and performance of your microservices\n\nMetrics such as response times, number and duration of database queries, number and duration of external service calls, the memory and CPU usage of the application (which, actually, come from container-level logs – i.e. Docker or Kubernetes – than the runtime of the application itself), and error rates.\n\nData Transmission: The collected data is asynchronously sent to New Relic’s cloud-based platform through secure communication channels. This will be explored in more detail in a separate paragraph (#TODO).\nAnalysis and Visualization: Once the data reaches New Relic, it is processed and made available in NewRelic. New Relic stores the data in its New Relic Telemetry Database (NRDB) across multiple clouds in the United States and the European Union. New Relic uses AWS and Azure for data storage. This data can then be used to generate dashboards, alerts, and reports, providing real-time insights into your application’s health and performance.\n\n\n\n\nThe installation process varies depending on the programming language and environment of your application. Below is a general step-by-step guide:\n\nSign Up and Obtain License Key:\n\nCreate an account on New Relic’s website.\nObtain your unique license key from the New Relic dashboard.\n\nChoose the Right Agent:\nNew Relic provides agents for various languages and platforms:\n\n\n\n\n\n\n\nLanguage/Platform\nAgent\n\n\n\n\nJava\nnewrelic.jar\n\n\n.NET\nNew Relic .NET Agent\n\n\nNode.js\nnewrelic npm package\n\n\nPython\nnewrelic pip package\n\n\nRuby\nnewrelic_rpm gem\n\n\nPHP\nNew Relic PHP Agent\n\n\nGo\nNew Relic Go Agent\n\n\n\nInstall the Agent:\n\nFor Java:\n\nDownload the newrelic.jar file.\nAdd the agent to your JVM startup parameters:\n-javaagent:/path/to/newrelic.jar\n\nFor Node.js:\n\nInstall via npm:\nnpm install newrelic\nRequire New Relic at the top of your main JavaScript file:\nrequire('newrelic');\n\nFor Python:\n\nInstall via pip:\npip install newrelic\nInitialize the agent in your application’s entry point.\n\n\nConfigure the Agent:\n\nUpdate the configuration file (e.g., newrelic.yml for Java or newrelic.js for Node.js) with your license key and application name.\nSet any additional settings like logging level or transaction tracing as needed.\n\nRestart Your Application:\n\nThe agent begins collecting data when the application restarts with the agent enabled.\n\nVerify Installation:\n\nLog in to your New Relic dashboard to confirm that data is being reported.\n\n\n\n\n\nIn a microservices architecture, each service operates independently, often across different environments or platforms. Here’s how the New Relic agent fits in:\n\nAgent in Each Microservice:\n\nInstall and configure the New Relic agent within each microservice you want to monitor.\nThis ensures that performance data is collected at the granular level of each service.\n\nIntegration Points:\n\nWithin the Application Code: For languages like Node.js or Python, the agent is imported directly into the application code.\nAt the Runtime Level: For compiled languages like Java or .NET, the agent attaches to the runtime environment via startup parameters.\n\nContainerized Environments:\n\nIf your microservices run in containers (e.g., Docker, Kubernetes), include the agent installation and configuration in your container images.\nAlternatively, use sidecar containers for the agents.\n\nDistributed Tracing:\n\nNew Relic supports distributed tracing, allowing you to trace requests as they flow through different microservices.\nEnsure that distributed tracing is enabled in the agent configuration to correlate data across services.\n\nData Aggregation:\n\nAll agents report back to New Relic’s centralized platform, where data is aggregated.\nYou can view the performance of individual microservices or the entire application stack."
  },
  {
    "objectID": "unpublished_posts/software_engineering/getting_started_with_new_relic_agents.html#newrelic-vs-splunk",
    "href": "unpublished_posts/software_engineering/getting_started_with_new_relic_agents.html#newrelic-vs-splunk",
    "title": "Getting Started with New Relic and Splunk - A Complete Guide from Installation to Microservices Integration",
    "section": "",
    "text": "NewRelic and Splunk are both powerful tools for monitoring and observability, but they have some key differences:\n\nNewRelic is more focused on application performance monitoring (APM), for which it offers a wide range of features, including distributed tracing (tracing the event flow across microservices).\nSplunk is a more focused on log aggregation and analysis.\n\nHaving said that, there’s increasingly significant overlap between them as these products grow more and gain a diverse set of features. As of the time of writing this, each has its core competencies, and the two complement (rather than compete with) each other.\n\n\n\n\nA New Relic agent (which comes in many different flavors, depending on your choice of a backend language) collects metrics by either instrumenting your application’s code or attaching to its runtime environment (e.g. on the same JVM that your Java (or Scala) application is running on). Here’s how it works:\n\nInstrumentation: The agent integrates with your application at the code level or runtime environment level\n\nCode level instrumentation essentially means that the NewRelic agent is imported into the application’s source code itself, giving the developer far more visibility and control over the application’s logs\nCode level instrumentation is often called monkey patching and it’s typically used for dynamic languages like Python or JS (which are interpreted, not compiled)\nHowever, instrumenting our code is not always practical, so NewRelic offers a more cookie-cutter solution that works right out of the box – runtime environment instrumentation. This means that the agent is started with the application itself, and attaches to the running process\nThe runtime environment level of NewRelic integration is sometimes also called bytecode manipulation, and it’s usually the approach taken when dealing with languages like Java or Scala (statically typed, compiled languages). It’s a method that inserts monitoring hooks into your application which collect real-time data on its health and performance\n\nData Collection: The NewRelic agent’s hooks collect data on various aspects of your application’s performance. They either do this automatically, by detecting the frameworks used in the application and wrapping key methods and functions that handle an incoming request within a framework (e.g. an API endpoint’s logic implemented in an HTTP library of the given language) within the bytecode itself (in the runtime environment), or by the developer’s manual addition of instrumentation points (which are the places in the source code where the agent will insert its hooks). The data collected includes metrics, events, logs, and traces (also known as MELT), representing the four key datapoints used in Observability to comprehensively monitor and understand the real-time health and performance of your microservices\n\nMetrics such as response times, number and duration of database queries, number and duration of external service calls, the memory and CPU usage of the application (which, actually, come from container-level logs – i.e. Docker or Kubernetes – than the runtime of the application itself), and error rates.\n\nData Transmission: The collected data is asynchronously sent to New Relic’s cloud-based platform through secure communication channels. This will be explored in more detail in a separate paragraph (#TODO).\nAnalysis and Visualization: Once the data reaches New Relic, it is processed and made available in NewRelic. New Relic stores the data in its New Relic Telemetry Database (NRDB) across multiple clouds in the United States and the European Union. New Relic uses AWS and Azure for data storage. This data can then be used to generate dashboards, alerts, and reports, providing real-time insights into your application’s health and performance.\n\n\n\n\nThe installation process varies depending on the programming language and environment of your application. Below is a general step-by-step guide:\n\nSign Up and Obtain License Key:\n\nCreate an account on New Relic’s website.\nObtain your unique license key from the New Relic dashboard.\n\nChoose the Right Agent:\nNew Relic provides agents for various languages and platforms:\n\n\n\n\n\n\n\nLanguage/Platform\nAgent\n\n\n\n\nJava\nnewrelic.jar\n\n\n.NET\nNew Relic .NET Agent\n\n\nNode.js\nnewrelic npm package\n\n\nPython\nnewrelic pip package\n\n\nRuby\nnewrelic_rpm gem\n\n\nPHP\nNew Relic PHP Agent\n\n\nGo\nNew Relic Go Agent\n\n\n\nInstall the Agent:\n\nFor Java:\n\nDownload the newrelic.jar file.\nAdd the agent to your JVM startup parameters:\n-javaagent:/path/to/newrelic.jar\n\nFor Node.js:\n\nInstall via npm:\nnpm install newrelic\nRequire New Relic at the top of your main JavaScript file:\nrequire('newrelic');\n\nFor Python:\n\nInstall via pip:\npip install newrelic\nInitialize the agent in your application’s entry point.\n\n\nConfigure the Agent:\n\nUpdate the configuration file (e.g., newrelic.yml for Java or newrelic.js for Node.js) with your license key and application name.\nSet any additional settings like logging level or transaction tracing as needed.\n\nRestart Your Application:\n\nThe agent begins collecting data when the application restarts with the agent enabled.\n\nVerify Installation:\n\nLog in to your New Relic dashboard to confirm that data is being reported.\n\n\n\n\n\nIn a microservices architecture, each service operates independently, often across different environments or platforms. Here’s how the New Relic agent fits in:\n\nAgent in Each Microservice:\n\nInstall and configure the New Relic agent within each microservice you want to monitor.\nThis ensures that performance data is collected at the granular level of each service.\n\nIntegration Points:\n\nWithin the Application Code: For languages like Node.js or Python, the agent is imported directly into the application code.\nAt the Runtime Level: For compiled languages like Java or .NET, the agent attaches to the runtime environment via startup parameters.\n\nContainerized Environments:\n\nIf your microservices run in containers (e.g., Docker, Kubernetes), include the agent installation and configuration in your container images.\nAlternatively, use sidecar containers for the agents.\n\nDistributed Tracing:\n\nNew Relic supports distributed tracing, allowing you to trace requests as they flow through different microservices.\nEnsure that distributed tracing is enabled in the agent configuration to correlate data across services.\n\nData Aggregation:\n\nAll agents report back to New Relic’s centralized platform, where data is aggregated.\nYou can view the performance of individual microservices or the entire application stack."
  },
  {
    "objectID": "unpublished_posts/software_engineering/build_and_ship_a_product.html",
    "href": "unpublished_posts/software_engineering/build_and_ship_a_product.html",
    "title": "How to Build and Ship a Product",
    "section": "",
    "text": "%%manim -qm ConnectedSinksAndSourcesDemo\n\nfrom manim import *\n\nclass ConnectedSinksAndSourcesDemo(Scene):\n    def construct(self):    \n        # Create table\n        table = Table(\n            simple_expanded_input\n        )\n        \n        # Animation sequence\n        self.play(Write(table))\n        self.wait(5)\n%%manim -qm ConnectedSinksAndSourcesDemo\n\nfrom manim import *\n\nclass ConnectedSinksAndSourcesDemo(Scene):\n    def construct(self):    \n        # Create table\n        table = Table(\n            simple_expanded_input\n        )\n        \n        # Animation sequence\n        self.play(Write(table))\n        self.wait(5)"
  },
  {
    "objectID": "unpublished_posts/software_engineering/build_and_ship_a_product.html#business-side",
    "href": "unpublished_posts/software_engineering/build_and_ship_a_product.html#business-side",
    "title": "How to Build and Ship a Product",
    "section": "Business Side",
    "text": "Business Side\nThe business side of our enterprise is concerned about identifying nice problems and marketing the products the technology side hands them.\n\n1. Identify a Niche Problem\nBegin by pinpointing a specific problem within a niche market. This approach increases the likelihood of your solution standing out and effectively addressing user needs.\n\n\n2. Implement Effective Marketing Strategies\nEmploy content marketing, social media, and SEO to reach your target audience. Tools like Buffer can assist in managing your social media presence.\n\n\n3. Scale and Diversify\nOnce your product gains traction, explore scaling opportunities and consider diversifying your offerings to mitigate risks."
  },
  {
    "objectID": "unpublished_posts/software_engineering/build_and_ship_a_product.html#technical-side",
    "href": "unpublished_posts/software_engineering/build_and_ship_a_product.html#technical-side",
    "title": "How to Build and Ship a Product",
    "section": "Technical side",
    "text": "Technical side\nJust like the business side can push any product, it should be the goal of the tech side to deliver an MVP for any client. Hence, the technology staff should develop with the north star of re0usability in mind. It’s much easier to containerize a core product and deliver it with various different requested configurations to different clients. These clients, by the way, can be internal clients as we grow as a company.\n\n1. Develop a Minimal Viable Product (MVP)\nCreate a basic version of a product that solves the core problem. This allows for early user feedback and iterative improvements.\n\n1.1. Pick a Tech Stack\nCheck out Svelte (a React competitor) for the front-end. Firebase can satisfy our hosting needs in the very beginning. Firebase is basically a toy version of Google Cloud with excellent documentation.\nReact vs Svelte - At a Glance*\nWhile React uses this verbose language to create state, including the need to create a React hook component…\n\n\n1.2 Devise a Client Authentication System\n\n\n1.3 Choose a database to use\nObject storage?\n\nimport React, { useState } from 'react';\n\nfunction Example() {\n    const [count, setCount] = useState(0);\n}\n\nSvelte just says:\n\nlet count = 0;\n\n\n%%manim -qm ConnectedSinksAndSourcesDemo\n\nfrom manim import *\n\nclass ConnectedSinksAndSourcesDemo(Scene):\n    def construct(self):    \n        # Create table\n        table = Table(\n            simple_expanded_input\n        )\n        \n        # Animation sequence\n        self.play(Write(table))\n        self.wait(5)\n\n\n%%manim -qm ConnectedSinksAndSourcesDemo\n\nfrom manim import *\n\nclass ConnectedSinksAndSourcesDemo(Scene):\n    def construct(self):    \n        # Create table\n        table = Table(\n            simple_expanded_input\n        )\n        \n        # Animation sequence\n        self.play(Write(table))\n        self.wait(5)\n\nFor frameworks, consider IBM’s Carbon Design Framework or, the more common, Bootstrap framework.\n\n\n\n2. Leverage Existing Platforms\nUtilize platforms like Gumroad, Shopify, or Square to sell your product, eliminating the need to develop a secure payment system that scales. Think about the time savings gained from not having to develop the payment system UI alone.\n\n\n3. Automate and Outsource\nAutomate repetitive tasks and consider outsourcing non-core activities to focus on product development and growth. Platforms such as Zapier can help automate workflows.\nThere are a lot of APIs available for outsourcing various components of our product. Here are a few:\n\n11Labs - An API for AI enabled speech-to-text"
  },
  {
    "objectID": "unpublished_posts/network_and_security/contract_testing.html",
    "href": "unpublished_posts/network_and_security/contract_testing.html",
    "title": "Contract Testing",
    "section": "",
    "text": "PactFlow\nThe idea behind bi-directional contract tracing, within the conmtext of API testing, is the client testing against a fresh mock of the server and the server, in reciprocation, testing against a fresh mock of the client. If the pact between the client and server is breached, neither the server team nor the client team can merge their code into the release branch."
  },
  {
    "objectID": "unpublished_posts/network_and_security/self-hosting.html",
    "href": "unpublished_posts/network_and_security/self-hosting.html",
    "title": "Self Hosting",
    "section": "",
    "text": "Useful Mac utilities:\n\nOh My Zsh\nHomebrew\nVim\n[Docker]\n\n\n\nAfter installing oh my zsh, we can set environment variables in ~/.zshrc using export. We can even set environment variables that affect the Z shell’s appearance. For example, adding the following in the initialization file ~/.zshrc displays the localhost, current working directory, and the prompt character (root vs user) in the shell prompt.\n# Define color codes\nRED=\"%F{red}\"\nGREEN=\"%F{green}\"\nYELLOW=\"%F{yellow}\"\nBLUE=\"%F{blue}\"\nMAGENTA=\"%F{magenta}\"\nCYAN=\"%F{cyan}\"\nWHITE=\"%F{white}\"\nRESET=\"%f\"\n\n# Set the prompt with colors\nPROMPT='${CYAN}%n@%m ${YELLOW}%1~ ${RESET}%# '\nA shell also knows of ‘hostname’ from a system command that’s part of the OS.\necho \"$(hostname)\"\nSome other variables the shell knows of include:\necho \"$USER\" # The name of the logged-in user.\necho \"$HOME\" # The path to the current user's home directory.\necho \"$PATH\" # The list of directories that the shell searches for executable files.\nThese are set by the login process, rather than being exposed by OS commands like hostname or being defined by the user in an initialization files like ~/.zshrc."
  },
  {
    "objectID": "unpublished_posts/network_and_security/self-hosting.html#whats-my-localhost",
    "href": "unpublished_posts/network_and_security/self-hosting.html#whats-my-localhost",
    "title": "Self Hosting",
    "section": "",
    "text": "After installing oh my zsh, we can set environment variables in ~/.zshrc using export. We can even set environment variables that affect the Z shell’s appearance. For example, adding the following in the initialization file ~/.zshrc displays the localhost, current working directory, and the prompt character (root vs user) in the shell prompt.\n# Define color codes\nRED=\"%F{red}\"\nGREEN=\"%F{green}\"\nYELLOW=\"%F{yellow}\"\nBLUE=\"%F{blue}\"\nMAGENTA=\"%F{magenta}\"\nCYAN=\"%F{cyan}\"\nWHITE=\"%F{white}\"\nRESET=\"%f\"\n\n# Set the prompt with colors\nPROMPT='${CYAN}%n@%m ${YELLOW}%1~ ${RESET}%# '\nA shell also knows of ‘hostname’ from a system command that’s part of the OS.\necho \"$(hostname)\"\nSome other variables the shell knows of include:\necho \"$USER\" # The name of the logged-in user.\necho \"$HOME\" # The path to the current user's home directory.\necho \"$PATH\" # The list of directories that the shell searches for executable files.\nThese are set by the login process, rather than being exposed by OS commands like hostname or being defined by the user in an initialization files like ~/.zshrc."
  },
  {
    "objectID": "unpublished_posts/network_and_security/self-hosting.html#creating-a-docker-container",
    "href": "unpublished_posts/network_and_security/self-hosting.html#creating-a-docker-container",
    "title": "Self Hosting",
    "section": "Creating a Docker Container",
    "text": "Creating a Docker Container"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html",
    "href": "unpublished_posts/python/introduction-to-numpy.html",
    "title": "Introduction to Numpy",
    "section": "",
    "text": "NumPy is a scientific computing library for Python. It’s an extensive collection of pre-written code that optimizes and extends, among other things, the Python array (i.e. list) object into an n-dimensional NumPy array called ndarray. It comes with a variety of tools, such as matrix operations and common mathematical functions, that enable Python to perform complex mathematical tasks such as solve linear algebraic problems, generate pseudo-random numbers, perform Fourier analysis, etc.\nWe import NumPy, as we import any other library, using the import keyword (with or without a shorthand).\nimport numpy\nOr, alternatively:\nimport numpy as np"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#scalable-memory-representation",
    "href": "unpublished_posts/python/introduction-to-numpy.html#scalable-memory-representation",
    "title": "Introduction to Numpy",
    "section": "Scalable Memory Representation",
    "text": "Scalable Memory Representation\nOne of the things NumPy optimizes is data storage. In contrast to Python 3.x’s scalable memory representation of numeric values, such as integers, which can grow to accommodate a given number, NumPy stores numeric types in fixed-sized blocks of memory (e.g. int32 or int64). This means NumPy is able to take advantage of the low-level CPU instructions of modern processors that are designed for fixed-sized numeric types. Another advantage of fixed-sized storage is that consecutive blocks of memory can be allocated, which enables the libraries upon which NumPy relies to do extremely performant computations. This enforcement of fixed-sized data types is part of the optimization strategy NumPy uses called vectorization."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#vectorization",
    "href": "unpublished_posts/python/introduction-to-numpy.html#vectorization",
    "title": "Introduction to Numpy",
    "section": "Vectorization",
    "text": "Vectorization\nAs already discussed in the aforementioned post, vectorization is the process by which NumPy stores an array internally in a contiguous block of memory, and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does as its iterating through a loop in order to speed up our code. Optimizing the array data structure in such a way enables NumPy to delegate most of the operations on such arrays to pre-written C code under the hood. In effect, this simply means that looping occurs in C instead of Python."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#broadcasting",
    "href": "unpublished_posts/python/introduction-to-numpy.html#broadcasting",
    "title": "Introduction to Numpy",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes the process by which NumPy performs arithmetic operations on arrays of different dimensions. The process is usually as follows: the smaller array is “broadcast” across the larger array so that the two arrays have compatible dimensions. Broadcasting provides a means of vectorizing array operations."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#comparing-runtime",
    "href": "unpublished_posts/python/introduction-to-numpy.html#comparing-runtime",
    "title": "Introduction to Numpy",
    "section": "Comparing Runtime",
    "text": "Comparing Runtime\nTo demonstrate the performance optimizations of NumPy, let’s compare squaring every element of a 1,000,000-element array and summing the results.\n\nUsing a Python List\nFirst, we will use a Python list:\n\n\nCode\nunoptimized_list = list(range(1000000))\n\n\nSquaring each element and summing:\n\n\nCode\nimport numpy as np\n%timeit np.sum([i**2 for i in unoptimized_list])\n\n\n342 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nNote: Even though we’re using NumPy’s sum() method, since the input we’re passing to it is a regular Python list, NumPy optimizations are not applied.\n\n\nAs we can see the whole thing took about 314 ms.\n\n\nUsing a NumPy Array\nNow let’s do the same with a NumPy array, which also gives us the opportunity to introduce the syntax for defining one using a range.\n\n\nCode\noptimized_array = np.arange(1000000)\n\n\nLet’s check the type of optimized_array to convince ourselves that it is, indeed, a NumPy ndarray.\n\n\nCode\ntype(optimized_array)\n\n\nnumpy.ndarray\n\n\nNow, finally, let’s square each element and sum the results:\n\n\nCode\n%timeit np.sum(optimized_array**2)\n\n\n1.61 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\nRemarkably, the run-time was cut from 314 ms to only around 1.61ms!"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#creating-arrays",
    "href": "unpublished_posts/python/introduction-to-numpy.html#creating-arrays",
    "title": "Introduction to Numpy",
    "section": "Creating Arrays",
    "text": "Creating Arrays\nWe’ve already seen how we can create a 1-dimensional NumPy array of consecutive integers \\(0,1,...,n-1\\) using the arrange() method.\nThe standard way of creating a NumPy array is passing a Python list to the constructor array() like so:\n\n\nCode\na = np.array([1,2,3])\na\n\n\narray([1, 2, 3])\n\n\nWe can also create some common arrays, such as an array of consecutive integers, with some special methods such as arange(), which takes an integer \\(n\\) as input and creates a sequential array from \\(0,...,n-1\\).\n\n\nCode\nnp.arange(5) # array([0, 1, 2, 3, 4])\n\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#representing-matrices",
    "href": "unpublished_posts/python/introduction-to-numpy.html#representing-matrices",
    "title": "Introduction to Numpy",
    "section": "Representing Matrices",
    "text": "Representing Matrices\nLet’s represent a \\(2 \\times 3\\) matrix $ A =\n\\[\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\\]\n$ using NumPy:\n\n\nCode\nA = np.array([[1,2,3],\n              [4,5,6]])\nA\n\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThere are also ways to quickly create some common matrices using special methods.\nFor example, ones() accepts a shape pair, and creates a matrix of \\(1\\)s with of the given shape.\n\n\nCode\nnp.ones((2,3))\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThe method, zeros() works the same way as ones():\n\n\nCode\nnp.zeros((2,3))\n\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\nMeanwhile, identity() accepts an integer \\(n\\) as input and creates a square \\(n \\times n\\) identity matrix.\n\n\nCode\nnp.identity(3)\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nCreating a matrix with identical elements in general uses the full() method which takes a shape attribute, a value attribute and on optional dtype attribute as follows:\n\n\nCode\nnp.full((2,3), 7, dtype = int)\n\n\narray([[7, 7, 7],\n       [7, 7, 7]])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#indexing",
    "href": "unpublished_posts/python/introduction-to-numpy.html#indexing",
    "title": "Introduction to Numpy",
    "section": "Indexing",
    "text": "Indexing\nIndexing a 1-dimensional NumPy array is done as expected, through the use of the trusty brackets []. Indexing an n-dimensional matrix in NumPy still uses [] but it introduces a new, improved, syntax.\nSuppose we’d like to access the element in the first row, and last column of A. The standard way would be:\n\n\nCode\nA[0][2]\n\n\n3\n\n\nAs we can see, that still works. But the recommended and, subjectively speaking, prettier way is:\n\n\nCode\nA[0,2]\n\n\n3\n\n\nOf course, slicing still works as expected.\nFor example, let’s print the entire first row of A:\n\n\nCode\nA[0,:]\n\n\narray([1, 2, 3])\n\n\nThe entire first column:\n\n\nCode\nA[:,0]\n\n\narray([1, 4])\n\n\nFinally, let’s print the submatrix $\n\\[\\begin{bmatrix}\n2 & 3\n\\end{bmatrix}\\]\n$:\n\n\nCode\nA[0,1:]\n\n\narray([2, 3])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#properties-and-methods-of-numpy-arrays",
    "href": "unpublished_posts/python/introduction-to-numpy.html#properties-and-methods-of-numpy-arrays",
    "title": "Introduction to Numpy",
    "section": "Properties and Methods of NumPy Arrays",
    "text": "Properties and Methods of NumPy Arrays\nA few of the useful properties and methods of ndarray are highlighted in this section.\n\nshape - returns the shape of the matrix as an \\((m,n)\\) pair\nA.shape # (2,3)\nndim - returns the dimension of a matrix as a single digit\nA.ndim # 2\n\nNote: The output of the ndim property should not be understood in a linear algebraic sense as the dimension of either the domain or range of the corresponding transformation, nor the dimension of either of its four fundamental subspaces. It is to only be understood in the data structure sense as the level of nestedness of the array.\n\nsize - returns the total number of elements in the matrix\nA.size # 6\ndtype - returns the data type of the elements in the matrix.\nA.dtype # dtype('int32')\n\nNote: If the ndarray does not represent a matrix, such as B = np.array([[1,2,3],[4,5]]) then dtype outputs O signifying that the entries are general Python objects. In such a case, the array loses its optimizations.\n\n\n\nStatistical and Mathematical Methods\nThere is also a vast selection of statistical, and more generally, mathematical methods that ndarrays come with. Here are a few of the common ones:\n\nsum() - returns the sum of all the entries\nA.sum() # 21\nIt also accepts an axis attribute where axis = 0 refers to the sum along the columns, and axis = 1 refers to the sum along the rows.\nA.sum(axis = 0) # [5,7,9]\nA.sum(axis = 1) # [6,15]\nmean() - returns the empirical mean of all the entries\nA.mean() # 3.5\nvar() - returns the variance of the entries\nA.var() # 2.9166666666666665\nstd() - returns the standard deviation of the entries\nA.std() # 1.707825127659933"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#multi-indexing-filtering-and-broadcasted-operations",
    "href": "unpublished_posts/python/introduction-to-numpy.html#multi-indexing-filtering-and-broadcasted-operations",
    "title": "Introduction to Numpy",
    "section": "Multi-Indexing, Filtering, and Broadcasted Operations",
    "text": "Multi-Indexing, Filtering, and Broadcasted Operations\nRecall from the Pandas article the ways in which we were able to multi-index and filter, and how we eliminated the need for using Python loops and list comprehensions using broadcasted operators instead. Since both a Pandas Series and a DataFrame are extensions of NumPy’s ndarray, all of these apply here as well.\nAs a refresher on broadcasted operations, here are a few filtering examples.\nLet’s obtain those elements of A that are greater than 3:\n\n\nCode\nA[A &gt; 3]\n\n\narray([4, 5, 6])\n\n\nNow let’s obtain those elements of A that are greater than the empirical mean:\n\n\nCode\nA[A &gt; A.mean()]\n\n\narray([4, 5, 6])\n\n\nWhat about those elements of A that are less than or equal to the empirical mean?\n\n\nCode\nA[~(A &gt; A.mean())]\n\n\narray([1, 2, 3])\n\n\nWhich is equivalent to:\n\n\nCode\nA[A &lt;= A.mean()]\n\n\narray([1, 2, 3])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#matrix-operations",
    "href": "unpublished_posts/python/introduction-to-numpy.html#matrix-operations",
    "title": "Introduction to Numpy",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nOne of NumPy’s key features is the way in which it simplifies matrix operations in Python. It offers simple syntax to add, multiply, transpose, invert, flatten, etc.\n\nAddition\nAddition of matrices is, by default, per-element (as are all NumPy operations). There’s no special syntax, it’s done through the + operator.\nFor example:\n\n\nCode\nA = np.ones((2,3))\nB = np.ones((2,3))\nA + B\n\n\narray([[2., 2., 2.],\n       [2., 2., 2.]])\n\n\n\n\nMultiplication\nThe operator * performs per-element multiplication.\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((2,3), 3, dtype = int)\nA * B\n\n\narray([[6, 6, 6],\n       [6, 6, 6]])\n\n\nBut this, as we know, isn’t matrix multiplication as it’s commonly defined in mathematics — that being the inner product of corresponding rows and columns. For instance, if we try to multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix, NumPy will throw the following error:\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA * B\n\n\nValueError: operands could not be broadcast together with shapes (2,3) (3,4) \nThis is because per-element operations require the shapes of the operands to be the same or compatible by broadcasting. Here NumPy attempts to broadcast one operand to match the shape of the other one, but broadcasting is impossible between matrices of shapes \\(2 \\times 3\\) and \\(3 \\times 4\\) per broadcasting rules.\nThere’s a workaround that lets us use *. Since NumPy overloads the * operator, it works as it should for numpy.matrix types.\n\n\nCode\nA = np.matrix([[2,2,2],\n               [2,2,2]])\nB = np.matrix([[3,3,3,3],\n               [3,3,3,3],\n               [3,3,3,3]])\nA * B\n\n\nmatrix([[18, 18, 18, 18],\n        [18, 18, 18, 18]])\n\n\nHowever, this is not the recommended way to carry out matrix multiplication in NumPy. Overloaded operators can produce convoluted code. For instance, we may have many different matrix and ndarray data structures and never be able to tell the outcome of a particular * operation. We should avoid such ambiguity whenever possible.\nInstead, the recommended way to do matrix multiplication is through the @ operator.\nWhen we use @ NumPy internally uses its matmul() method. So, the following are equivalent and both produce the matrix product of A and B.\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA @ B\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nnp.matmul(A,B)\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\nNumPy also offers, dot() which, for one and two dimensional matrices, is equivalent to matmul(). So, the following is yet another way we can multiply two matrices:\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA.dot(B)\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\nHowever, matmul() is preferred over dot() because of the clarity of its name, and because the dot product has a distinct mathematical meaning separate from matrix multiplication."
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "Perhaps the most important package for scientific computing included with Conda is NumPy. Let’s get a feel for what NumPy offers.\n\n\n\n\nA 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices.\n\n\n\n\nNumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e. its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don’t have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we’re working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let’s examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one’s shape is (3,1).\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can’t include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "A 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices."
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "NumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e. its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don’t have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we’re working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let’s examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one’s shape is (3,1).\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can’t include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/web_scraping/web_scraping.html",
    "href": "unpublished_posts/web_scraping/web_scraping.html",
    "title": "Price Scraper: Web Scraping and a Simple Notification System",
    "section": "",
    "text": "/etc/sbin/crontab contains a blueprint of a CRON job. These are run by a cron binary file in Linux systems periodically according to a set schedule. They are provided the schedule in a specific format (found in the crontab) and a tool + a script to run. For example python3 /my_script.py.\nFor periodic web-scraping, we can spin up an ECS cluster (or use self-hosting, more on that later). We can spin up a very small instance of a Linux container, optimizing for high availability."
  },
  {
    "objectID": "unpublished_posts/web_scraping/web_scraping.html#cron-jobs",
    "href": "unpublished_posts/web_scraping/web_scraping.html#cron-jobs",
    "title": "Price Scraper: Web Scraping and a Simple Notification System",
    "section": "",
    "text": "/etc/sbin/crontab contains a blueprint of a CRON job. These are run by a cron binary file in Linux systems periodically according to a set schedule. They are provided the schedule in a specific format (found in the crontab) and a tool + a script to run. For example python3 /my_script.py.\nFor periodic web-scraping, we can spin up an ECS cluster (or use self-hosting, more on that later). We can spin up a very small instance of a Linux container, optimizing for high availability."
  },
  {
    "objectID": "unpublished_posts/general_computer_science/data_structures_primer.html",
    "href": "unpublished_posts/general_computer_science/data_structures_primer.html",
    "title": "Data Structures Primer",
    "section": "",
    "text": "Graphs\nAll trees are graphs, but not all graphs are trees. Graphs that are acyclic are trees.\nBut how do you represent a tree data structure anyway?\n\n\nCode\nclass Graph:\n    def __init__(self):\n        self.graph = {}  # Initialize the adjacency list\n\n    def add_vertex(self, v):\n        if v not in self.graph:\n            self.graph[v] = []  # Add a new vertex with an empty adjacency list\n\n    def add_edge(self, v1, v2):\n        # Assuming an undirected graph...\n        if v1 in self.graph:\n            self.graph[v1].append(v2)\n        else:\n            self.graph[v1] = [v2]\n        \n        if v2 in self.graph:\n            self.graph[v2].append(v1)\n        else:\n            self.graph[v2] = [v1]\n\n    def __repr__(self): # String representation of object for logging\n        print(f\"-----{type(self).__name__}-----\\n\")\n        string_repr = \"\" # Initialize the string representation\n        for v, e in self.graph.items():\n            string_repr += f\"{v}: {e}\\n\"\n        return string_repr\n\n# Example usage\ng = Graph()\ng.add_vertex('╠11')\ng.add_vertex('C10')\ng.add_edge('╠11', 'C10')\ng.add_edge('╠11', '═21')  # Our add_edge assumes ═21 is automatically added as a vertex\nprint(g)\n\n\n-----Graph-----\n\n╠11: ['C10', '═21']\nC10: ['╠11']\n═21: ['╠11']"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html",
    "href": "unpublished_posts/local_development_setup/brew.html",
    "title": "Homebrew",
    "section": "",
    "text": "The collection of software files to be installed as described by a Formula\n\n\n\nEach Package has a Formula stored in an online repo called the Tap. For example, the command brew install git tells Homebrew to install the git Package. Homebrew knows where the Tap for the git Package is, however sometimes we may need to add a Tap manually before (e.g. brew tap homebrew/my-org git@github.cloud.my-org.com:Homebrew/homebrew-my-org.git).\nThe Formula is a script which defines the Package’s dependencies and contains instructions on how to install them. Some Formulas contain Bottles: precompiled software binaries\n\n\n\nAll files belonging to one Formula are kept in a single directory called a Keg after installation. This is further stored in a file tree called a Cellar (Cellar/Rack/Keg).By default, Homebrew will install all packages in the directory /usr/local/Cellar/, and also creates symbolic links at /usr/local/opt/ and /usr/local/bin/ (for executable files).\nFor example, if we have installed the git Package through Homebrew, when we run brew list git we will get all the files in its Formula organized in directories similar to /usr/local/Cellar/git/2.39.2/.bottle/etc/gitconfig. As we can see, in the path Cellar/Rack/Keg, Rack corresponds roughly to the Package and Keg to the version number.\n\n\n\nCasks are like Formulae but they install apps to the Mac’s application folder instead of Homebrew’s Cellar.\n\n\n\n\nformula - lives in the tap, defines the package’s dependencies and contains their installation instructions\nbottles - some formulas contain bottles which are just precompiled binaries\ntap - the repository where the packages live (brew itself has a default tap but more can be added like a company’s internal homebrew tap)\ncask - just a formula that installs the app (instead of a package) in the application folder"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html#package",
    "href": "unpublished_posts/local_development_setup/brew.html#package",
    "title": "Homebrew",
    "section": "",
    "text": "The collection of software files to be installed as described by a Formula"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html#taps-and-formulas",
    "href": "unpublished_posts/local_development_setup/brew.html#taps-and-formulas",
    "title": "Homebrew",
    "section": "",
    "text": "Each Package has a Formula stored in an online repo called the Tap. For example, the command brew install git tells Homebrew to install the git Package. Homebrew knows where the Tap for the git Package is, however sometimes we may need to add a Tap manually before (e.g. brew tap homebrew/my-org git@github.cloud.my-org.com:Homebrew/homebrew-my-org.git).\nThe Formula is a script which defines the Package’s dependencies and contains instructions on how to install them. Some Formulas contain Bottles: precompiled software binaries"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html#keg",
    "href": "unpublished_posts/local_development_setup/brew.html#keg",
    "title": "Homebrew",
    "section": "",
    "text": "All files belonging to one Formula are kept in a single directory called a Keg after installation. This is further stored in a file tree called a Cellar (Cellar/Rack/Keg).By default, Homebrew will install all packages in the directory /usr/local/Cellar/, and also creates symbolic links at /usr/local/opt/ and /usr/local/bin/ (for executable files).\nFor example, if we have installed the git Package through Homebrew, when we run brew list git we will get all the files in its Formula organized in directories similar to /usr/local/Cellar/git/2.39.2/.bottle/etc/gitconfig. As we can see, in the path Cellar/Rack/Keg, Rack corresponds roughly to the Package and Keg to the version number."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html#casks",
    "href": "unpublished_posts/local_development_setup/brew.html#casks",
    "title": "Homebrew",
    "section": "",
    "text": "Casks are like Formulae but they install apps to the Mac’s application folder instead of Homebrew’s Cellar."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/brew.html#summary",
    "href": "unpublished_posts/local_development_setup/brew.html#summary",
    "title": "Homebrew",
    "section": "",
    "text": "formula - lives in the tap, defines the package’s dependencies and contains their installation instructions\nbottles - some formulas contain bottles which are just precompiled binaries\ntap - the repository where the packages live (brew itself has a default tap but more can be added like a company’s internal homebrew tap)\ncask - just a formula that installs the app (instead of a package) in the application folder"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html",
    "href": "unpublished_posts/local_development_setup/shell_configs.html",
    "title": "Shell Configurations",
    "section": "",
    "text": "In Unix-like operating systems, shells like Bash and Zsh use configuration files to set up the user’s environment. These files control everything from environment variables, command aliases, shell options, custom functions, and more… Understanding the differences between these files helps in customizing and managing your shell environment effectively.\nBut first, we need to distinguish between interactive login shells and interactive non-login shells.\n\n\nA login shell is a shell session that starts when you log into your system, either directly or remotely. It’s called a “login shell” because it is associated with an initial user login.\nExamples of login shells:\n\nLogging in via console (i.e. keyboard & mouse) or SSH into a physical or a virtual machine (or any kind of remote server)\nExecuting bash --login or zsh --login to start a shell as a login shell.\n\nPurpose and common settings in login shells: - Environment variables: Often, login shells set environment variables like PATH, EDITOR, and others that should be configured only once per session - Startup Commands: Commands that need to run only once per session (commonly used for setting up session-specific services)\nNon-login shells, then, are shells that aren’t login shells. Now that we have a better understanding of the different kinds of shells, let’s proceed with the comparison of the shell configuration files.\n\n\n\nBelow is a detailed comparison of these shell configuration files.\n\n\n\n\n\n\n\nConfiguration file for interactive non-login Bash shells\n\n\n\n\nAliases (e.g., alias ll='ls -alF')\nShell options (e.g., shopt -s histappend)\nFunctions and prompt customization.\n\n\n\n\n\n\n\nConfiguration file for interactive login Bash shells\n\n\n\n\nEnvironment variables (e.g., export PATH=...)\nCommands that should run only once at login.\nOften includes sourcing .bashrc to ensure non-login shell configurations are available.\n\nTo source .bashrc, use:\nif [ -f ~/.bashrc ]; then\n  . ~/.bashrc\nfi\n\n\n\n\n\n\n\nThese files are the analogs of .bashrc and .bash_profile and the distinction between then is also analogous, One’s for non-login shells and the other’s for login shells. One key difference is that unlike .bashrc, .zshrc is is also sourced in login shells by default (eliminating the need to source it from .zprofile).\n\n\n\nThese files store the command history for Bash or Zsh shells, respectively.\n\n\n\nUsed by the history command (history)\nConfigurable via shell variables like HISTSIZE, HISTFILESIZE, and HISTCONTROL. For a simple configuration that retains only the history we want to keep, see this excellent post by Benjamin Wüthrich.\n\n\n\n\nFunctionality - Used by the history command (history). - Configurable via options like HISTSIZE, SAVEHIST, and HISTFILE. For ideas how to configure the history\n\n\n\n\n\n\n\nLogin Shell: - Reads /etc/profile (system-wide configurations). - Reads the first found file among ~/.bash_profile, ~/.bash_login, or ~/.profile. Non-login Interactive Shell: - Reads ~/.bashrc.\n\n\n\nLogin Shell: - Reads /etc/zprofile (system-wide configurations). - Reads ~/.zprofile. - Then proceeds to read ~/.zshrc (unlike Bash, Zsh reads .zshrc for login shells by default). Interactive Shell: - Reads ~/.zshrc.\n\n\n\n\n\n\n\nFor Bash, source .bashrc from .bash_profile to ensure configurations are consistent across login and non-login shells.\nFor Zsh, since .zshrc is read by both login and interactive shells, you can place most configurations there.\n\n\n\n\nSet in .bash_profile or .zprofile for variables that should be set once per session.\n\n\n\nPlace in .bashrc or .zshrc as they are needed for interactive use.\n\n\n\n\n\n\n\n# .bash_profile\n\n# Set environment variables\nexport PATH=\"$HOME/bin:$PATH\"\nexport EDITOR=\"vim\"\n\n# Source .bashrc if it exists\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n\n\n# .bashrc\n\n# Aliases\nalias ll='ls -alF'\nalias gs='git status'\n\n# Prompt customization\nPS1='[\\u@\\h \\W]\\$ '\n\n# Shell options\nshopt -s histappend\n\n\n\n# .zshrc\n\n# Load Oh My Zsh\nexport ZSH=\"$HOME/.oh-my-zsh\"\nZSH_THEME=\"agnoster\"\nplugins=(git docker kubectl)\nsource $ZSH/oh-my-zsh.sh\n\n# Aliases\nalias k='kubectl'\nalias d='docker'\n\n# Prompt customization\nPROMPT='%n@%m %1~ %# '\n\n# History settings\nHISTSIZE=10000\nSAVEHIST=10000\nHISTFILE=~/.zsh_history\n\n\n\n\n\n\nBoth Bash and Zsh allow you to configure how commands are saved to history files. You can prevent certain commands from being saved or set up shared history between sessions.\n\n\n\nUse the source or . command to include configurations from other files. This helps in modularizing your shell configurations.\n\n\n\nBe cautious when using shell-specific features if you switch between shells (like between Bash and Zsh). You can use .profile for environment variables to maintain compatibility across different shells if needed.\n\n\n\n\n\nBash:\n\n.bash_profile: For login shells; set environment variables and startup programs.\n.bashrc: For interactive non-login shells; set aliases, functions, and shell options.\n.bash_history: Stores command history.\n\nZsh:\n\n.zprofile: For login shells; serves the same function as .bash_profile.\n.zshrc: For all interactive shells; configure shell environment.\n.zsh_history: Stores command history."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#interactive-login-shells-vs-non-login-shells",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#interactive-login-shells-vs-non-login-shells",
    "title": "Shell Configurations",
    "section": "",
    "text": "A login shell is a shell session that starts when you log into your system, either directly or remotely. It’s called a “login shell” because it is associated with an initial user login.\nExamples of login shells:\n\nLogging in via console (i.e. keyboard & mouse) or SSH into a physical or a virtual machine (or any kind of remote server)\nExecuting bash --login or zsh --login to start a shell as a login shell.\n\nPurpose and common settings in login shells: - Environment variables: Often, login shells set environment variables like PATH, EDITOR, and others that should be configured only once per session - Startup Commands: Commands that need to run only once per session (commonly used for setting up session-specific services)\nNon-login shells, then, are shells that aren’t login shells. Now that we have a better understanding of the different kinds of shells, let’s proceed with the comparison of the shell configuration files."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#bash-vs-zsh-configuration-files",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#bash-vs-zsh-configuration-files",
    "title": "Shell Configurations",
    "section": "",
    "text": "Below is a detailed comparison of these shell configuration files.\n\n\n\n\n\n\n\nConfiguration file for interactive non-login Bash shells\n\n\n\n\nAliases (e.g., alias ll='ls -alF')\nShell options (e.g., shopt -s histappend)\nFunctions and prompt customization.\n\n\n\n\n\n\n\nConfiguration file for interactive login Bash shells\n\n\n\n\nEnvironment variables (e.g., export PATH=...)\nCommands that should run only once at login.\nOften includes sourcing .bashrc to ensure non-login shell configurations are available.\n\nTo source .bashrc, use:\nif [ -f ~/.bashrc ]; then\n  . ~/.bashrc\nfi\n\n\n\n\n\n\n\nThese files are the analogs of .bashrc and .bash_profile and the distinction between then is also analogous, One’s for non-login shells and the other’s for login shells. One key difference is that unlike .bashrc, .zshrc is is also sourced in login shells by default (eliminating the need to source it from .zprofile).\n\n\n\nThese files store the command history for Bash or Zsh shells, respectively.\n\n\n\nUsed by the history command (history)\nConfigurable via shell variables like HISTSIZE, HISTFILESIZE, and HISTCONTROL. For a simple configuration that retains only the history we want to keep, see this excellent post by Benjamin Wüthrich.\n\n\n\n\nFunctionality - Used by the history command (history). - Configurable via options like HISTSIZE, SAVEHIST, and HISTFILE. For ideas how to configure the history"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#login-and-non-login-shell-execution-flow",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#login-and-non-login-shell-execution-flow",
    "title": "Shell Configurations",
    "section": "",
    "text": "Login Shell: - Reads /etc/profile (system-wide configurations). - Reads the first found file among ~/.bash_profile, ~/.bash_login, or ~/.profile. Non-login Interactive Shell: - Reads ~/.bashrc.\n\n\n\nLogin Shell: - Reads /etc/zprofile (system-wide configurations). - Reads ~/.zprofile. - Then proceeds to read ~/.zshrc (unlike Bash, Zsh reads .zshrc for login shells by default). Interactive Shell: - Reads ~/.zshrc."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#best-practices",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#best-practices",
    "title": "Shell Configurations",
    "section": "",
    "text": "For Bash, source .bashrc from .bash_profile to ensure configurations are consistent across login and non-login shells.\nFor Zsh, since .zshrc is read by both login and interactive shells, you can place most configurations there.\n\n\n\n\nSet in .bash_profile or .zprofile for variables that should be set once per session.\n\n\n\nPlace in .bashrc or .zshrc as they are needed for interactive use."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#practical-examples",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#practical-examples",
    "title": "Shell Configurations",
    "section": "",
    "text": "# .bash_profile\n\n# Set environment variables\nexport PATH=\"$HOME/bin:$PATH\"\nexport EDITOR=\"vim\"\n\n# Source .bashrc if it exists\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n\n\n# .bashrc\n\n# Aliases\nalias ll='ls -alF'\nalias gs='git status'\n\n# Prompt customization\nPS1='[\\u@\\h \\W]\\$ '\n\n# Shell options\nshopt -s histappend\n\n\n\n# .zshrc\n\n# Load Oh My Zsh\nexport ZSH=\"$HOME/.oh-my-zsh\"\nZSH_THEME=\"agnoster\"\nplugins=(git docker kubectl)\nsource $ZSH/oh-my-zsh.sh\n\n# Aliases\nalias k='kubectl'\nalias d='docker'\n\n# Prompt customization\nPROMPT='%n@%m %1~ %# '\n\n# History settings\nHISTSIZE=10000\nSAVEHIST=10000\nHISTFILE=~/.zsh_history"
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#additional-notes",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#additional-notes",
    "title": "Shell Configurations",
    "section": "",
    "text": "Both Bash and Zsh allow you to configure how commands are saved to history files. You can prevent certain commands from being saved or set up shared history between sessions.\n\n\n\nUse the source or . command to include configurations from other files. This helps in modularizing your shell configurations.\n\n\n\nBe cautious when using shell-specific features if you switch between shells (like between Bash and Zsh). You can use .profile for environment variables to maintain compatibility across different shells if needed."
  },
  {
    "objectID": "unpublished_posts/local_development_setup/shell_configs.html#comparison-in-summary",
    "href": "unpublished_posts/local_development_setup/shell_configs.html#comparison-in-summary",
    "title": "Shell Configurations",
    "section": "",
    "text": "Bash:\n\n.bash_profile: For login shells; set environment variables and startup programs.\n.bashrc: For interactive non-login shells; set aliases, functions, and shell options.\n.bash_history: Stores command history.\n\nZsh:\n\n.zprofile: For login shells; serves the same function as .bash_profile.\n.zshrc: For all interactive shells; configure shell environment.\n.zsh_history: Stores command history."
  },
  {
    "objectID": "unpublished_posts/parallel_systems/asynchronous_programming.html",
    "href": "unpublished_posts/parallel_systems/asynchronous_programming.html",
    "title": "Parallel Programming",
    "section": "",
    "text": "CPU Threads\nIt all begins with CPU threads….\n\n\nParallelism vs Concurrency vs Asynchronicity\nAsynchronicity is about processes that do not overalap in time. When it comes to concurrency and parallelism, the distinction is subtle. Quoting Sun’s Multithreaded Programming Guide:\n\nConcurrency: A condition that exists when at least two threads are making progress. A more generalized form of parallelism that can include time-slicing as a form of virtual parallelism.\nParallelism: A condition that arises when at least two threads are executing simultaneously.\n\nSo it appears concurrency is more about processes that may overlap somewhat in time, whereas parallelism is about processes that literally occupy the same instance/interval in time.\nAll of these use similar or related constructs to help them achieve this. The use of callbacks, promises, or other constructs is common in handle=ng the execution of asynchronous code (such as file I/O, network requests, or user input). Instead of waiting, it’s about not blocking the execution of the main program’s thread which allows the program to move on to other tasks. When the long-running code is finished, the program is notified and can move on to other tasks. It’s all about multi-threaded architecture, and bringing it all together at the end to notify the calling thread of the asynchronous code’s completion, failure, or progress.\nThese programming paradigms can make our application more responsive and improve its speed, but they can also make the code more complex and harder to debug, as we now have to deal with issues like synchronization and the management of shared resources. One way to mitigate these problems is functional programming which you can read more about here.\n\n\n\n\n\n\n\nMetric\nUseful Plumbing Analogy\n\n\n\n\nLatency\nThe amount of time it takes to travel through a tube\n\n\nBandwidth\nHow wide the tube is\n\n\nThroughput\nThe rate of water flow"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "",
    "text": "Manim (docs) is a custom Python library for mathematical animation. It has two versions,. The first, and original, is maintained by Grant Sanderson himself (the creator of the library and the Stanford professor who goes by the username 3blue1brown on YouTube). The forked version, called the Manim community edition enjoys more open source contributions and has the better documentation and support (attentiveness to PRs, etc.). For this series of posts on Manim, I will use the community edition to keep this document relevant.\n\n\nUsing virtualenv (venv), let’s create an isolated Python environment to contain the Python version and the versions of library dependencies for our Manim projects. Note that venv is quite old, and the new way of managing Python environments and packages is Pipenv (a merger of pip and venv). More on that later… For this project we’ll stick with venv because I like having physical folders that contain the various environments used across my notes on this site.\n\n\nIf you have multiple Python versions installed (through various different environment managers available for Python), do which pip or which pip3 (or which python/which python3). The former, pip, is a package manager that comes pre-installed with a Python installation (it pulls from places like pypi.org), so it will always correspond with some version of Python installed on your machine.\nIn my case, pip corresponds to ~/anaconda3/bin/pip (which is the Python installation that came with Conda).\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou have to be in an active Conda environment to see any sort of meaningful output for which pip (assuming, of course, that there’s no globally installed version on your machine that’s aliased to pip. If there is such an aliasing, the unalias command can help).\n\n\n\nConda is an environment and package manager for Python, much like venv but also for managing packages (and not just environments). However, it also supports R, C++, and other languages and frameworks. This is typically useful for data science/analysis purposes, not so much in our use case. Conda also lacks support for some Manim community libraries/plugins.\nThe other installation, pip3, corresponds to /opt/homebrew/bin/pip3 which came with the python3 installation from Homebrew (for more on Homebrew, see this post).\nFor more on Conda see this legacy post (legacy posts are unpublished posts that serve as repositories of unorganized thoughts and snippets).\n\n\n\n\n\n\nTip\n\n\n\n\n\nUltimately, having a mess of a Python environment like what we saw above is bad. Very bad. If you find yourself encountering any issues (such as certain libraries that should be installed not being recognized inside the cells, or failures being thrown that have cryptic error messages indicating the problem lies in one of these libraries) clean up your Python environment(s) by uninstalling Homebrew and Conda. For Homebrew:\n\nCheck this README for a CURL to run a provided uninstaller.\nOn Apple Silicon, edit shell configuration file ~/.zprofile (using vim) which sets the $PATH variable to give priority to Homebrew packages inside /opt/homebrew (for the various different shell configuration files see this post).\nAlso remove the residual files the uninstaller leaves behind by sudo rm -rf /opt/homebrew/\n\nSimilar instructions exist for uninstalling Conda. At the end of this, which python should fail to output anything and which python3 will correspond to /usr/bin/python3 but, depending on the version of Mac OS, that may actually just be a pointer to /usr/local/bin/python3 (where it’s installed through apt/yum or shipped with the OS). If you have multiple OS-level Python installations, these will all show up in /usr/local/bin. For example, if you have two versions of python3 installed, the output of ls -a /usr/local/bin | gerp python will be something like:\npython3\npython3-config\npython3-intel64\npython3t\npython3t-config\npython3t-intel64\npython3.13\npython3.13-config\npython3.13-intel64\npython3.13t\npython3.13t-config\npython3.13t-intel64\nNow you may install Brew again, this time stick with Python 3.9. Then stick to using pip3.9 install &lt;PackageName&gt; all the time, or make sure the python3 in the terminal configuration is correctly linked to the version installed with Homebrew. You can do so by unalias pip and alias pip=pip3.9.\nOther than aliasing, there are also symbolic links. This is where things can get confusing, read this Stack thread for more information. For instance, again depending on the version of Mac OS, /usr/local/bin/python3 could be a symbolic link to a Brew installation. Running ls -al /usr/local/bin/python3 will reveal if that’s the case. If the output is something like /usr/local/bin/python3 -&gt; ../../../Library/Frameworks/Python.framework/Versions/3.13/bin/python3, then it’s not a symbolic link.\n\n\n\n\n\n\n\n\n\nManim depends on py3cairo, ffmpeg and some other dependencies for Apple Silicon machines and LaTeX support.\n\n\n\nLibrary\nInstallation command\n\n\n\n\npy3cairo\nbrew install py3cairo\n\n\nffmpeg\nbrew install ffmpeg\n\n\nmactex\nbrew install --cask mactex-no-gui\n\n\npango\nbrew install pango\n\n\npkg-config\nbrew install pkg-config\n\n\nscipy\nbrew install scipy\n\n\n\nExecute:\nbrew install py3cairo ffmpeg\n\n# Form LaTeX support in Manim\nbrew install --cask mactex-no-gui\n\n# Extra dependencies for Apple Silicon\nbrew install pango pkg-config scipy\n\n\n\n\n\n\nTip\n\n\n\n\n\nA combination of brew info &lt;PackageName&gt; and which &lt;PackageName&gt; can tell us where each package got installed (either in /opt/homebrew/bin or /opt/homebrew/Cellar).\nNote that the command which py3cairo, for example, won’t return anything because py3cairo is a Python module, not an executable program. The which command is used to locate the executable file associated with a given command by searching the directories listed in your PATH environment variable. Since py3cairo doesn’t install an executable named py3cairo, which won’t find it.\nHowever, if Homebrew (brew) says it’s installed, it means the py3cairo package is present on your system.\n\n\n\n\n\n\nFirst, create a new virtual environment. Issue the following command in your terminal:\npython3 -m venv --system-site-package manim-sandbox\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe --system-site-package flag ensures the environment will inherit from the global packages installed by Homebrew later on, and therefore get pycairo and other dependencies from the Brew installation.\n\n\n\nThis will create a folder /manim-sandbox in the project’s root directory.\n\n\n\nTo activate, issue command:\nsource manim-sandbox/bin/activate\nWhich runs the activate script of the virtual environment.\nTo deactivate, simply do:\ndeactivate\n\n\n\nInside an active virtual environment, issue which pip. The output now is: ~/Documents/github/v-poghosyan.github.io/manim-sandbox/bin/pip which corresponds to the version of Python installed inside the project’s environment folder manim-sandbox. This is just what we want. Note that if we do which pip3 or which python3 we will still get the versions that are contained within the virtual environment because venv creates virtual environments with multiple versions of pip and python already installed alongside one another (including pip3/python3).\nTo check if we indeed have a clean slate of an environment, we can do pip list which should just show one package, pip itself.\nOutput:\nPackage Version\n------- -------\npip     24.0\nLet’s upgrade pip for good measure. Issue:\npip install --upgrade pip\nNow, install Manim.\npip install manim\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\n\nThis might hang up on getting some of the dependencies (such as, maybe, opencv-python) with an error that looks like Building wheel for opencv-python.... If you see this, don’t just re-try. First, check the version history of the package on pypi.org and then try manually installing that particular dependency with something like:\npip install opencv-python==4.7.0.72\n\n\n\nOptionally, let’s also install jupyter-manim which enables the magic command in Jupyter notebooks:\n%%manim -qm &lt;SceneName&gt;\nThis must be provided at the top of each cell. It is similar to issuing the manim command in a terminal (cell magic is just Jupyter’s equivalent of that).\n\n\n\n\n\n\nNote\n\n\n\n\n\nMagic commands behave exactly like command line commands with their arguments. For example, to display manim help options, you can use:\n%%manim -h\nTo see a list of all magics available, do %lsmagic\n\n\n\nTo add this ability to Jupyter notebooks, install the following package:\npip install jupyter-manim\n\n\n\nRather than committing the entire environment to the repository of a project, we commit a requirements.txt containing the output of pip list generated by:\npip freeze &gt; requirements.txt\nTo get the packages inside this file in a new environment, do:\npip install -r requirements.txt\n\n\n\n\n\nGrant Sanderson himself defines a custom script that hooks into a local Python file defining the entire scene. The script continually parses the file looking for certain leading comments (usually before class or function definitions). It then renders the scene only up to that comment, so that the scene pauses at the part of the code he’s working on (so that the state of the scene is maintained until that point). He then uses an IDE shortcut to execute that single code block (which is clearly demarcated with a leading comment) inside a Python interpreter. The interpreter then renders the relevant portion of the scene to the output window. In this way, it’s possible to incrementally test parts of a scene (mimicking a Jupyter notebook setup).\nMore on this specific setup can be found in the official Manim YouTube tutorial.\nI have created a dedicated repository called manim-projects (which is also linked in the title banner) which will contain my Manim projects and will mimic this setup. However, throughout these notes, I will use jupyter-manim’s cell magic %%manim to render Manim scenes directly inside the repository containing these notes. When I do so, on my end, the result will get rendered inside v-poghosyan.github.io/posts/mathematical_visualization/media directory (and from there I can reference them inside these posts).\nNow let’s draw some actual Manim scenes."
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#local-development-environment-setup",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#local-development-environment-setup",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "",
    "text": "Manim (docs) is a custom Python library for mathematical animation. It has two versions,. The first, and original, is maintained by Grant Sanderson himself (the creator of the library and the Stanford professor who goes by the username 3blue1brown on YouTube). The forked version, called the Manim community edition enjoys more open source contributions and has the better documentation and support (attentiveness to PRs, etc.). For this series of posts on Manim, I will use the community edition to keep this document relevant.\n\n\nUsing virtualenv (venv), let’s create an isolated Python environment to contain the Python version and the versions of library dependencies for our Manim projects. Note that venv is quite old, and the new way of managing Python environments and packages is Pipenv (a merger of pip and venv). More on that later… For this project we’ll stick with venv because I like having physical folders that contain the various environments used across my notes on this site.\n\n\nIf you have multiple Python versions installed (through various different environment managers available for Python), do which pip or which pip3 (or which python/which python3). The former, pip, is a package manager that comes pre-installed with a Python installation (it pulls from places like pypi.org), so it will always correspond with some version of Python installed on your machine.\nIn my case, pip corresponds to ~/anaconda3/bin/pip (which is the Python installation that came with Conda).\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou have to be in an active Conda environment to see any sort of meaningful output for which pip (assuming, of course, that there’s no globally installed version on your machine that’s aliased to pip. If there is such an aliasing, the unalias command can help).\n\n\n\nConda is an environment and package manager for Python, much like venv but also for managing packages (and not just environments). However, it also supports R, C++, and other languages and frameworks. This is typically useful for data science/analysis purposes, not so much in our use case. Conda also lacks support for some Manim community libraries/plugins.\nThe other installation, pip3, corresponds to /opt/homebrew/bin/pip3 which came with the python3 installation from Homebrew (for more on Homebrew, see this post).\nFor more on Conda see this legacy post (legacy posts are unpublished posts that serve as repositories of unorganized thoughts and snippets).\n\n\n\n\n\n\nTip\n\n\n\n\n\nUltimately, having a mess of a Python environment like what we saw above is bad. Very bad. If you find yourself encountering any issues (such as certain libraries that should be installed not being recognized inside the cells, or failures being thrown that have cryptic error messages indicating the problem lies in one of these libraries) clean up your Python environment(s) by uninstalling Homebrew and Conda. For Homebrew:\n\nCheck this README for a CURL to run a provided uninstaller.\nOn Apple Silicon, edit shell configuration file ~/.zprofile (using vim) which sets the $PATH variable to give priority to Homebrew packages inside /opt/homebrew (for the various different shell configuration files see this post).\nAlso remove the residual files the uninstaller leaves behind by sudo rm -rf /opt/homebrew/\n\nSimilar instructions exist for uninstalling Conda. At the end of this, which python should fail to output anything and which python3 will correspond to /usr/bin/python3 but, depending on the version of Mac OS, that may actually just be a pointer to /usr/local/bin/python3 (where it’s installed through apt/yum or shipped with the OS). If you have multiple OS-level Python installations, these will all show up in /usr/local/bin. For example, if you have two versions of python3 installed, the output of ls -a /usr/local/bin | gerp python will be something like:\npython3\npython3-config\npython3-intel64\npython3t\npython3t-config\npython3t-intel64\npython3.13\npython3.13-config\npython3.13-intel64\npython3.13t\npython3.13t-config\npython3.13t-intel64\nNow you may install Brew again, this time stick with Python 3.9. Then stick to using pip3.9 install &lt;PackageName&gt; all the time, or make sure the python3 in the terminal configuration is correctly linked to the version installed with Homebrew. You can do so by unalias pip and alias pip=pip3.9.\nOther than aliasing, there are also symbolic links. This is where things can get confusing, read this Stack thread for more information. For instance, again depending on the version of Mac OS, /usr/local/bin/python3 could be a symbolic link to a Brew installation. Running ls -al /usr/local/bin/python3 will reveal if that’s the case. If the output is something like /usr/local/bin/python3 -&gt; ../../../Library/Frameworks/Python.framework/Versions/3.13/bin/python3, then it’s not a symbolic link.\n\n\n\n\n\n\n\n\n\nManim depends on py3cairo, ffmpeg and some other dependencies for Apple Silicon machines and LaTeX support.\n\n\n\nLibrary\nInstallation command\n\n\n\n\npy3cairo\nbrew install py3cairo\n\n\nffmpeg\nbrew install ffmpeg\n\n\nmactex\nbrew install --cask mactex-no-gui\n\n\npango\nbrew install pango\n\n\npkg-config\nbrew install pkg-config\n\n\nscipy\nbrew install scipy\n\n\n\nExecute:\nbrew install py3cairo ffmpeg\n\n# Form LaTeX support in Manim\nbrew install --cask mactex-no-gui\n\n# Extra dependencies for Apple Silicon\nbrew install pango pkg-config scipy\n\n\n\n\n\n\nTip\n\n\n\n\n\nA combination of brew info &lt;PackageName&gt; and which &lt;PackageName&gt; can tell us where each package got installed (either in /opt/homebrew/bin or /opt/homebrew/Cellar).\nNote that the command which py3cairo, for example, won’t return anything because py3cairo is a Python module, not an executable program. The which command is used to locate the executable file associated with a given command by searching the directories listed in your PATH environment variable. Since py3cairo doesn’t install an executable named py3cairo, which won’t find it.\nHowever, if Homebrew (brew) says it’s installed, it means the py3cairo package is present on your system.\n\n\n\n\n\n\nFirst, create a new virtual environment. Issue the following command in your terminal:\npython3 -m venv --system-site-package manim-sandbox\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe --system-site-package flag ensures the environment will inherit from the global packages installed by Homebrew later on, and therefore get pycairo and other dependencies from the Brew installation.\n\n\n\nThis will create a folder /manim-sandbox in the project’s root directory.\n\n\n\nTo activate, issue command:\nsource manim-sandbox/bin/activate\nWhich runs the activate script of the virtual environment.\nTo deactivate, simply do:\ndeactivate\n\n\n\nInside an active virtual environment, issue which pip. The output now is: ~/Documents/github/v-poghosyan.github.io/manim-sandbox/bin/pip which corresponds to the version of Python installed inside the project’s environment folder manim-sandbox. This is just what we want. Note that if we do which pip3 or which python3 we will still get the versions that are contained within the virtual environment because venv creates virtual environments with multiple versions of pip and python already installed alongside one another (including pip3/python3).\nTo check if we indeed have a clean slate of an environment, we can do pip list which should just show one package, pip itself.\nOutput:\nPackage Version\n------- -------\npip     24.0\nLet’s upgrade pip for good measure. Issue:\npip install --upgrade pip\nNow, install Manim.\npip install manim\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\n\nThis might hang up on getting some of the dependencies (such as, maybe, opencv-python) with an error that looks like Building wheel for opencv-python.... If you see this, don’t just re-try. First, check the version history of the package on pypi.org and then try manually installing that particular dependency with something like:\npip install opencv-python==4.7.0.72\n\n\n\nOptionally, let’s also install jupyter-manim which enables the magic command in Jupyter notebooks:\n%%manim -qm &lt;SceneName&gt;\nThis must be provided at the top of each cell. It is similar to issuing the manim command in a terminal (cell magic is just Jupyter’s equivalent of that).\n\n\n\n\n\n\nNote\n\n\n\n\n\nMagic commands behave exactly like command line commands with their arguments. For example, to display manim help options, you can use:\n%%manim -h\nTo see a list of all magics available, do %lsmagic\n\n\n\nTo add this ability to Jupyter notebooks, install the following package:\npip install jupyter-manim\n\n\n\nRather than committing the entire environment to the repository of a project, we commit a requirements.txt containing the output of pip list generated by:\npip freeze &gt; requirements.txt\nTo get the packages inside this file in a new environment, do:\npip install -r requirements.txt"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#alternate-setup",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#alternate-setup",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "",
    "text": "Grant Sanderson himself defines a custom script that hooks into a local Python file defining the entire scene. The script continually parses the file looking for certain leading comments (usually before class or function definitions). It then renders the scene only up to that comment, so that the scene pauses at the part of the code he’s working on (so that the state of the scene is maintained until that point). He then uses an IDE shortcut to execute that single code block (which is clearly demarcated with a leading comment) inside a Python interpreter. The interpreter then renders the relevant portion of the scene to the output window. In this way, it’s possible to incrementally test parts of a scene (mimicking a Jupyter notebook setup).\nMore on this specific setup can be found in the official Manim YouTube tutorial.\nI have created a dedicated repository called manim-projects (which is also linked in the title banner) which will contain my Manim projects and will mimic this setup. However, throughout these notes, I will use jupyter-manim’s cell magic %%manim to render Manim scenes directly inside the repository containing these notes. When I do so, on my end, the result will get rendered inside v-poghosyan.github.io/posts/mathematical_visualization/media directory (and from there I can reference them inside these posts).\nNow let’s draw some actual Manim scenes."
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#the-scene-object",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#the-scene-object",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "The Scene object",
    "text": "The Scene object\nWe can now draw out first Scene. A guiding principle behind Manim’s design is that everything should be transformable into everything else. We can transform shapes into other shapes and even transform text elements into shapes (we will see an example of this shortly)."
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#square-to-circle-transformation",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#square-to-circle-transformation",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "Square to circle transformation",
    "text": "Square to circle transformation\nImporting jupyter-manim for the access to the manim cell magic within these notes. This step is imperative. It must complete inside its own Jupyter Notebook cell before any cell that uses the %%manim cell magic can be run.\n\nimport jupyter_manim\n\n\n%%manim -qm SquareToCircle\n\nfrom manim import *\n\nclass SquareToCircle(Scene):\n    def construct(self):\n        # Adding simple geometry\n        circle = Circle()\n        square = Square()\n\n        # Animating geometry\n        self.play(Create(square))\n        self.play(Transform(square,circle))\n        self.play(FadeOut(square))\n\nHere I have manually referenced the video created by Manim in the aforementioned /media directory within the post category directory (/mathematical_visualization).\n\n\n\n\n\nVideo\nSquare to Circle Manim Animation\n\n\n\n\nFigure 1\n\n\n\nAs we can see from the Manim code above:\n\nEverything in Manim derives from the Scene class which offers a constructor method.\nIn this constructor method, we define every object that’s in our scene, every transformation, and every other sequence of actions. Everything interesting, in short, happens inside the constructor."
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#rendering-text",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#rendering-text",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "Rendering text",
    "text": "Rendering text\nLet’s do some stuff with Text.\n\n%%manim -qm HelloWorld\n\nfrom manim import *\n\nclass HelloWorld(Scene):\n    def construct(self):\n        # Adding simple text\n        text = Text(\"Hello World!\", font_size=42)\n\n        # Rendering text\n        self.play(Write(text, run_time=3))\n        self.play(FadeOut(text))\n        \n\n\n\n\n\n\nVideo\nHello World Manim Animation\n\n\n\n\nFigure 2\n\n\n\nYou’ll notice that the text has property font_size which we set to 42. There are more of these properies that Mobjects have, here are several (more will be introduced later):\n\n\n\nProperty\nDescription\n\n\n\n\nfont_size\nThe size of the text (e.g. 42)\n\n\ncolor\nThe color of the text (e.g. BLUE, GREEN)\n\n\nfill_opacity\nThe font of the text (e.g. 0.8)\n\n\n\n\nText to square to circle transformation\nA core design philosophy of Manim is that everything should be transformable into everything else. We can transform shapes into other shapes and even transform text elements into shapes. Let’s see an example of this now.\nWe will transform the text “Hello World!” into a circle, which will then morph into a square. To see how to index into the Text object, we’ll only transform an individual character of the text into the Manim object (or Mobject, in short).\n\n%%manim -qm HelloWorldToSquareToCircle\n\nfrom manim import *\n\nclass HelloWorldToSquareToCircle(Scene):\n    def construct(self):\n        # Adding simple text\n        text = Text(\"Hello World!\", font_size=42)\n        text.to_edge(UP)\n\n        # Adding simple geometry        \n        circle = Circle()\n        square = Square()\n\n        # Rendering text\n        self.play(Write(text, run_time=3))\n\n        # Transforming the text into a square\n        self.play(Transform(text[0], square))\n\n        # Animating geometry\n        self.play(Transform(text[0], circle))\n        self.play(FadeOut(text[0]))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThere’s a quirk in the way Manim reads syntactically… One would hope to say Transform(text[0], square), then Transform(square, circle), and finally FadeOut(circle). However, the square is the transformed text[0] Mobject, and the circle continues to be that very same Mobject, so we continue referring to it as that. Later on we’ll see how to organize our code to get around this problem.\n\n\n\n\n\n\n\n\nVideo\nHello World to Square to Circle Manim Animation\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#different-ways-of-rendering-objects---add-play-wait",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#different-ways-of-rendering-objects---add-play-wait",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "Different ways of rendering objects - add, play, wait",
    "text": "Different ways of rendering objects - add, play, wait\nNote that we can also render objects by using add (see docs), as opposed to play (see docs). The difference is that play is animated, while add is not."
  },
  {
    "objectID": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#axes-and-other-math-y-stuff",
    "href": "unpublished_posts/mathematical_visualization/seting_up_manim_in_jupyter.html#axes-and-other-math-y-stuff",
    "title": "Setting up Manimin Jupyter Notebooks",
    "section": "Axes and other math-y stuff",
    "text": "Axes and other math-y stuff\nWhat’s the point of using a mathematical animation library if we can’t animate math? Let’s see how to animate some math-y stuff with Manim.\n\n%%manim -qm MathDemo\n\nfrom manim import *\n\nclass MathDemo(Scene):\n    def construct(self):\n        # Adding axes\n        ax = Axes(x_range=(-3,3,1), y_range=(-3,3,1))\n        self.add(ax)\n\n        # Adding a quadratic polynomial with zeros at x=0, and x=2\n        poly = ax.plot(lambda x: -x * (x-2), color=RED)\n        self.add(poly)\n\n        # Highlighting the area under the graph on [0,2]\n        area = ax.get_area(poly, x_range=(0,2), opacity=0.3)\n        self.add(area)\n\nBut because we’re using the add method of Scene instead of play, we’re not producing animations. We’ll fix that shortly, but first let’s look at the general mechanism of Manim’s animation system.\n\nAnimation system in Manim\nwhile Scene.add adds Mobjects immediately to the scene, and Scane.wait adds a pause, Scene.play plays animations (or Manimations…).\nAnimations can:\n\nadd Mobjects (e.g. Create, FadeIn, …)\nchange Mobjects (e.g. Transform)\nemphasize Mobjects (e.g. Indicate, Circumscribe, …)\nremove Mobjects (e.g. Uncreate, FadeOut, …)\n\nNow let’s go back to our math-y example and animate it!\n\n%%manim -qm MathDemo\n\nfrom manim import *\n\nclass MathDemo(Scene):\n    def construct(self):\n        # Adding axes\n        ax = Axes(x_range=(-3,3,1), y_range=(-3,3,1), color=BLACK)\n\n        # Adding a quadratic polynomial with zeros at x=0, and x=2\n        poly = ax.plot(lambda x: -x * (x-2), color=RED)\n\n        # Highlighting the area under the graph on [0,2]\n        area = ax.get_area(poly, x_range=(0,2), opacity=0.3)\n        self.play(Create(ax), Create(poly), run_time=3)\n        self.play(FadeIn(area))\n\n\n\n\n\n\nVideo\nMath Demo Manim Animation\n\n\n\n\nFigure 4"
  }
]
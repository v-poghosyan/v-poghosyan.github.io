[
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let‚Äôs define a package: com.myservice."
  },
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let‚Äôs define a package: com.myservice."
  },
  {
    "objectID": "unpublished_posts/machine_learning/convolutional_neural_nets.html",
    "href": "unpublished_posts/machine_learning/convolutional_neural_nets.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Let‚Äôs ask ChatGPT to create a CNN with some given specifications using the following prompt:\nUsing tensorflow build a CNN model with the following specifications: \n1. Input shape (28,28,1)\n2. 2d convolutional layer with 32 filters, kernel size (3,3), and relu activation\n3. MaxPooling2D layer with pool size of (2,2)\n4. A flatten layer to convert the 2d output into a 1d vector\n5. A dense layer with 128 units and relu activation\n6. An output layer with 1o units and softmax activation\nGive the model summary\nThe output is:\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Define the CNN model\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n D)                                                              \n                                                                 \n flatten (Flatten)           (None, 5408)              0         \n                                                                 \n dense (Dense)               (None, 128)               692352    \n                                                                 \n dense_1 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 693962 (2.65 MB)\nTrainable params: 693962 (2.65 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "",
    "text": "In this post we‚Äôll be going over how to set up our local development environment for making machine learning applications and blogging about the process. This is my attempt at installing the required software packages on a Windows machine. I‚Äôll do my best to keep things general but some of the steps will be Windows specific."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Installing Python - System Level",
    "text": "Installing Python - System Level\nFirst, we download and install the latest version of Python for our OS from the official website.\n\n\n\n\n\n\nüí° Tip\n\n\n\n\n\nMake sure to tick the ‚Äúadd to PATH‚Äù box during the installation so that the path of the Python executible is added to our system‚Äôs PATH environment variable. The path in question, by default, is ~\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n\n\n\n\n\n\n\n\n\nüîß Troubleshooting\n\n\n\n\n\nIf the command python is unrecognized on Windows after installation, try py. We should be able to issue the command py to invoke the Python interpreter. Running py --version should return the version number (e.g.¬†Python 3.11.5)"
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Why use Conda",
    "text": "Why use Conda\nWhile pip is Python‚Äôs built-in package manager and venv is its built-in virtual environment manager, we use Conda because it attempts to do more than what pip and venv try to accomplish do individually by extending support to library dependencies not written in Python.\nOccasionally, when a Conda distribution is not available, but an PyPI distribution exists, it makes sense to combine use of conda and pip. This is done by:\n\nInstalling pip within a Conda environment: conda install pip\nInstalling the required package from inside the active Conda environment: pip install &lt;package_name&gt;\n\nThis way, the packages do not go to the system-level Python‚Äôs packages directory C:\\Users\\&lt;username&gt;\\AppData\\Local\\Python\\&lt;version&gt;\\ (or Roaming instead of Local, if Python was installed only for a specific user on Windows). Instead, pip installs them in the Conda environment‚Äôs C:\\ProgramData\\anaconda3\\Lib\\site-packages (or similar) package directory. We can check each package, along with its installation destination by running pip list -v.\n\nInstalling Anaconda Navigator (or Miniconda)\nNext, download and install Anaconda Navigator (or Miniconda, which installs the Conda scientific package and Python environment manager without additional software and without the GUI navigator). This installation includes tools like Jupyter Notebooks, Spyder, PyCharm, and other scientific packages and IDEs.\n\n\n\n\n\n\nüìñ Note\n\n\n\n\n\nAnaconda‚Äôs built in Python distribution: Anaconda comes with its own latest Python version distribution (by default installed into path c:\\ProgramData\\anaconda3\\python.exe). The installer will prompt us to select an option which enables third-party editors, such as VSCode, to recognize this Python distribution.\n\n\n\n\n\n\n\n\n\nüìñ Note\n\n\n\n\n\nDifferent Python distributions can live on the same machine: Running python --version in the Anaconda Prompt returns Python 3.11.4 as of the time of writing this, which is the version of Python that Conda installed in its base environment. Crucially, running py --version, even in the Anaconda Prompt, still returns Python 3.11.5, which is the system‚Äôs version of Python."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Choosing the Right Python Kernel in VSCode",
    "text": "Choosing the Right Python Kernel in VSCode\nIn VSCode, we can open the Command Palette and run the command Notebook: Select Notebook Kernel. At first, this will prompt us to install the Jupyter and Python VSCode extensions. Once that‚Äôs done, we can rerun the command and select the Python kernel in the desired Conda environment (by default base)."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Initializing Conda in the Shell",
    "text": "Initializing Conda in the Shell\nBefore we can use the full capabilities of Conda in the terminal, we need to initialize it by running the command:\nconda init &lt;bash|powershell|tsh|...&gt; # Depending on the shell we're using\nRestart your terminal for changes to take hold.\n\n\n\n\n\n\nüîß Troubleshooting\n\n\n\n\n\nFor Windows users, Powershell may throw the following error in trying to load the user profile: execution of scripts is disabled on this system. This is Powershell‚Äôs security measure against command hijacking, its way of enforcing control of execution and establishing identity. If this is the case, run cmd.exe as Administrator and execute command powershell Set-ExecutionPolicy RemoteSigned -Scope CurrentUser. We should now see the active environment in parentheses (e.g.¬†base) to the left of the input in Powershell."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Conda Commands",
    "text": "Conda Commands\nSome common Conda commands are:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nconda env list\nShows all the Conda environments (the active environment is marked with *)\n\n\nconda list\nShows all ther packages installed in the currently active environment\n\n\nconda update --all\nUpdates all packages in the active environment (frequently resolves environment is inconsistent errors)\n\n\nconda info\nShows, among other things, the directory where the environment is stored\n\n\nconda activate &lt;myenv&gt;\nActivate environment &lt;myenv&gt;\n\n\nconda deactivate\nDeactivates the currently active environment\n\n\nconda create --name &lt;myenv&gt;\nCreate a new empty environment\n\n\nconda create --name &lt;myenv&gt; --clone base\nClone the base environment\n\n\nconda env export -f &lt;path/to/envfile.yml&gt;\nExport the package list of the active environment (e.g.¬†conda env export -f  /Users/&lt;username&gt;/Documents/MyFiles/personal-blog.yml)\n\n\nconda compare &lt;path/to/envfile.yml&gt;\nCompare the active environment to the exported file of another environment\n\n\nconda remove --name &lt;myenv&gt; --all\nDeletes the environment\n\n\n\n\nComparing Conda Environments\nOften we need to compare the packages between two environments. Here‚Äôs the workflow to do that:\n\nActivate one of the environments using activate\nExport its package list using export as a .yml file to a destination of our choice\nActivate the second environment\nExecute the compare command, providing the path to the .yml file created in the previous step"
  },
  {
    "objectID": "unpublished_posts/python/conways_game_of_life.html",
    "href": "unpublished_posts/python/conways_game_of_life.html",
    "title": "Conway‚Äôs Game of Life - First Scala Project",
    "section": "",
    "text": "Conway‚Äôs Game of Life\nConway‚Äôs Game of Life is a zero-player game on a two dimensional grid of cells with 4 simple rules:\n\nAny live cell with &lt; 2 live neighbors dies (as if by underpopulation)\nAny live cell with 2-3 live neighbors lives on to the next generation\nAny live cell with &gt; 3 live neighbors dies (as if by overpopulation)\nAny dead cell with exactly 3 live cells becomes alive (as if by reproduction)\n\nThese rules are inteded to loosely model the mechanisms of reproduction in evolutionary biology. Interestingly, these simple rules result in a highly complex world in which perpetual patterns (such as gliders) can be used to transmit information over long distances and execute computational tasks by coming together in specific arrangements to form logic gates. To see this in action, check out this cool post by Nicholas Carlini. The existence of logic gates in Conway‚Äôs Game of Life makes it possible to program within the game, leading to incredible projects like Life in Life, where Conway‚Äôs Game of Life runs itself.\nFor the sake of practice, we will use functional programming to implement the game. Functional programming draws inspiration from the mathematical definition of a function ‚Äì a well-defined operation on sets. For more information see the this post on the topic of FP."
  },
  {
    "objectID": "unpublished_posts/go/concurrency_in_go.html",
    "href": "unpublished_posts/go/concurrency_in_go.html",
    "title": "Concurrency in Go",
    "section": "",
    "text": "# Review of Linear Algebra and Geometry\nLet‚Äôs start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that‚Äôs commonly used in the subject."
  },
  {
    "objectID": "unpublished_posts/go/concurrency_in_go.html#convexity",
    "href": "unpublished_posts/go/concurrency_in_go.html#convexity",
    "title": "Concurrency in Go",
    "section": "Convexity",
    "text": "Convexity\nSet convexity is defined as follows:\n\nDefinition: ¬† A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\nFigure¬†1: Set A is convex, set B is non-convex\n\n\n\nOperations that Preserve Convexity\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "unpublished_posts/go/concurrency_in_go.html#examples-of-convex-sets",
    "href": "unpublished_posts/go/concurrency_in_go.html#examples-of-convex-sets",
    "title": "Concurrency in Go",
    "section": "Examples of Convex Sets",
    "text": "Examples of Convex Sets\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\nConvex Hull of \\(n\\) Points\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\nConvex hull\n\n\nFigure¬†2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis‚Äôs algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\nConvex Hull of a Set\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there‚Äôs an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it‚Äôs the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(‚Ä†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(‚Ä†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\nConvex hull of a set\n\n\nFigure¬†3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points ‚Äî it‚Äôs simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\nAffine Combination of \\(n\\) Points\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)‚Äôs need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it‚Äôs the line that passes through them, and for three points it‚Äôs the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nLinear Combinations - Hyperplanes and Halfspaces\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)‚Äôs totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\nHyperplanes\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere‚Äôs a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that‚Äôs been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we‚Äôll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\nHalfspaces\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\nConic Combinations of \\(n\\) Points\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\nEllipses\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)‚Äôs positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\nNorm Balls\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it‚Äôs a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\nPolyhedra\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\nThe Set of All Positive Semidefinite Matrices\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html",
    "href": "unpublished_posts/neuroscience_notes/grief.html",
    "title": "Overcoming Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "href": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "title": "Overcoming Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vahram's blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSSL - The Internet‚Äôs Trust Protocol\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecursion Optimizations\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctional Programming\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLC11 - Container with Most Water\n\n\n\nJan 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Optimization\n\n\n\nJan 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Algebra Refresher for Optimization\n\n\n\nJan 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html",
    "href": "posts/functional_programming/functional_programming.html",
    "title": "Functional Programming",
    "section": "",
    "text": "Functional programming draws inspiration from the mathematical definition of a function which is a well-defined operation on sets.\n\n\nTake \\(f: X \\rightarrow Y\\), which is a function that maps elements of set \\(X\\) to those of set \\(Y\\) such that for each \\(x\\) in set \\(X\\) (also denoted as ‚Äú\\(x \\in X\\)‚Äù), there‚Äôs one and only one \\(y \\in Y\\) which satisfies \\(f(x) = y\\). In ordinary language, we say that a mathematical function maps any given input to a unique output. That‚Äôs not to say that \\(f\\) can‚Äôt map the two different \\(x\\)‚Äôs to the same \\(y\\), but it can‚Äôt map the same \\(x\\) to more than one \\(y\\). Notice that the output in set \\(Y\\) depends only on the input from set \\(X\\), and that the function \\(f\\) only operates on set \\(X\\) and nothing external to it. In other words there is no hidden state (i.e.¬†some value outside \\(X\\)) that affects \\(f\\)-s output, so \\(f\\) always produces predictable output. Also, \\(f\\) doesn‚Äôt affect any element in \\(X\\) (or, for that matter, \\(Y\\)). The expression \\(f(x)\\) is simply understood as the function \\(f\\) applied to an element \\(x \\in X\\) which produces a value from the set \\(Y\\) (but, once again, it‚Äôs not like the specific element in the set \\(Y\\) is somehow retrieved or affected in any way).\n\n\n\nIn functional programming, we loosely think of built-in types or abstract classes (i.e.¬†some chunks of computer memory) as being like sets. This lets us, somewhat awkwardly, pluck the mathematical definition of a function from the concrete world of mathematics and bring it into the practical world of software engineering.\nA pure function then, in the FP sense, is a function which depends only on its input (and not on any other value stored elsewhere in external computer memory or other external source) and it affects nothing outside itself. Additionally, like the mathematical functions they try to emulate, pure functions have to output a value and that value must be unique for a given input.\nTo recap, here are the above properties again.\n\n\n\nA pure function must return a single output for a given input\nA pure function‚Äôs output should only depend on its input (in other words, no external hidden state should affect the output)\nA pure function shouldn‚Äôt change any external state (i.e.¬†an external chunk of computer memory)\n\n\n\nWorking with pure functions conveys some great benefits. For instance, properties 1 and 2 make pure functions interchangeable with their output value just as, say, \\(f(2)\\) given \\(f(x)=x^2\\) can, just as well, stand in for the number \\(4\\). This allows us to pass pure functions as arguments into other pure functions, and return pure functions from within other pure functions with entirely predictable results. By contrast, if our functions affected external state somewhere or produced other effects (more on these in the section on side effects) such as returning random and/or some other type of non-deterministic output for a given input, it would be a lot more difficult to reason about programs as chains of function calls. In fact, functional programs are compositions of pure functions written in a declarative (as opposed to an imperative) style.\n\n\n\n\nAn example is worth a thousand words. Since Python provides a good enough playground for showcasing both of these approaches, here is an example in Python.\nImperative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = []\nfor num in numbers:\n    squared.append(num ** 2)\n\nprint(squared)\n\n[1, 4, 9, 16, 25]\n\n\nDeclarative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = map(lambda x: x**2, numbers)\n\nprint(list(squared))\n\n[1, 4, 9, 16, 25]\n\n\nAt a very basic level, an imperative style can be likened to cooking at home with a cookbook. Imperative languages look more like a list of commands directed at the computer. Declarative writing, by contrast, can be compared to dining at a restaurant. We aren‚Äôt issuing commands at a grueling level of granularity (e.g.¬†iterating over an array manually, appending to a list manually, etc.). Instead, we‚Äôre specifying the desired outcome without the implementation details.\nIn the imperative style, we‚Äôre saying ‚Äústep through the list, read each item, square it and append it to a new list.‚Äù In the declarative style we‚Äôre simply saying ‚Äúsquare every element of the list.‚Äù Of course these differences are highly semantic and in real life projects contain a mix of both styles, so there‚Äôs no clear black and white separation.\n\n\n\nFunctions which violate any of these properties are said to produce side effects. The most common side effect is when a function modifies state (i.e.¬†a chunk of computer memory) outside itself (violating property 3). Examples of side effects include:\n\n\n\n\n\n\n\nEffect\nFunctional Programming Way\n\n\n\n\nA function directly modifying a variable defined in the global scope.\nThe FP approach is to pass the global variable as input instead, and have the function return a modified copy of the input.\n\n\nA function writing to an external database.\nThis is an example of an unavoidable side effect in practice. The FP approach is to mitigate. Specifics are language dependent, but usually the strategy involves gathering all such unavoidable side effects into one impure corner of the code, and keeping the rest of the code pure.\n\n\nA function like the built-in functions of printing to the console, retrieving system time, or a random number generator (or those functions which use them)\nYet more examples of unavoidable side effects. Such functions are inherently dependent on external or hidden state such as the time of day in the real world and, in general, things other than their input.\n\n\n\nAlthough some side effects are unavoidable, we should minimize their use in our code. Functional programming languages offer to do just that.\n\n\n\n\nIn functional programming, we also distinguish between mere instructions to the computer (also known as statements) and expressions. This distinction is similar to the distinction between functions in the traditional programming sense and pure functions in that expressions must also return a value. Contrast this requirement with instructions like the if/else statements and while loops which simply direct the control flow but don‚Äôt necessarily evaluate to anything.\nAs mentioned earlier, the use of instructions and impure functions is unavoidable at times. Different languages have different strategies of mitigating these impurities. Usually the aim is to gather the impurities together at the top in some clearly demarcated lexical block. Furthermore, some languages (such as Scala which is a blend of OOP and FP), go to great lengths to minimize side effects by enforcing the return requirement on instructions. All instructions in Scala evaluate to a value, effectively making them expressions.\nThe idea is to use a clever type system to capture impurities. If side effects must exist, they should be known to Scala. To achieve this, instructions in Scala return a type known as Unit which can hold only () as its value. So, in Scala, instructions are essentially treated as expressions which return this very specific type. We will see an example illustrating the power of this design choice in the next section.\nWe already touched on this briefly when we discussed declarative and imperative styles of writing code, but the distinction between instruction and expression further leads functional programming to favor certain programming styles over others.\n\n\nIn functional languages like Scala, if statements are implemented as expressions similar to the familiar ternary expressions in Python. Scala also has an if statement like the regular if statements of Python, but the expression is what‚Äôs preferred. Here are some examples to show the FP approach to writing if statements in both languages:\nPython:\nx = 1 if condition == True else 0\nScala:\nval x = if (condition) 1 else 0\nIn this example, x evaluates to a value: one of possible two. The if expression may still produce a side effect, but it‚Äôs not as open-ended as a normal if statement. In a normal if statement, the programmer might do something entirely crazy and unheard of such as accessing a database, or printing a line to the console (both considered side effects).\nThis brings us to an important point. It‚Äôs not that if statements would necessarily result in side effects, it‚Äôs that functional programming simply discourages the use of language constructs that lend themselves to producing side effects. Syntactic choices like this are a common theme in FP. For instance, Scala‚Äôs choice to treat () as a returnable value rather than just syntax is very deliberate. In Scala, for comprehensions are favored over for loops.\nScala For Comprehension:\nval myNewList = \n    for {\n        element &lt;- myOldList\n    } yield (element)\nIt‚Äôs important to note that this isn‚Äôt the best way to copy a list in Scala, it‚Äôs just a toy example of for comprehensions with the intent to illustrate two things about them:\n\nThe for comprehension is treated as an expression which returns a value captured by myNewList\nIf returning no value, we‚Äôd normally simply say yield (). However, because of Scala‚Äôs clever design, () actually is a value in Scala so even instructions, which normally wouldn‚Äôt return anything, do return something in Scala\n\n\n\n\nSimilarly, while loops (and to a lesser extent also for loops) are considered bad practice in functional programming because of their potential to produce side effects. Take, for example, a while loop that runs until a key press (or any other user input). Of course, this may be an unavoidable side effect in many cases. But then the FP approach would just be to contain this impurity somewhere with the rest of its kind.\nIn general, instead of iteration, recursion (or, more generally, function composition) is preferred. Of course, recursion is the best choice anyway if our data is represented in a recursive data structure (like a tree) or when the problem has some optimal substructure, but FP prefers this approach in general. This may sound unreasonable at first but a couple of familiar examples of function composition that‚Äôs have been adopted by popular languages like Python are the map and the filter functions. We already saw an example of map in the declarative code snippet above, so we won‚Äôt dive into its details here. Both mapand filter are examples of higher-order functions (HOFs) ‚Äì functions that either take other functions as input or output other functions. The map and filter functions show that function composition can be very readable and intuitive. Furthermore, neither map nor filter modify their input in-place but rather return a modified copy of it avoiding external state mutation which, as we know, is considered good practice in FP.\n\n\n\n\n\nIn terms of parallelization, both iterative and recursive solutions can be sequential processes which don‚Äôt lend themselves well to parallelization (or independent processes which do). However, FP still confers some benefit in terms of parallelization. Not because it favors recursion but because:\n\nA common challenge in parallel programming is to avoid mutating data while another thread is using it. Due to immutability in FP, this problem is eliminated\nFP avoids hidden state, so functions can be executed in parallel without the concern of synchronizing access to some shared state.\nFP making it easier to identify opportunities for parallelization (arguably).\nLanguages which are built around FP have powerful parallelization libraries that offer parallelized versions of common operations like map\n\n\n\n\n\n\n\nIf we‚Äôre going to favor the use of recursion (or, in general function composition) in FP over the more imperative style of writing iterative algorithms, we ought to tread carefully as to not cause stack overflow (which, as we know, is when the system runs out of working memory). Tail recursion optimization (similar to other techniques like memoization) helps us drastically cut the amount of stack memory used. It takes a constant amount of memory on the stack, instead of the linear, with input size, or worse. Read more about tail recursive optimization here."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "href": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "title": "Functional Programming",
    "section": "",
    "text": "Take \\(f: X \\rightarrow Y\\), which is a function that maps elements of set \\(X\\) to those of set \\(Y\\) such that for each \\(x\\) in set \\(X\\) (also denoted as ‚Äú\\(x \\in X\\)‚Äù), there‚Äôs one and only one \\(y \\in Y\\) which satisfies \\(f(x) = y\\). In ordinary language, we say that a mathematical function maps any given input to a unique output. That‚Äôs not to say that \\(f\\) can‚Äôt map the two different \\(x\\)‚Äôs to the same \\(y\\), but it can‚Äôt map the same \\(x\\) to more than one \\(y\\). Notice that the output in set \\(Y\\) depends only on the input from set \\(X\\), and that the function \\(f\\) only operates on set \\(X\\) and nothing external to it. In other words there is no hidden state (i.e.¬†some value outside \\(X\\)) that affects \\(f\\)-s output, so \\(f\\) always produces predictable output. Also, \\(f\\) doesn‚Äôt affect any element in \\(X\\) (or, for that matter, \\(Y\\)). The expression \\(f(x)\\) is simply understood as the function \\(f\\) applied to an element \\(x \\in X\\) which produces a value from the set \\(Y\\) (but, once again, it‚Äôs not like the specific element in the set \\(Y\\) is somehow retrieved or affected in any way)."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "href": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "title": "Functional Programming",
    "section": "",
    "text": "In functional programming, we loosely think of built-in types or abstract classes (i.e.¬†some chunks of computer memory) as being like sets. This lets us, somewhat awkwardly, pluck the mathematical definition of a function from the concrete world of mathematics and bring it into the practical world of software engineering.\nA pure function then, in the FP sense, is a function which depends only on its input (and not on any other value stored elsewhere in external computer memory or other external source) and it affects nothing outside itself. Additionally, like the mathematical functions they try to emulate, pure functions have to output a value and that value must be unique for a given input.\nTo recap, here are the above properties again.\n\n\n\nA pure function must return a single output for a given input\nA pure function‚Äôs output should only depend on its input (in other words, no external hidden state should affect the output)\nA pure function shouldn‚Äôt change any external state (i.e.¬†an external chunk of computer memory)\n\n\n\nWorking with pure functions conveys some great benefits. For instance, properties 1 and 2 make pure functions interchangeable with their output value just as, say, \\(f(2)\\) given \\(f(x)=x^2\\) can, just as well, stand in for the number \\(4\\). This allows us to pass pure functions as arguments into other pure functions, and return pure functions from within other pure functions with entirely predictable results. By contrast, if our functions affected external state somewhere or produced other effects (more on these in the section on side effects) such as returning random and/or some other type of non-deterministic output for a given input, it would be a lot more difficult to reason about programs as chains of function calls. In fact, functional programs are compositions of pure functions written in a declarative (as opposed to an imperative) style.\n\n\n\n\nAn example is worth a thousand words. Since Python provides a good enough playground for showcasing both of these approaches, here is an example in Python.\nImperative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = []\nfor num in numbers:\n    squared.append(num ** 2)\n\nprint(squared)\n\n[1, 4, 9, 16, 25]\n\n\nDeclarative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = map(lambda x: x**2, numbers)\n\nprint(list(squared))\n\n[1, 4, 9, 16, 25]\n\n\nAt a very basic level, an imperative style can be likened to cooking at home with a cookbook. Imperative languages look more like a list of commands directed at the computer. Declarative writing, by contrast, can be compared to dining at a restaurant. We aren‚Äôt issuing commands at a grueling level of granularity (e.g.¬†iterating over an array manually, appending to a list manually, etc.). Instead, we‚Äôre specifying the desired outcome without the implementation details.\nIn the imperative style, we‚Äôre saying ‚Äústep through the list, read each item, square it and append it to a new list.‚Äù In the declarative style we‚Äôre simply saying ‚Äúsquare every element of the list.‚Äù Of course these differences are highly semantic and in real life projects contain a mix of both styles, so there‚Äôs no clear black and white separation.\n\n\n\nFunctions which violate any of these properties are said to produce side effects. The most common side effect is when a function modifies state (i.e.¬†a chunk of computer memory) outside itself (violating property 3). Examples of side effects include:\n\n\n\n\n\n\n\nEffect\nFunctional Programming Way\n\n\n\n\nA function directly modifying a variable defined in the global scope.\nThe FP approach is to pass the global variable as input instead, and have the function return a modified copy of the input.\n\n\nA function writing to an external database.\nThis is an example of an unavoidable side effect in practice. The FP approach is to mitigate. Specifics are language dependent, but usually the strategy involves gathering all such unavoidable side effects into one impure corner of the code, and keeping the rest of the code pure.\n\n\nA function like the built-in functions of printing to the console, retrieving system time, or a random number generator (or those functions which use them)\nYet more examples of unavoidable side effects. Such functions are inherently dependent on external or hidden state such as the time of day in the real world and, in general, things other than their input.\n\n\n\nAlthough some side effects are unavoidable, we should minimize their use in our code. Functional programming languages offer to do just that."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#instructions-or-statements-vs-expressions",
    "href": "posts/functional_programming/functional_programming.html#instructions-or-statements-vs-expressions",
    "title": "Functional Programming",
    "section": "",
    "text": "In functional programming, we also distinguish between mere instructions to the computer (also known as statements) and expressions. This distinction is similar to the distinction between functions in the traditional programming sense and pure functions in that expressions must also return a value. Contrast this requirement with instructions like the if/else statements and while loops which simply direct the control flow but don‚Äôt necessarily evaluate to anything.\nAs mentioned earlier, the use of instructions and impure functions is unavoidable at times. Different languages have different strategies of mitigating these impurities. Usually the aim is to gather the impurities together at the top in some clearly demarcated lexical block. Furthermore, some languages (such as Scala which is a blend of OOP and FP), go to great lengths to minimize side effects by enforcing the return requirement on instructions. All instructions in Scala evaluate to a value, effectively making them expressions.\nThe idea is to use a clever type system to capture impurities. If side effects must exist, they should be known to Scala. To achieve this, instructions in Scala return a type known as Unit which can hold only () as its value. So, in Scala, instructions are essentially treated as expressions which return this very specific type. We will see an example illustrating the power of this design choice in the next section.\nWe already touched on this briefly when we discussed declarative and imperative styles of writing code, but the distinction between instruction and expression further leads functional programming to favor certain programming styles over others.\n\n\nIn functional languages like Scala, if statements are implemented as expressions similar to the familiar ternary expressions in Python. Scala also has an if statement like the regular if statements of Python, but the expression is what‚Äôs preferred. Here are some examples to show the FP approach to writing if statements in both languages:\nPython:\nx = 1 if condition == True else 0\nScala:\nval x = if (condition) 1 else 0\nIn this example, x evaluates to a value: one of possible two. The if expression may still produce a side effect, but it‚Äôs not as open-ended as a normal if statement. In a normal if statement, the programmer might do something entirely crazy and unheard of such as accessing a database, or printing a line to the console (both considered side effects).\nThis brings us to an important point. It‚Äôs not that if statements would necessarily result in side effects, it‚Äôs that functional programming simply discourages the use of language constructs that lend themselves to producing side effects. Syntactic choices like this are a common theme in FP. For instance, Scala‚Äôs choice to treat () as a returnable value rather than just syntax is very deliberate. In Scala, for comprehensions are favored over for loops.\nScala For Comprehension:\nval myNewList = \n    for {\n        element &lt;- myOldList\n    } yield (element)\nIt‚Äôs important to note that this isn‚Äôt the best way to copy a list in Scala, it‚Äôs just a toy example of for comprehensions with the intent to illustrate two things about them:\n\nThe for comprehension is treated as an expression which returns a value captured by myNewList\nIf returning no value, we‚Äôd normally simply say yield (). However, because of Scala‚Äôs clever design, () actually is a value in Scala so even instructions, which normally wouldn‚Äôt return anything, do return something in Scala\n\n\n\n\nSimilarly, while loops (and to a lesser extent also for loops) are considered bad practice in functional programming because of their potential to produce side effects. Take, for example, a while loop that runs until a key press (or any other user input). Of course, this may be an unavoidable side effect in many cases. But then the FP approach would just be to contain this impurity somewhere with the rest of its kind.\nIn general, instead of iteration, recursion (or, more generally, function composition) is preferred. Of course, recursion is the best choice anyway if our data is represented in a recursive data structure (like a tree) or when the problem has some optimal substructure, but FP prefers this approach in general. This may sound unreasonable at first but a couple of familiar examples of function composition that‚Äôs have been adopted by popular languages like Python are the map and the filter functions. We already saw an example of map in the declarative code snippet above, so we won‚Äôt dive into its details here. Both mapand filter are examples of higher-order functions (HOFs) ‚Äì functions that either take other functions as input or output other functions. The map and filter functions show that function composition can be very readable and intuitive. Furthermore, neither map nor filter modify their input in-place but rather return a modified copy of it avoiding external state mutation which, as we know, is considered good practice in FP.\n\n\n\n\n\nIn terms of parallelization, both iterative and recursive solutions can be sequential processes which don‚Äôt lend themselves well to parallelization (or independent processes which do). However, FP still confers some benefit in terms of parallelization. Not because it favors recursion but because:\n\nA common challenge in parallel programming is to avoid mutating data while another thread is using it. Due to immutability in FP, this problem is eliminated\nFP avoids hidden state, so functions can be executed in parallel without the concern of synchronizing access to some shared state.\nFP making it easier to identify opportunities for parallelization (arguably).\nLanguages which are built around FP have powerful parallelization libraries that offer parallelized versions of common operations like map\n\n\n\n\n\n\n\nIf we‚Äôre going to favor the use of recursion (or, in general function composition) in FP over the more imperative style of writing iterative algorithms, we ought to tread carefully as to not cause stack overflow (which, as we know, is when the system runs out of working memory). Tail recursion optimization (similar to other techniques like memoization) helps us drastically cut the amount of stack memory used. It takes a constant amount of memory on the stack, instead of the linear, with input size, or worse. Read more about tail recursive optimization here."
  },
  {
    "objectID": "posts/general_computer_science/recursion_optimizations.html",
    "href": "posts/general_computer_science/recursion_optimizations.html",
    "title": "Recursion Optimizations",
    "section": "",
    "text": "Recursion Optimizations\nRecursive algorithms, while elegant and expressive, can sometimes lead to performance issues due to the overhead of function calls and potential for repeated computations. Several optimization techniques can be employed to enhance their efficiency. One common method is memoization, which stores the results of expensive function calls and reuses them when the same inputs occur again, avoiding repeated computations. Another technique is tail recursion optimization, where the recursive call is the final operation in the function, allowing the system to reuse the current stack frame for each recursive call. Tail recursion optimization reduces the space complexity from \\(O(n)\\) to \\(O(1)\\). Additionally, iterative solutions can often be more efficient than their recursive counterparts, so converting a recursive algorithm to an iterative one can be considered an optimization. Understanding these techniques, and variations on them, can greatly improve the performance of our recursive algorithms.\n\nTail Recursion - Avoiding Stack Overflow\nStack overflow (which is when the system runs out of short term memory) is a common concern when working with recursive functions or when doing functional programming, where function composition is the mode in which we think. Tail recursion optimization helps us drastically cut the number of stack frames. It makes our recursive algorithms take a constant amount of space, \\(O(1)\\), instead of a linear amount, $O(n) where \\(n\\) is the input size, or worse.\n\nClassic Example: Factorial\nLet‚Äôs take the classic example of calculating a factorial.\nNaive Recursive Implementation:\n\ndef factorial(n):\n    if n == 0: # Base case: 0! = 1\n        return 1\n    else: \n        return n * factorial(n-1) # Recursive step\n\nfactorial(4)\n\n24\n\n\nFrom the below untangled definition of factorial (for \\(n=4\\)) we can surmise what goes on in the stack. The stack first fills up with stack frames for factorial(n) down to factorial(0), which is the last frame on the stack before it begins to pop and actual evaluation happens.\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = 4 * factorial(3) \\\\\n        & = 4 * (3 * factorial(2)) \\\\\n        & = 4 * (3 * (2 * factorial(1))) \\\\\n        & = 4 * (3 * (2 * (1 * factorial(0)))) \\\\\n        & = 4 * (3 * (2 * (1 * 1))) \\\\\n        & = 4 * (3 * (2 * 1)) \\\\\n        & = 4 * (3 * 2) \\\\\n        & = 4 * 6 \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nAs we can see the function is called for \\(n = 4\\) down to the base case of \\(n = 0\\) (each call stacking up in memory) before evaluation even begins. Evaluation then happens step-by-step inside each stack frame until all of them have popped.\nIt‚Äôs not immediately clear how to make the calls independent of each other given that there is a multiplicative factor in front of the recursive call (which is what makes this particular function fail to be tail-recursive). It helps to think in terms of carried state. In this case the idea is simple, if we can carry the state of the current stack frame into the next one as input, then we can pop each frame right after it calls the next frame. Why? Because at that point, having carried its state into the next frame, the current frame exhausts its usefulness.\nIn the case of the factorial function above, this means that in the tail-recursive implementation the stack is not filled up with as many frames of recursive factorial calls as the input (\\(n\\)) is big. There are still \\(n\\) total calls, however the memory used in the stack is held constant (at a single frame in this case) as each old frame gives way to the new one.\nSo, for now, let‚Äôs define a magic function called go(n,acc) with inputs n and what‚Äôs called an accumulator acc such that factorial(n) := go(n,1). We take this to be by construction. The function go will be the tail-recursive helper of factorial. The accumulator acc, which is initialized to 1, will be used to remember the state inside the current stack frame (in this case just the multiplicative factor before the recursive call).\nBut so far we‚Äôve only given go(n, acc) its desired properties without actually defining it. The following is the tail-recursive version of factorial which includes the definition of go.\nTail-Recursive Factorial:\n\ndef factorial(n):\n    def go(n,acc): # Helper function with an accumulator\n        if n == 0: # Base case: 0! = 1\n            return acc\n        else:\n            return go(n-1, n * acc) # Tail-recusrive step\n        \n    return go(n,1) # Delegate the problem solution to a helper function\n\nfactorial(4)\n\n24\n\n\nLet‚Äôs unpack this:\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = go(4,1) \\\\\n        & = go(3,4) \\\\\n        & = go(2,12) \\\\\n        & = go(1,24) \\\\\n        & = go(0,24) \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nRight away we can see that, with this approach, we can pop the previous stack frame at any time without losing any information it holds because all state is carried over from the previous frame into the current one by the accumulator and, finally, returned at the end. A visual cue of this fact is that in the expression above evaluation happens immediately, rather than step-by-step (with each step corresponding to the popping of a stack frame), as is the case in the naive implementation.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIt‚Äôs important to note that this effort only pays off if the language compiler in question supports TCO (Tail Call Optimization). Most, in fact, do. If the language supports TCO the compiler can recognize tail calls and simply pop the current stack frame after the recursive call, replacing it with the subsequent call (rather than blindly stacking frames on top of each other as in the naive recursive algorithm)"
  },
  {
    "objectID": "posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "href": "posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "title": "SSL - The Internet‚Äôs Trust Protocol",
    "section": "",
    "text": "Secure Sockets Layer (SSL) and its successor Transport Layer Security (TLS) are cryptographic protocols that provide security in communication over a computer network using a combination of symmetric and asymmetric encryption (both of which are introduced later in this post). The protocol is widely used for such applications as HTTPS (HTTP protocol extended with TLS encryption), email, instant messaging, etc.\n\n\n\n\n\n\n\n(a) Private and public keys\n\n\nFigure¬†1: Each party has a set of public and private keys.\n\n\n\n\n\n\n\n\nSSL figure 1\n\n\n\n\n\n\n\n\nSSL figure 3\n\n\n\n\n\n\n\n\nSSL figure w"
  },
  {
    "objectID": "posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "href": "posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "title": "SSL - The Internet‚Äôs Trust Protocol",
    "section": "",
    "text": "(a) Private and public keys\n\n\nFigure¬†1: Each party has a set of public and private keys."
  },
  {
    "objectID": "posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "href": "posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "title": "SSL - The Internet‚Äôs Trust Protocol",
    "section": "",
    "text": "SSL figure 1"
  },
  {
    "objectID": "posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "href": "posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "title": "SSL - The Internet‚Äôs Trust Protocol",
    "section": "",
    "text": "SSL figure 3"
  },
  {
    "objectID": "posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "href": "posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "title": "SSL - The Internet‚Äôs Trust Protocol",
    "section": "",
    "text": "SSL figure w"
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html",
    "href": "posts/leetcode/lc11_container_with_most_water.html",
    "title": "LC11 - Container with Most Water",
    "section": "",
    "text": "We are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the i-th line are (i, 0) and (i, height[i]).\nThe objective is to find two lines which, together with the x-axis, form a container which can hold the most water.\nWe should return the maximum amount of water this container can store.\nExample\n\n\n\nContainer with most water\n\n\nInput: height = [1,8,6,2,5,4,8,3,7]\nOutput: 49\nExplanation\nThe above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of the water (highlighted section) the container can contain is 49 (in units of area).\n\n\n\n\n\nThe brute force solution to this problem consists of checking each pair of vertical lines (or each subarray of size \\(\\geq 2)\\). Since order of any given pair does not matter, this solution has time complexity \\({O \\left ({n \\choose 2} \\right )= O \\left (\\frac{n(n-1)}{2} \\right ) = O(n^2)}\\) where \\(n\\) is the length of the height array.\n\n\n\nThe non-brute-force solution, i.e.¬†the Two Pointer Solution, follows trivially after discovering an optimal substructure of the problem. The optimal substructure allows us to remove certain pairs \\((i,j)\\) from consideration, thereby reducing the number of pairs to be considered.The difficulty lies in noticing and proving the optimal substructure.\n\n\n\nLet \\(h(i)\\) denote the height of the \\(i\\)-th vertical line.\nLet \\(a(i,j)\\) denote the area of the container formed by the pair of vertical lines \\((i,j)\\).\nLet \\(maxArea(i,j)\\) denote the maximum area formed by the lines \\({i,...,j}\\) ‚Äì that is the output of the procedure on the subarray height[i:j].\n\nSuppose, without loss of generality, \\(h(1) \\leq h(n)\\). Then, the problem has top-down optimal substructure:\n \\[maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\] \n\n\n\n\n\n\nNote\n\n\n\n\n\nThe optimal substructure also offers an obvious DP solution to the problem. We simply consider the sub-problems of sizes \\(n-1\\), \\(n-2\\), and so on. However, in order to minimize the complexity, we offer a tabulation approach instead ‚Äî the Two Pointer Solution.\n\n\n\n\n\n\nFor the initial pair \\((1,n)\\) where \\(h(1) \\leq h(n)\\) we have \\(a(1,n) &gt; a(1,k) \\ \\ \\forall k\\). This is because we‚Äôre starting out from the widest container formed by \\({(1,n)}\\) and considering containers of decreasing width formed by the pairs \\({(1, n-1), (1, n-2), ..., (1,2)}\\).\nIn case \\({h(k) &gt; h(1)}\\) for some \\({n \\geq k &gt; 1}\\) the area of the container formed by \\({(1,k)}\\) is still determined by \\({h(1)}\\), except now it‚Äôs less wide. Whereas if \\({h(k) &lt; h(1)}\\) the area of the container decreases not only in width but also in height.\nIn both cases we have \\({a(1,n) &gt; a(1,k)}\\) which means in general \\({a(1,n) &gt; a(1, k) \\ \\ \\forall k}\\).\nTherefore, we may omit the first vertical line from consideration and consider the subproblem on the indices \\({2,...,n}\\). The overall optimal solution will then be \\(maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\) as was the claim.\n\n\n\nAt this point, the Two Pointers Algorithm is trivial to come up with:\n\nInitialize two pointers, ‚Äòleft‚Äô and ‚Äòright‚Äô, at the first and last index respectively.\nWhile the pointers do not intersect:\n\nCalculate the area of the container formed by the pointers and determine if it‚Äôs the maximum area encountered so far\nKeep the position of the pointer of the vertical line that‚Äôs longer fixed\nAdvance the pointer of vertical line that‚Äôs shorter towards the fixed pointer\n\n\n\n\n\ndef maxArea(height) -&gt; int:\n    i, j = 0, len(height) - 1\n    water = 0\n    while i &lt; j:\n        water = max(water, (j - i) * min(height[i], height[j]))\n        if height[i] &lt; height[j]:\n            i += 1\n        else:\n            j -= 1\n    return water\n\n# Example input\nheights = [1,8,6,2,5,4,8,3,7]\nprint(f'The container with most water has area: {maxArea(heights)}')\n\nThe container with most water has area: 49"
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#problem-statement",
    "href": "posts/leetcode/lc11_container_with_most_water.html#problem-statement",
    "title": "LC11 - Container with Most Water",
    "section": "",
    "text": "We are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the i-th line are (i, 0) and (i, height[i]).\nThe objective is to find two lines which, together with the x-axis, form a container which can hold the most water.\nWe should return the maximum amount of water this container can store.\nExample\n\n\n\nContainer with most water\n\n\nInput: height = [1,8,6,2,5,4,8,3,7]\nOutput: 49\nExplanation\nThe above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of the water (highlighted section) the container can contain is 49 (in units of area)."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#foreword",
    "href": "posts/leetcode/lc11_container_with_most_water.html#foreword",
    "title": "LC11 - Container with Most Water",
    "section": "",
    "text": "The brute force solution to this problem consists of checking each pair of vertical lines (or each subarray of size \\(\\geq 2)\\). Since order of any given pair does not matter, this solution has time complexity \\({O \\left ({n \\choose 2} \\right )= O \\left (\\frac{n(n-1)}{2} \\right ) = O(n^2)}\\) where \\(n\\) is the length of the height array.\n\n\n\nThe non-brute-force solution, i.e.¬†the Two Pointer Solution, follows trivially after discovering an optimal substructure of the problem. The optimal substructure allows us to remove certain pairs \\((i,j)\\) from consideration, thereby reducing the number of pairs to be considered.The difficulty lies in noticing and proving the optimal substructure.\n\n\n\nLet \\(h(i)\\) denote the height of the \\(i\\)-th vertical line.\nLet \\(a(i,j)\\) denote the area of the container formed by the pair of vertical lines \\((i,j)\\).\nLet \\(maxArea(i,j)\\) denote the maximum area formed by the lines \\({i,...,j}\\) ‚Äì that is the output of the procedure on the subarray height[i:j].\n\nSuppose, without loss of generality, \\(h(1) \\leq h(n)\\). Then, the problem has top-down optimal substructure:\n \\[maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\] \n\n\n\n\n\n\nNote\n\n\n\n\n\nThe optimal substructure also offers an obvious DP solution to the problem. We simply consider the sub-problems of sizes \\(n-1\\), \\(n-2\\), and so on. However, in order to minimize the complexity, we offer a tabulation approach instead ‚Äî the Two Pointer Solution.\n\n\n\n\n\n\nFor the initial pair \\((1,n)\\) where \\(h(1) \\leq h(n)\\) we have \\(a(1,n) &gt; a(1,k) \\ \\ \\forall k\\). This is because we‚Äôre starting out from the widest container formed by \\({(1,n)}\\) and considering containers of decreasing width formed by the pairs \\({(1, n-1), (1, n-2), ..., (1,2)}\\).\nIn case \\({h(k) &gt; h(1)}\\) for some \\({n \\geq k &gt; 1}\\) the area of the container formed by \\({(1,k)}\\) is still determined by \\({h(1)}\\), except now it‚Äôs less wide. Whereas if \\({h(k) &lt; h(1)}\\) the area of the container decreases not only in width but also in height.\nIn both cases we have \\({a(1,n) &gt; a(1,k)}\\) which means in general \\({a(1,n) &gt; a(1, k) \\ \\ \\forall k}\\).\nTherefore, we may omit the first vertical line from consideration and consider the subproblem on the indices \\({2,...,n}\\). The overall optimal solution will then be \\(maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\) as was the claim.\n\n\n\nAt this point, the Two Pointers Algorithm is trivial to come up with:\n\nInitialize two pointers, ‚Äòleft‚Äô and ‚Äòright‚Äô, at the first and last index respectively.\nWhile the pointers do not intersect:\n\nCalculate the area of the container formed by the pointers and determine if it‚Äôs the maximum area encountered so far\nKeep the position of the pointer of the vertical line that‚Äôs longer fixed\nAdvance the pointer of vertical line that‚Äôs shorter towards the fixed pointer\n\n\n\n\n\ndef maxArea(height) -&gt; int:\n    i, j = 0, len(height) - 1\n    water = 0\n    while i &lt; j:\n        water = max(water, (j - i) * min(height[i], height[j]))\n        if height[i] &lt; height[j]:\n            i += 1\n        else:\n            j -= 1\n    return water\n\n# Example input\nheights = [1,8,6,2,5,4,8,3,7]\nprint(f'The container with most water has area: {maxArea(heights)}')\n\nThe container with most water has area: 49"
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html",
    "href": "posts/optimization/introduction_to_optimization.html",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "An optimization problem can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.¬†minimize or maximize) some objective function. The objective function can be almost anything ‚Äî cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are basically the same problem. Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\), the reflected objective function.\n\n\nThis post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, let‚Äôs define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems ‚Äî problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "First, let‚Äôs define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems ‚Äî problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html",
    "title": "Linear Algebra Refresher for Optimization",
    "section": "",
    "text": "Let‚Äôs start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that‚Äôs commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: ¬† A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\nFigure¬†1: Set A is convex, set B is non-convex\n\n\n\n\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\nConvex hull\n\n\nFigure¬†2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis‚Äôs algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there‚Äôs an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it‚Äôs the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(‚Ä†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(‚Ä†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\nConvex hull of a set\n\n\nFigure¬†3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points ‚Äî it‚Äôs simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)‚Äôs need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it‚Äôs the line that passes through them, and for three points it‚Äôs the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)‚Äôs totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere‚Äôs a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that‚Äôs been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we‚Äôll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)‚Äôs positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it‚Äôs a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#convexity",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#convexity",
    "title": "Linear Algebra Refresher for Optimization",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: ¬† A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\nFigure¬†1: Set A is convex, set B is non-convex\n\n\n\n\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#examples-of-convex-sets",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#examples-of-convex-sets",
    "title": "Linear Algebra Refresher for Optimization",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\nConvex hull\n\n\nFigure¬†2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis‚Äôs algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there‚Äôs an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it‚Äôs the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(‚Ä†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(‚Ä†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\nConvex hull of a set\n\n\nFigure¬†3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points ‚Äî it‚Äôs simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)‚Äôs need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it‚Äôs the line that passes through them, and for three points it‚Äôs the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)‚Äôs totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere‚Äôs a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that‚Äôs been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we‚Äôll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)‚Äôs positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it‚Äôs a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Hi, I‚Äôm Vahram\nSoftware engineer and graduate student of computer science with a passion for machine learning and the applied maths/sciences in general.\n\nüìô About this blog\nThis blog is where I write about the subjects that interest me. It‚Äôs where I collect my notes and ideas on the web. I started it in late 2021 from some notes I took in Jupyter Notebooks.\nThe content will be centered around machine learning, however I will expand into other subject areas as well. In the spirit of AI the thumbnails and other images featured in this blog are generated using DALL-E. The content, however, will not be AI generated.\n\n\nüéì Education\nUniversity of Texas at Austin | M.S in Computer Science (In progress)\nUniversity of California, Los Angeles | B.S. in Applied Mathematics (2020)\n\n\nüíª Experience\nCapital One | Software Engineer (Feb 2023 - Present) | Software Intern (Jun 2022 - Aug 2022)\nOmron Automation | Software Intern (Aug 2015 - Feb 2017)"
  },
  {
    "objectID": "unpublished_posts/network_and_security/contract_testing.html",
    "href": "unpublished_posts/network_and_security/contract_testing.html",
    "title": "Contract Testing",
    "section": "",
    "text": "Code\n# Make a binary tree data structure\n# Make a binary tree data structure\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self, value):\n        self.root = Node(value)\n\n    def insert(self, value):\n        self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if value &lt; node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(node.left, value)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(node.right, value)\n\n    def search(self, value):\n        return self._search(self.root, value)\n\n    def _search(self, node, value):\n        if node is None:\n            return False\n        if node.value == value:\n            return True\n        if value &lt; node.value:\n            return self._search(node.left, value)\n        else:\n            return self._search(node.right, value)\n\n    def delete(self, value):\n        self._delete(self.root, value)\n\n    def _delete(self, node, value):\n        if node is None:\n            return node\n        if value &lt; node.value:\n            node.left = self._delete(node.left, value)\n        elif value &gt; node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._minValue(node.right)\n            node.right = self._delete(node.right, node.value)\n        return node\n\n    def _minValue(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n\n    def inorder(self):\n        self._inorder(self.root)\n\n    def _inorder(self, node):\n        if node is not None:\n            self._inorder(node.left)\n            print(node.value, end=\" \")\n            self._inorder(node.right)\n\n    def preorder(self):\n        self._preorder(self.root)\n\n    def _preorder(self, node):\n        if node is not None:\n            print(node.value, end=\" \")\n            self._preorder(node.left)\n            self._preorder(node.right)\n\n    def postorder(self):\n        self._postorder(self.root)\n\n    def _postorder(self, node):\n        if node is not None:\n            self._postorder(node.left)\n            self._postorder(node.right)\n            print(node.value, end=\" \")"
  },
  {
    "objectID": "unpublished_posts/game_development/unreal_engine_first_steps.html",
    "href": "unpublished_posts/game_development/unreal_engine_first_steps.html",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local\n\n\n\n\nRun the following commands"
  },
  {
    "objectID": "unpublished_posts/game_development/unreal_engine_first_steps.html#steps",
    "href": "unpublished_posts/game_development/unreal_engine_first_steps.html#steps",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local"
  },
  {
    "objectID": "unpublished_posts/game_development/unreal_engine_first_steps.html#building-the-engine",
    "href": "unpublished_posts/game_development/unreal_engine_first_steps.html#building-the-engine",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "Run the following commands"
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "Perhaps the most important package for scientific computing included with Conda is NumPy. Let‚Äôs get a feel for what NumPy offers.\n\n\n\n\nA 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices.\n\n\n\n\nNumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e.¬†its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don‚Äôt have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we‚Äôre working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let‚Äôs examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one‚Äôs shape is (3,1).\n\n\n\n\n\n\nüìñ Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can‚Äôt include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "A 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices."
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "NumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e.¬†its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don‚Äôt have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we‚Äôre working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let‚Äôs examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one‚Äôs shape is (3,1).\n\n\n\n\n\n\nüìñ Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can‚Äôt include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/python/ny_housing_market_analysis.html",
    "href": "unpublished_posts/python/ny_housing_market_analysis.html",
    "title": "Vahram's blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\nfile_path = \"C://Users/vapog/Downloads/airbnb_data.csv\"\n\n# Examining the dataframe\ndf = pd.read_csv(file_path)\nprint(df.head(10))\n\n     id                                              name  host_id  \\\n0  2539                Clean & quiet apt home by the park     2787   \n1  2595                             Skylit Midtown Castle     2845   \n2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n3  3831                   Cozy Entire Floor of Brownstone     4869   \n4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n5  5099         Large Cozy 1 BR Apartment In Midtown East     7322   \n6  5121                                   BlissArtsSpace!     7356   \n7  5178                  Large Furnished Room Near B'way      8967   \n8  5203                Cozy Clean Guest Room - Family Apt     7490   \n9  5238                Cute & Cozy Lower East Side 1 bdrm     7549   \n\n     host_name neighbourhood_group       neighbourhood  latitude  longitude  \\\n0         John            Brooklyn          Kensington  40.64749  -73.97237   \n1     Jennifer           Manhattan             Midtown  40.75362  -73.98377   \n2    Elisabeth           Manhattan              Harlem  40.80902  -73.94190   \n3  LisaRoxanne            Brooklyn        Clinton Hill  40.68514  -73.95976   \n4        Laura           Manhattan         East Harlem  40.79851  -73.94399   \n5        Chris           Manhattan         Murray Hill  40.74767  -73.97500   \n6        Garon            Brooklyn  Bedford-Stuyvesant  40.68688  -73.95596   \n7     Shunichi           Manhattan      Hell's Kitchen  40.76489  -73.98493   \n8    MaryEllen           Manhattan     Upper West Side  40.80178  -73.96723   \n9          Ben           Manhattan           Chinatown  40.71344  -73.99037   \n\n         room_type  price  minimum_nights  number_of_reviews last_review  \\\n0     Private room    149               1                  9  2018-10-19   \n1  Entire home/apt    225               1                 45  2019-05-21   \n2     Private room    150               3                  0         NaN   \n3  Entire home/apt     89               1                270  2019-07-05   \n4  Entire home/apt     80              10                  9  2018-11-19   \n5  Entire home/apt    200               3                 74  2019-06-22   \n6     Private room     60              45                 49  2017-10-05   \n7     Private room     79               2                430  2019-06-24   \n8     Private room     79               2                118  2017-07-21   \n9  Entire home/apt    150               1                160  2019-06-09   \n\n   reviews_per_month  calculated_host_listings_count  availability_365  \n0               0.21                               6               365  \n1               0.38                               2               355  \n2                NaN                               1               365  \n3               4.64                               1               194  \n4               0.10                               1                 0  \n5               0.59                               1               129  \n6               0.40                               1                 0  \n7               3.47                               1               220  \n8               0.99                               1                 0  \n9               1.33                               4               188  \n\n\n\n# Number of rentals by New York borough\nbronx_df = df[df['neighbourhood_group'] == 'Bronx']\nprint(\"Bronx: # of rentals \",bronx_df.shape[0])\nbrooklyn_df = df[df['neighbourhood_group'] == 'Brooklyn']\nprint(\"Brooklyn: # of rentals \",brooklyn_df.shape[0])\nmanhattan_df = df[df['neighbourhood_group'] == 'Manhattan']\nprint(\"Manhattan: # of rentals \",manhattan_df.shape[0])\nstaten_island_df = df[df['neighbourhood_group'] == 'Staten Island']\nprint(\"Staten Island: # of rentals \",staten_island_df.shape[0])\n\nBronx: # of rentals  1091\nBrooklyn: # of rentals  20104\nManhattan: # of rentals  21661\nStaten Island: # of rentals  373\n\n\n\n# Most popular neighborhood by number of reviews\nsorted_df = df.sort_values(by='number_of_reviews', ascending=False)\nlocations_and_num_reviews_df = sorted_df[['neighbourhood','number_of_reviews']]\ntop_20_df = locations_and_num_reviews_df.head(20)\n\n# Group by neighborhood and take aggregate mean\ngrouped_df = top_20_df.groupby('neighbourhood')['number_of_reviews'].mean()\nprint(grouped_df)\n\nneighbourhood\nAstoria            441.0\nBushwick           480.0\nEast Elmhurst      485.2\nEast Village       451.0\nFlushing           474.0\nHarlem             564.0\nJamaica            553.0\nLower East Side    540.0\nPark Slope         488.0\nSouth Slope        467.0\nTribeca            447.0\nName: number_of_reviews, dtype: float64\n\n\n\n# Filtering for a client by price in Manhattan's Upper East Side\nupper_east_df = df[df['neighbourhood'] == 'Upper East Side']\nninetieth_percentile = np.quantile(upper_east_df['number_of_reviews'], 0.85) # Normalizes the results by getting rid of bad postings\nupper_east_df = upper_east_df[upper_east_df['number_of_reviews'] &gt;= ninetieth_percentile]\n\n\n## Used to examine the unique room types available\nprint(df['room_type'].unique())\n\n## Cheapest private room type rental in Manhattan's Upper East Side\nprivate_rooms_df = upper_east_df[upper_east_df['room_type'] == 'Private room']\nprivate_rooms_df = private_rooms_df.sort_values('price', ascending=True)\nprivate_rooms_df = private_rooms_df[['neighbourhood','room_type','price']]\nprint(private_rooms_df.head(5))\n\n## Cheapest entire homes/appartments\nentire_homes_df = upper_east_df[upper_east_df['room_type'] == 'Entire home/apt']\nentire_homes_df = entire_homes_df.sort_values('price', ascending=True)\nentire_homes_df = entire_homes_df[['neighbourhood','room_type','price']]\nprint(entire_homes_df.head(5))\n\n['Private room' 'Entire home/apt' 'Shared room']\n         neighbourhood     room_type  price\n8416   Upper East Side  Private room     49\n40185  Upper East Side  Private room     50\n35976  Upper East Side  Private room     60\n21283  Upper East Side  Private room     65\n19830  Upper East Side  Private room     65\n         neighbourhood        room_type  price\n18882  Upper East Side  Entire home/apt     69\n7181   Upper East Side  Entire home/apt     75\n5759   Upper East Side  Entire home/apt     92\n22122  Upper East Side  Entire home/apt     95\n27040  Upper East Side  Entire home/apt     95"
  },
  {
    "objectID": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html",
    "href": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html",
    "title": "Build Jarvis - Your Own AI Assistant",
    "section": "",
    "text": "To one day write this article. For now, here‚Äôs the CoPilot-generated steps."
  },
  {
    "objectID": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#prerequisites",
    "href": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#prerequisites",
    "title": "Build Jarvis - Your Own AI Assistant",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic understanding of Python programming\nFamiliarity with Machine Learning concepts\nAn account on Hugging Face‚Äôs Model Hub\nYour notes hosted on GitHub or served as static pages"
  },
  {
    "objectID": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#steps",
    "href": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#steps",
    "title": "Build Jarvis - Your Own AI Assistant",
    "section": "Steps",
    "text": "Steps\n\n1. Setup Your Environment\nFirst, you need to set up your Python environment. This involves installing necessary libraries and tools, including Hugging Face‚Äôs Transformers library.\npip install transformers\n\n\n2. Prepare Your Training Data\nYour training data will be your local notes. If they‚Äôre hosted on GitHub, you can clone the repository to your local machine. If they‚Äôre served as static pages, you can use a web scraper to download the notes.\n\n\n3. Preprocess Your Data\nBefore training the model, you need to preprocess your data. This involves tokenizing the text and formatting it in a way that the model can understand.\n\n\n4. Train the Model\nNow you‚Äôre ready to train the model. You can use Hugging Face‚Äôs Trainer class to do this. Here‚Äôs a basic example:\n\n\nCode\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset\n)\n\ntrainer.train()\n\n\n\n\n5. Upload the Model to Hugging Face\nAfter training the model, you can upload it to Hugging Face‚Äôs Model Hub. This allows others to use your model for their own tasks.\n\n\nCode\nmodel.push_to_hub(\"my-model\")"
  },
  {
    "objectID": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#conclusion",
    "href": "unpublished_posts/machine_learning/build_jarvis_your_own_ai_assistant.html#conclusion",
    "title": "Build Jarvis - Your Own AI Assistant",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs it! You‚Äôve now trained an open source LLM with your local notes and uploaded it to Hugging Face. Happy training!\n```\nPlease note that this is a high-level guide and the actual code will depend on your specific use case and the format of your notes."
  }
]
[
  {
    "objectID": "unpublished_posts/parallel_systems/asynchronous_programming.html",
    "href": "unpublished_posts/parallel_systems/asynchronous_programming.html",
    "title": "Parallel Programming",
    "section": "",
    "text": "CPU Threads\nIt all begins with CPU threads….\n\n\nParallelism vs Concurrency vs Asynchronicity\nAsynchronicity is about processes that do not overalap in time. When it comes to concurrency and parallelism, the distinction is subtle. Quoting Sun’s Multithreaded Programming Guide:\n\nConcurrency: A condition that exists when at least two threads are making progress. A more generalized form of parallelism that can include time-slicing as a form of virtual parallelism.\nParallelism: A condition that arises when at least two threads are executing simultaneously.\n\nSo it appears concurrency is more about processes that may overlap somewhat in time, whereas parallelism is about processes that literally occupy the same instance/interval in time.\nAll of these use similar or related constructs to help them achieve this. The use of callbacks, promises, or other constructs is common in handle=ng the execution of asynchronous code (such as file I/O, network requests, or user input). Instead of waiting, it’s about not blocking the execution of the main program’s thread which allows the program to move on to other tasks. When the long-running code is finished, the program is notified and can move on to other tasks. It’s all about multi-threaded architecture, and bringing it all together at the end to notify the calling thread of the asynchronous code’s completion, failure, or progress.\nThese programming paradigms can make our application more responsive and improve its speed, but they can also make the code more complex and harder to debug, as we now have to deal with issues like synchronization and the management of shared resources. One way to mitigate these problems is functional programming which you can read more about here.\n\n\n\n\n\n\n\nMetric\nUseful Plumbing Analogy\n\n\n\n\nLatency\nThe amount of time it takes to travel through a tube\n\n\nBandwidth\nHow wide the tube is\n\n\nThroughput\nThe rate of water flow"
  },
  {
    "objectID": "unpublished_posts/networking/self-hosting.html",
    "href": "unpublished_posts/networking/self-hosting.html",
    "title": "Self Hosting",
    "section": "",
    "text": "Useful Mac utilities:\n\nOh My Zsh\nHomebrew\nVim\n[Docker]\n\n\n\nAfter installing oh my zsh, we can set environment variables in ~/.zshrc using export. We can even set environment variables that affect the Z shell’s appearance. For example, adding the following in the initialization file ~/.zshrc displays the localhost, current working directory, and the prompt character (root vs user) in the shell prompt.\n# Define color codes\nRED=\"%F{red}\"\nGREEN=\"%F{green}\"\nYELLOW=\"%F{yellow}\"\nBLUE=\"%F{blue}\"\nMAGENTA=\"%F{magenta}\"\nCYAN=\"%F{cyan}\"\nWHITE=\"%F{white}\"\nRESET=\"%f\"\n\n# Set the prompt with colors\nPROMPT='${CYAN}%n@%m ${YELLOW}%1~ ${RESET}%# '\nA shell also knows of ‘hostname’ from a system command that’s part of the OS.\necho \"$(hostname)\"\nSome other variables the shell knows of include:\necho \"$USER\" # The name of the logged-in user.\necho \"$HOME\" # The path to the current user's home directory.\necho \"$PATH\" # The list of directories that the shell searches for executable files.\nThese are set by the login process, rather than being exposed by OS commands like hostname or being defined by the user in an initialization files like ~/.zshrc."
  },
  {
    "objectID": "unpublished_posts/networking/self-hosting.html#whats-my-localhost",
    "href": "unpublished_posts/networking/self-hosting.html#whats-my-localhost",
    "title": "Self Hosting",
    "section": "",
    "text": "After installing oh my zsh, we can set environment variables in ~/.zshrc using export. We can even set environment variables that affect the Z shell’s appearance. For example, adding the following in the initialization file ~/.zshrc displays the localhost, current working directory, and the prompt character (root vs user) in the shell prompt.\n# Define color codes\nRED=\"%F{red}\"\nGREEN=\"%F{green}\"\nYELLOW=\"%F{yellow}\"\nBLUE=\"%F{blue}\"\nMAGENTA=\"%F{magenta}\"\nCYAN=\"%F{cyan}\"\nWHITE=\"%F{white}\"\nRESET=\"%f\"\n\n# Set the prompt with colors\nPROMPT='${CYAN}%n@%m ${YELLOW}%1~ ${RESET}%# '\nA shell also knows of ‘hostname’ from a system command that’s part of the OS.\necho \"$(hostname)\"\nSome other variables the shell knows of include:\necho \"$USER\" # The name of the logged-in user.\necho \"$HOME\" # The path to the current user's home directory.\necho \"$PATH\" # The list of directories that the shell searches for executable files.\nThese are set by the login process, rather than being exposed by OS commands like hostname or being defined by the user in an initialization files like ~/.zshrc."
  },
  {
    "objectID": "unpublished_posts/networking/self-hosting.html#creating-a-docker-container",
    "href": "unpublished_posts/networking/self-hosting.html#creating-a-docker-container",
    "title": "Self Hosting",
    "section": "Creating a Docker Container",
    "text": "Creating a Docker Container"
  },
  {
    "objectID": "unpublished_posts/general_computer_science/recursion_optimizations.html",
    "href": "unpublished_posts/general_computer_science/recursion_optimizations.html",
    "title": "Recursion Optimizations",
    "section": "",
    "text": "Recursion Optimizations\nRecursive algorithms, while elegant and expressive, can sometimes lead to performance issues due to the overhead of function calls and potential for repeated computations. Several optimization techniques can be employed to enhance their efficiency. One common method is memoization, which stores the results of expensive function calls and reuses them when the same inputs occur again, avoiding repeated computations. Another technique is tail recursion optimization, where the recursive call is the final operation in the function, allowing the system to reuse the current stack frame for each recursive call. Tail recursion optimization reduces the space complexity from \\(O(n)\\) to \\(O(1)\\). Additionally, iterative solutions can often be more efficient than their recursive counterparts, so converting a recursive algorithm to an iterative one can be considered an optimization. Understanding these techniques, and variations on them, can greatly improve the performance of our recursive algorithms.\n\nTail Recursion - Avoiding Stack Overflow\nStack overflow (which is when the system runs out of short term memory) is a common concern when working with recursive functions or when doing functional programming, where function composition is the mode in which we think. Tail recursion optimization helps us drastically cut the number of stack frames and, as mentioned earlier, makes our recursive algorithms take a constant amount of space instead of a linear amount (or worse).\n\nClassic Example: Factorial\nLet’s take the classic example of calculating a factorial.\nNaive Recursive Implementation:\n\ndef factorial(n):\n    if n == 0: # Base case: 0! = 1\n        return 1\n    else: \n        return n * factorial(n-1) # Recursive step\n\nfactorial(4)\n\n24\n\n\nFrom the below untangled definition of factorial (for \\(n=4\\)) we can surmise what goes on in the stack. The stack first fills up with stack frames for factorial(n) down to factorial(0), which is the last frame on the stack before it begins to pop and actual evaluation happens.\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = 4 * factorial(3) \\\\\n        & = 4 * (3 * factorial(2)) \\\\\n        & = 4 * (3 * (2 * factorial(1))) \\\\\n        & = 4 * (3 * (2 * (1 * factorial(0)))) \\\\\n        & = 4 * (3 * (2 * (1 * 1))) \\\\\n        & = 4 * (3 * (2 * 1)) \\\\\n        & = 4 * (3 * 2) \\\\\n        & = 4 * 6 \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nAs we can see the function is called for \\(n = 4\\) down to the base case of \\(n = 0\\) (each call stacking up in memory) before evaluation even begins. Evaluation then happens step-by-step inside each stack frame until all of them have popped.\nIt’s not immediately clear how to make the calls independent of each other given that there is a multiplicative factor in front of the recursive call (which is what makes this particular function fail to be tail-recursive). It helps to think in terms of carried state. In this case the idea is simple, if we can carry the state of the current stack frame into the next one as input, then we can pop each frame right after it calls the next frame. Why? Because at that point, having carried its state into the next frame, the current frame exhausts its usefulness.\nIn the case of the factorial function above, this means that in the tail-recursive implementation the stack is not filled up with as many frames of recursive factorial calls as the input (\\(n\\)) is big. There are still \\(n\\) total calls, however the memory used in the stack is held constant (at a single frame in this case) as each old frame gives way to the new one.\nSo, for now, let’s define a magic function called go(n,acc) with inputs n and what’s called an accumulator acc such that factorial(n) := go(n,1). We take this to be by construction. The function go will be the tail-recursive helper of factorial. The accumulator acc, which is initialized to 1, will be used to remember the state inside the current stack frame (in this case just the multiplicative factor before the recursive call).\nBut so far we’ve only given go(n, acc) its desired properties without actually defining it. The following is the tail-recursive version of factorial which includes the definition of go.\nTail-Recursive Factorial:\n\ndef factorial(n):\n    def go(n,acc): # Helper function with an accumulator\n        if n == 0: # Base case: 0! = 1\n            return acc\n        else:\n            return go(n-1, n * acc) # Tail-recusrive step\n        \n    return go(n,1) # Delegate the problem solution to a helper function\n\nfactorial(4)\n\n24\n\n\nLet’s unpack this:\n\\[\n\\begin{equation}\n    \\begin{split}\n        factorial(4) & = go(4,1) \\\\\n        & = go(3,4) \\\\\n        & = go(2,12) \\\\\n        & = go(1,24) \\\\\n        & = go(0,24) \\\\\n        & = 24\n    \\end{split}\n\\end{equation}\n\\]\nRight away we can see that, with this approach, we can pop the previous stack frame at any time without losing any information it holds because all state is carried over from the previous frame into the current one by the accumulator and, finally, returned at the end. A visual cue of this fact is that in the expression above evaluation happens immediately, rather than step-by-step (with each step corresponding to the popping of a stack frame), as is the case in the naive implementation.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIt’s important to note that this effort only pays off if the language compiler in question supports TCO (Tail Call Optimization). Most, in fact, do. If the language supports TCO the compiler can recognize tail calls and simply pop the current stack frame after the recursive call, replacing it with the subsequent call (rather than blindly stacking frames on top of each other as in the naive recursive algorithm)"
  },
  {
    "objectID": "unpublished_posts/python/ny_housing_market_analysis.html",
    "href": "unpublished_posts/python/ny_housing_market_analysis.html",
    "title": "v-poghosyan.github.io",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\nfile_path = \"C://Users/vapog/Downloads/airbnb_data.csv\"\n\n# Examining the dataframe\ndf = pd.read_csv(file_path)\nprint(df.head(10))\n\n     id                                              name  host_id  \\\n0  2539                Clean & quiet apt home by the park     2787   \n1  2595                             Skylit Midtown Castle     2845   \n2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n3  3831                   Cozy Entire Floor of Brownstone     4869   \n4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n5  5099         Large Cozy 1 BR Apartment In Midtown East     7322   \n6  5121                                   BlissArtsSpace!     7356   \n7  5178                  Large Furnished Room Near B'way      8967   \n8  5203                Cozy Clean Guest Room - Family Apt     7490   \n9  5238                Cute & Cozy Lower East Side 1 bdrm     7549   \n\n     host_name neighbourhood_group       neighbourhood  latitude  longitude  \\\n0         John            Brooklyn          Kensington  40.64749  -73.97237   \n1     Jennifer           Manhattan             Midtown  40.75362  -73.98377   \n2    Elisabeth           Manhattan              Harlem  40.80902  -73.94190   \n3  LisaRoxanne            Brooklyn        Clinton Hill  40.68514  -73.95976   \n4        Laura           Manhattan         East Harlem  40.79851  -73.94399   \n5        Chris           Manhattan         Murray Hill  40.74767  -73.97500   \n6        Garon            Brooklyn  Bedford-Stuyvesant  40.68688  -73.95596   \n7     Shunichi           Manhattan      Hell's Kitchen  40.76489  -73.98493   \n8    MaryEllen           Manhattan     Upper West Side  40.80178  -73.96723   \n9          Ben           Manhattan           Chinatown  40.71344  -73.99037   \n\n         room_type  price  minimum_nights  number_of_reviews last_review  \\\n0     Private room    149               1                  9  2018-10-19   \n1  Entire home/apt    225               1                 45  2019-05-21   \n2     Private room    150               3                  0         NaN   \n3  Entire home/apt     89               1                270  2019-07-05   \n4  Entire home/apt     80              10                  9  2018-11-19   \n5  Entire home/apt    200               3                 74  2019-06-22   \n6     Private room     60              45                 49  2017-10-05   \n7     Private room     79               2                430  2019-06-24   \n8     Private room     79               2                118  2017-07-21   \n9  Entire home/apt    150               1                160  2019-06-09   \n\n   reviews_per_month  calculated_host_listings_count  availability_365  \n0               0.21                               6               365  \n1               0.38                               2               355  \n2                NaN                               1               365  \n3               4.64                               1               194  \n4               0.10                               1                 0  \n5               0.59                               1               129  \n6               0.40                               1                 0  \n7               3.47                               1               220  \n8               0.99                               1                 0  \n9               1.33                               4               188  \n\n\n\n# Number of rentals by New York borough\nbronx_df = df[df['neighbourhood_group'] == 'Bronx']\nprint(\"Bronx: # of rentals \",bronx_df.shape[0])\nbrooklyn_df = df[df['neighbourhood_group'] == 'Brooklyn']\nprint(\"Brooklyn: # of rentals \",brooklyn_df.shape[0])\nmanhattan_df = df[df['neighbourhood_group'] == 'Manhattan']\nprint(\"Manhattan: # of rentals \",manhattan_df.shape[0])\nstaten_island_df = df[df['neighbourhood_group'] == 'Staten Island']\nprint(\"Staten Island: # of rentals \",staten_island_df.shape[0])\n\nBronx: # of rentals  1091\nBrooklyn: # of rentals  20104\nManhattan: # of rentals  21661\nStaten Island: # of rentals  373\n\n\n\n# Most popular neighborhood by number of reviews\nsorted_df = df.sort_values(by='number_of_reviews', ascending=False)\nlocations_and_num_reviews_df = sorted_df[['neighbourhood','number_of_reviews']]\ntop_20_df = locations_and_num_reviews_df.head(20)\n\n# Group by neighborhood and take aggregate mean\ngrouped_df = top_20_df.groupby('neighbourhood')['number_of_reviews'].mean()\nprint(grouped_df)\n\nneighbourhood\nAstoria            441.0\nBushwick           480.0\nEast Elmhurst      485.2\nEast Village       451.0\nFlushing           474.0\nHarlem             564.0\nJamaica            553.0\nLower East Side    540.0\nPark Slope         488.0\nSouth Slope        467.0\nTribeca            447.0\nName: number_of_reviews, dtype: float64\n\n\n\n# Filtering for a client by price in Manhattan's Upper East Side\nupper_east_df = df[df['neighbourhood'] == 'Upper East Side']\nninetieth_percentile = np.quantile(upper_east_df['number_of_reviews'], 0.85) # Normalizes the results by getting rid of bad postings\nupper_east_df = upper_east_df[upper_east_df['number_of_reviews'] &gt;= ninetieth_percentile]\n\n\n## Used to examine the unique room types available\nprint(df['room_type'].unique())\n\n## Cheapest private room type rental in Manhattan's Upper East Side\nprivate_rooms_df = upper_east_df[upper_east_df['room_type'] == 'Private room']\nprivate_rooms_df = private_rooms_df.sort_values('price', ascending=True)\nprivate_rooms_df = private_rooms_df[['neighbourhood','room_type','price']]\nprint(private_rooms_df.head(5))\n\n## Cheapest entire homes/appartments\nentire_homes_df = upper_east_df[upper_east_df['room_type'] == 'Entire home/apt']\nentire_homes_df = entire_homes_df.sort_values('price', ascending=True)\nentire_homes_df = entire_homes_df[['neighbourhood','room_type','price']]\nprint(entire_homes_df.head(5))\n\n['Private room' 'Entire home/apt' 'Shared room']\n         neighbourhood     room_type  price\n8416   Upper East Side  Private room     49\n40185  Upper East Side  Private room     50\n35976  Upper East Side  Private room     60\n21283  Upper East Side  Private room     65\n19830  Upper East Side  Private room     65\n         neighbourhood        room_type  price\n18882  Upper East Side  Entire home/apt     69\n7181   Upper East Side  Entire home/apt     75\n5759   Upper East Side  Entire home/apt     92\n22122  Upper East Side  Entire home/apt     95\n27040  Upper East Side  Entire home/apt     95"
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "Perhaps the most important package for scientific computing included with Conda is NumPy. Let’s get a feel for what NumPy offers.\n\n\n\n\nA 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices.\n\n\n\n\nNumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e. its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don’t have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we’re working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let’s examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one’s shape is (3,1).\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can’t include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#pythons-built-in-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "A 1D array, or a vector, is a collection of scalars (usually, but not necessarily, of similar data type) in a contiguous chunk of computer memory. A 2D array, or a matrix, is a collection of vectors. A 3D array (or a higher dimensional array), also referred to as a tensor, is a collection of matrices."
  },
  {
    "objectID": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "href": "unpublished_posts/python/numpy_quick_start_guide.html#numpy-data-types",
    "title": "NumPy: Quick Start Guide",
    "section": "",
    "text": "NumPy exposes the ndarray type. This is a multidimensional, homogeneous array type (i.e. its elements are of the same data type) optimized for computing and indexed by a tuple. It offeres mathematical indexing (based on Boolean expressions) so that we don’t have to write inefficient loops. The terms vector, matrix, and tensor equally apply to ndarrays.\nTo import NumPy, we can type:\n\nimport numpy as np\n\n\n\n\n\n\nsequence_array = np.arange(10)\nprint(sequence_array)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\nzeros_array = np.zeros((3,4),dtype='int32')\nprint(zeros_array)\nprint(zeros_array.dtype)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\nint32\n\n\n\nones_array = np.ones((3,2))\nprint(ones_array)\nprint(ones_array.dtype)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\nfloat64\n\n\n\n\n\nWe can verify that the object we’re working with is, indeed, and ndarray by using the built-in Python type function.\n\narray1 = np.array([1,2,3])\nprint('array1 type: ', type(array1))\n\narray1 type:  &lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe shape of an ndarray is in format (x,y,...) where x corresponds to the number rows, y corresponds to the number of columns, and so on.\n\nprint('array1 shape: ', array1.shape)\n\narray1 shape:  (3,)\n\n\nHigher dimensional ndarrays take tuples of arrays as input:\n\n\nThere is a subtle difference between a 1D array and a 2D array with a single column which is worth exploring.\nAs we saw above, array1 was of shape (3,). Now let’s examine the shape of a similar ndarray instance.\n\narray2 = np.array([[1],[2],[3]])\nprint('array2 shape: ', array2.shape)\n\narray2 shape:  (3, 1)\n\n\nAs we can see, this one’s shape is (3,1).\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nThe shape (3,) means a 1D array with 3 elements, meanwhile the shape (3,1) means a 2D array with 3 rows and a single column.\n\n\n\nSometimes these differences are just superficial, or the result of data impurities. NumPy provides a method called np.squeeze which flattens the arrays by removing axes of length 1.\n\nprint(np.squeeze(array2).shape == array1.shape)\n\nTrue\n\n\n\n\n\n\narray3 = np.array([[1,2,3], \n                  [4,5,6]])\nprint('array3 shape: ', array3.shape)\n\narray3 shape:  (2, 3)\n\n\n\n\n\n\nTo get the dimension, we use ndarray.ndim.\n\nprint(array1.ndim, array2.ndim, array3.ndim)\n\n1 2 2\n\n\n\n\n\nndarrays can include numeric types (int, unsigned int, float, complex), text types (string), and null. However, as mentioned above, ndarrays can’t include more than one data type. To get the data type of the elements, we use ndarray.dtype.\n\n\n\nWe can reshape ndarrays where it makes sense. For example, we can reshape array3, of shape (2,3) into an array of shape (3,2), (6,1), or (1,6).\n\nprint(array3)\nprint(array3.shape)\narray4 = array3.reshape(3,2)\nprint(array4)\nprint(array4.shape)\n\n[[1 2 3]\n [4 5 6]]\n(2, 3)\n[[1 2]\n [3 4]\n [5 6]]\n(3, 2)\n\n\nProviding the value -1 for either row or column makes the reshape automatic across that dimension. For instance, instead of array3.reshape(3,2) we could say array3.reshape(-1,2) or array3.reshape(3,-1). This would achieve the same effect.\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\nc=np.stack((a,b), axis=1)\nprint(c.shape)\n\n(3, 2)"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html",
    "href": "unpublished_posts/python/introduction-to-numpy.html",
    "title": "Introduction to Numpy",
    "section": "",
    "text": "NumPy is a scientific computing library for Python. It’s an extensive collection of pre-written code that optimizes and extends, among other things, the Python array (i.e. list) object into an n-dimensional NumPy array called ndarray. It comes with a variety of tools, such as matrix operations and common mathematical functions, that enable Python to perform complex mathematical tasks such as solve linear algebraic problems, generate pseudo-random numbers, perform Fourier analysis, etc.\nWe import NumPy, as we import any other library, using the import keyword (with or without a shorthand).\nimport numpy\nOr, alternatively:\nimport numpy as np"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#scalable-memory-representation",
    "href": "unpublished_posts/python/introduction-to-numpy.html#scalable-memory-representation",
    "title": "Introduction to Numpy",
    "section": "Scalable Memory Representation",
    "text": "Scalable Memory Representation\nOne of the things NumPy optimizes is data storage. In contrast to Python 3.x’s scalable memory representation of numeric values, such as integers, which can grow to accommodate a given number, NumPy stores numeric types in fixed-sized blocks of memory (e.g. int32 or int64). This means NumPy is able to take advantage of the low-level CPU instructions of modern processors that are designed for fixed-sized numeric types. Another advantage of fixed-sized storage is that consecutive blocks of memory can be allocated, which enables the libraries upon which NumPy relies to do extremely performant computations. This enforcement of fixed-sized data types is part of the optimization strategy NumPy uses called vectorization."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#vectorization",
    "href": "unpublished_posts/python/introduction-to-numpy.html#vectorization",
    "title": "Introduction to Numpy",
    "section": "Vectorization",
    "text": "Vectorization\nAs already discussed in the aforementioned post, vectorization is the process by which NumPy stores an array internally in a contiguous block of memory, and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does as its iterating through a loop in order to speed up our code. Optimizing the array data structure in such a way enables NumPy to delegate most of the operations on such arrays to pre-written C code under the hood. In effect, this simply means that looping occurs in C instead of Python."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#broadcasting",
    "href": "unpublished_posts/python/introduction-to-numpy.html#broadcasting",
    "title": "Introduction to Numpy",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes the process by which NumPy performs arithmetic operations on arrays of different dimensions. The process is usually as follows: the smaller array is “broadcast” across the larger array so that the two arrays have compatible dimensions. Broadcasting provides a means of vectorizing array operations."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#comparing-runtime",
    "href": "unpublished_posts/python/introduction-to-numpy.html#comparing-runtime",
    "title": "Introduction to Numpy",
    "section": "Comparing Runtime",
    "text": "Comparing Runtime\nTo demonstrate the performance optimizations of NumPy, let’s compare squaring every element of a 1,000,000-element array and summing the results.\n\nUsing a Python List\nFirst, we will use a Python list:\n\n\nCode\nunoptimized_list = list(range(1000000))\n\n\nSquaring each element and summing:\n\n\nCode\nimport numpy as np\n%timeit np.sum([i**2 for i in unoptimized_list])\n\n\n342 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nNote: Even though we’re using NumPy’s sum() method, since the input we’re passing to it is a regular Python list, NumPy optimizations are not applied.\n\n\nAs we can see the whole thing took about 314 ms.\n\n\nUsing a NumPy Array\nNow let’s do the same with a NumPy array, which also gives us the opportunity to introduce the syntax for defining one using a range.\n\n\nCode\noptimized_array = np.arange(1000000)\n\n\nLet’s check the type of optimized_array to convince ourselves that it is, indeed, a NumPy ndarray.\n\n\nCode\ntype(optimized_array)\n\n\nnumpy.ndarray\n\n\nNow, finally, let’s square each element and sum the results:\n\n\nCode\n%timeit np.sum(optimized_array**2)\n\n\n1.61 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\nRemarkably, the run-time was cut from 314 ms to only around 1.61ms!"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#creating-arrays",
    "href": "unpublished_posts/python/introduction-to-numpy.html#creating-arrays",
    "title": "Introduction to Numpy",
    "section": "Creating Arrays",
    "text": "Creating Arrays\nWe’ve already seen how we can create a 1-dimensional NumPy array of consecutive integers \\(0,1,...,n-1\\) using the arrange() method.\nThe standard way of creating a NumPy array is passing a Python list to the constructor array() like so:\n\n\nCode\na = np.array([1,2,3])\na\n\n\narray([1, 2, 3])\n\n\nWe can also create some common arrays, such as an array of consecutive integers, with some special methods such as arange(), which takes an integer \\(n\\) as input and creates a sequential array from \\(0,...,n-1\\).\n\n\nCode\nnp.arange(5) # array([0, 1, 2, 3, 4])\n\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#representing-matrices",
    "href": "unpublished_posts/python/introduction-to-numpy.html#representing-matrices",
    "title": "Introduction to Numpy",
    "section": "Representing Matrices",
    "text": "Representing Matrices\nLet’s represent a \\(2 \\times 3\\) matrix $ A =\n\\[\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\\]\n$ using NumPy:\n\n\nCode\nA = np.array([[1,2,3],\n              [4,5,6]])\nA\n\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThere are also ways to quickly create some common matrices using special methods.\nFor example, ones() accepts a shape pair, and creates a matrix of \\(1\\)s with of the given shape.\n\n\nCode\nnp.ones((2,3))\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThe method, zeros() works the same way as ones():\n\n\nCode\nnp.zeros((2,3))\n\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\nMeanwhile, identity() accepts an integer \\(n\\) as input and creates a square \\(n \\times n\\) identity matrix.\n\n\nCode\nnp.identity(3)\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nCreating a matrix with identical elements in general uses the full() method which takes a shape attribute, a value attribute and on optional dtype attribute as follows:\n\n\nCode\nnp.full((2,3), 7, dtype = int)\n\n\narray([[7, 7, 7],\n       [7, 7, 7]])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#indexing",
    "href": "unpublished_posts/python/introduction-to-numpy.html#indexing",
    "title": "Introduction to Numpy",
    "section": "Indexing",
    "text": "Indexing\nIndexing a 1-dimensional NumPy array is done as expected, through the use of the trusty brackets []. Indexing an n-dimensional matrix in NumPy still uses [] but it introduces a new, improved, syntax.\nSuppose we’d like to access the element in the first row, and last column of A. The standard way would be:\n\n\nCode\nA[0][2]\n\n\n3\n\n\nAs we can see, that still works. But the recommended and, subjectively speaking, prettier way is:\n\n\nCode\nA[0,2]\n\n\n3\n\n\nOf course, slicing still works as expected.\nFor example, let’s print the entire first row of A:\n\n\nCode\nA[0,:]\n\n\narray([1, 2, 3])\n\n\nThe entire first column:\n\n\nCode\nA[:,0]\n\n\narray([1, 4])\n\n\nFinally, let’s print the submatrix $\n\\[\\begin{bmatrix}\n2 & 3\n\\end{bmatrix}\\]\n$:\n\n\nCode\nA[0,1:]\n\n\narray([2, 3])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#properties-and-methods-of-numpy-arrays",
    "href": "unpublished_posts/python/introduction-to-numpy.html#properties-and-methods-of-numpy-arrays",
    "title": "Introduction to Numpy",
    "section": "Properties and Methods of NumPy Arrays",
    "text": "Properties and Methods of NumPy Arrays\nA few of the useful properties and methods of ndarray are highlighted in this section.\n\nshape - returns the shape of the matrix as an \\((m,n)\\) pair\nA.shape # (2,3)\nndim - returns the dimension of a matrix as a single digit\nA.ndim # 2\n\nNote: The output of the ndim property should not be understood in a linear algebraic sense as the dimension of either the domain or range of the corresponding transformation, nor the dimension of either of its four fundamental subspaces. It is to only be understood in the data structure sense as the level of nestedness of the array.\n\nsize - returns the total number of elements in the matrix\nA.size # 6\ndtype - returns the data type of the elements in the matrix.\nA.dtype # dtype('int32')\n\nNote: If the ndarray does not represent a matrix, such as B = np.array([[1,2,3],[4,5]]) then dtype outputs O signifying that the entries are general Python objects. In such a case, the array loses its optimizations.\n\n\n\nStatistical and Mathematical Methods\nThere is also a vast selection of statistical, and more generally, mathematical methods that ndarrays come with. Here are a few of the common ones:\n\nsum() - returns the sum of all the entries\nA.sum() # 21\nIt also accepts an axis attribute where axis = 0 refers to the sum along the columns, and axis = 1 refers to the sum along the rows.\nA.sum(axis = 0) # [5,7,9]\nA.sum(axis = 1) # [6,15]\nmean() - returns the empirical mean of all the entries\nA.mean() # 3.5\nvar() - returns the variance of the entries\nA.var() # 2.9166666666666665\nstd() - returns the standard deviation of the entries\nA.std() # 1.707825127659933"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#multi-indexing-filtering-and-broadcasted-operations",
    "href": "unpublished_posts/python/introduction-to-numpy.html#multi-indexing-filtering-and-broadcasted-operations",
    "title": "Introduction to Numpy",
    "section": "Multi-Indexing, Filtering, and Broadcasted Operations",
    "text": "Multi-Indexing, Filtering, and Broadcasted Operations\nRecall from the Pandas article the ways in which we were able to multi-index and filter, and how we eliminated the need for using Python loops and list comprehensions using broadcasted operators instead. Since both a Pandas Series and a DataFrame are extensions of NumPy’s ndarray, all of these apply here as well.\nAs a refresher on broadcasted operations, here are a few filtering examples.\nLet’s obtain those elements of A that are greater than 3:\n\n\nCode\nA[A &gt; 3]\n\n\narray([4, 5, 6])\n\n\nNow let’s obtain those elements of A that are greater than the empirical mean:\n\n\nCode\nA[A &gt; A.mean()]\n\n\narray([4, 5, 6])\n\n\nWhat about those elements of A that are less than or equal to the empirical mean?\n\n\nCode\nA[~(A &gt; A.mean())]\n\n\narray([1, 2, 3])\n\n\nWhich is equivalent to:\n\n\nCode\nA[A &lt;= A.mean()]\n\n\narray([1, 2, 3])"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-numpy.html#matrix-operations",
    "href": "unpublished_posts/python/introduction-to-numpy.html#matrix-operations",
    "title": "Introduction to Numpy",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nOne of NumPy’s key features is the way in which it simplifies matrix operations in Python. It offers simple syntax to add, multiply, transpose, invert, flatten, etc.\n\nAddition\nAddition of matrices is, by default, per-element (as are all NumPy operations). There’s no special syntax, it’s done through the + operator.\nFor example:\n\n\nCode\nA = np.ones((2,3))\nB = np.ones((2,3))\nA + B\n\n\narray([[2., 2., 2.],\n       [2., 2., 2.]])\n\n\n\n\nMultiplication\nThe operator * performs per-element multiplication.\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((2,3), 3, dtype = int)\nA * B\n\n\narray([[6, 6, 6],\n       [6, 6, 6]])\n\n\nBut this, as we know, isn’t matrix multiplication as it’s commonly defined in mathematics — that being the inner product of corresponding rows and columns. For instance, if we try to multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix, NumPy will throw the following error:\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA * B\n\n\nValueError: operands could not be broadcast together with shapes (2,3) (3,4) \nThis is because per-element operations require the shapes of the operands to be the same or compatible by broadcasting. Here NumPy attempts to broadcast one operand to match the shape of the other one, but broadcasting is impossible between matrices of shapes \\(2 \\times 3\\) and \\(3 \\times 4\\) per broadcasting rules.\nThere’s a workaround that lets us use *. Since NumPy overloads the * operator, it works as it should for numpy.matrix types.\n\n\nCode\nA = np.matrix([[2,2,2],\n               [2,2,2]])\nB = np.matrix([[3,3,3,3],\n               [3,3,3,3],\n               [3,3,3,3]])\nA * B\n\n\nmatrix([[18, 18, 18, 18],\n        [18, 18, 18, 18]])\n\n\nHowever, this is not the recommended way to carry out matrix multiplication in NumPy. Overloaded operators can produce convoluted code. For instance, we may have many different matrix and ndarray data structures and never be able to tell the outcome of a particular * operation. We should avoid such ambiguity whenever possible.\nInstead, the recommended way to do matrix multiplication is through the @ operator.\nWhen we use @ NumPy internally uses its matmul() method. So, the following are equivalent and both produce the matrix product of A and B.\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA @ B\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nnp.matmul(A,B)\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\nNumPy also offers, dot() which, for one and two dimensional matrices, is equivalent to matmul(). So, the following is yet another way we can multiply two matrices:\n\n\nCode\nA = np.full((2,3), 2, dtype = int)\nB = np.full((3,4), 3, dtype = int)\nA.dot(B)\n\n\narray([[18, 18, 18, 18],\n       [18, 18, 18, 18]])\n\n\nHowever, matmul() is preferred over dot() because of the clarity of its name, and because the dot product has a distinct mathematical meaning separate from matrix multiplication."
  },
  {
    "objectID": "unpublished_posts/network_and_security/contract_testing.html",
    "href": "unpublished_posts/network_and_security/contract_testing.html",
    "title": "Contract Testing",
    "section": "",
    "text": "PactFlow\nThe idea behind bi-directional contract tracing, within the conmtext of API testing, is the client testing against a fresh mock of the server and the server, in reciprocation, testing against a fresh mock of the client. If the pact between the client and server is breached, neither the server team nor the client team can merge their code into the release branch."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "About this site\nHi there, I’m Vahram.\nThese are my notes on the internet. I started them in late 2021 from some university notes I took in Jupyter Notebooks (and online material I was learning in my free time). None of this is intended as original work, although some of it may be (and I will try to clearly communicate that where possible). Please refer to the copyright note below for more details. More recently I started using various static page generators (such as quarto) to convert my notes into a form that looks presentable in a browser while preserving the interactibility of Jupyter Notebooks.\n\n\nLicense\nCopyright (c) 2023 Vahram Poghosyan\nThe styling, assets (including pictures, videos, code, etc.) and most of the written content (with the exception of attributed content or such content that is in the common domain) on these pages are strictly copyrighted. Exceptions will be considered, please reach out to vapogyan@gmail.com to inquire about re-using specific assets."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html",
    "title": "Linear Algebra for Optimization",
    "section": "",
    "text": "Let’s start exploring mathematics for machine learning with a refresher on convexity in optimization and the linear algebra that’s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition:   A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\n\n\n\nFigure 1: Set A is convex, set B is non-convex\n\n\n\n\n\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\n\nConvex hull\n\n\n\n\nFigure 2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis’s algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there’s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it’s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\n\nConvex hull of a set\n\n\n\n\nFigure 3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points — it’s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)’s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it’s the line that passes through them, and for three points it’s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)’s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere’s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x :  a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that’s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we’ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x  : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)’s positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it’s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q :  x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#convexity",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#convexity",
    "title": "Linear Algebra for Optimization",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition:   A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) is also in \\(C\\).\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points (or, more generally, vectors) in the set is also entirely inside the set. \n\n\n\n\n\n\n\n\n\n\n\n(a) Convex\n\n\n\n\n\n\n\n\n\n\n\n(b) Non-convex\n\n\n\n\n\n\n\nFigure 1: Set A is convex, set B is non-convex\n\n\n\n\n\nScaling, skewing, and rotation (which can be thought of as linear transformations) preserve convexity as does shifting (an affine transformation). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#examples-of-convex-sets",
    "href": "posts/linear_algebra/linear_algebra_refresher_for_optimization.html#examples-of-convex-sets",
    "title": "Linear Algebra for Optimization",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice. To discuss sets, we should build up from points. For the purposes of the discussion that follows, A point and a vector mean the same thing.\n\n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. By varying parameters \\(\\theta_i\\) we generate the convex hull as the set of all convex combinations of these points.\n\n\n\n\n\n\nConvex hull\n\n\n\n\nFigure 2: The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points (if we imagine those to be pegs sticking out of the screen).\n\n\n\nThe convex hull of two points is the line segment joining them. That of three points is the triangle whose vertices they form (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\nSeveral algorithms exist for generating convex hulls efficiently. The most popular one is Jarvis’s algorithm which simulates a rope wrapping around the leftmost point of the point set. More on that here.\n\n\n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there’s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it’s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique \\(^{(†)}\\) smallest convex superset of \\(C\\), its convex hull.\n\n\n(†) Proof of uniqueness: Let \\(C_1\\) and \\(C_2\\) be two convex hulls of \\(C\\). Let \\(c_1 \\in C_1\\) be a point. Since \\(c_1 \\in C_1\\), \\(c_1 \\in\\) at least one of the convex supersets \\(C^{i}\\)of \\(C\\). Hence, \\(c_1 \\in C_2\\) since \\(C_2 = \\bigcap^{i=1 \\to n}C^{i}\\). Similarly, it can be shown that any \\(c_2 \\in C_2\\) also belongs to \\(C_1\\). Hence, \\(C_1 \\subseteq C_2\\) and vice versa. This proves that \\(C_1 = C_2\\) and completes the proof of uniqueness.\n\n\n\n\n\n\nConvex hull of a set\n\n\n\n\nFigure 3: Visualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points — it’s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)’s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it’s the line that passes through them, and for three points it’s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)’s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThere’s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x :  a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that’s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we’ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and \\(\\{ x : a^T x \\leq b\\}\\).\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x  : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\n\n\n\n\n\nNote\n\n\n\n\n\nMore generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)’s positive semidefiniteness.\n\n\n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it’s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q :  x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/optimization/duality-theory.html",
    "href": "posts/optimization/duality-theory.html",
    "title": "Duality Theory",
    "section": "",
    "text": "Every convex optimization problem, designated as the primal, has a related problem called its dual which can be colloquially thought of as its evil twin. The primal and the dual represent two different perspectives on the same problem.\nIn the most general case, if the primal is a minimization problem, its dual is a maximization problem. In the case of constrained optimization, if the primal is minimization in \\(n\\) variables and \\(m\\) constraints then its dual is a maximization in \\(m\\) variables and \\(n\\) constraints.\nFurthermore, any feasible value of the dual is a lower-bound for all feasible values of the primal. In particular, should they both exist, the dual optimum is a lower bound for the primal optimum. This property, called weak duality, lies at the core of duality theory. The utility of formulating a problem whose solution obtains, at least, a lower-bound for the primal optimum and, in the special case, the primal optimum itself should be self-evident.\nIn the best case scenario a problem exhibits a property called strong duality, which guarantees that the primal and the dual optima agree. Such problems are called strongly dual problems and include, but are not limited to, all linear programs (LPs) and a category of convex non-linear optimization problems. For strongly dual problems, solving the dual guarantees that we’ve also solved the primal. Furthermore, as we shall see, taking the dual of the dual gives back the primal. So this relationship is true in the converse — if we’ve solved the primal then we’ve also solved its dual.\nThis is what makes duality theory so useful in practice. Having a related, usually easier, optimization problem gives applied scientists a huge computational advantage. However, even if the dual does not turn out to be any easier to solve and/or strong duality fails to hold, we still stand to gain structural insight about the primal problem itself.\nIn this post we show how the dual of a problem arises, we examine its relationship with the primal, and list all possible primal-dual outcomes. In doing so, we look at duality in the general case of constrained optimization, in the specific case of linear programs, and in a category of unconstrained problems."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#the-lagrangian-dual-variables-and-the-dual-function",
    "href": "posts/optimization/duality-theory.html#the-lagrangian-dual-variables-and-the-dual-function",
    "title": "Duality Theory",
    "section": "The Lagrangian, Dual Variables, and the Dual Function",
    "text": "The Lagrangian, Dual Variables, and the Dual Function\nThe Lagrangian linear relaxation, sometimes simply referred to as the Lagrangian, is:\n\\[\\mathcal{L}(x,\\lambda,\\mu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\mu_i h_i(x)\\] \\[\\textrm{where} \\ \\lambda \\geq 0\\]\nWe call the \\(\\lambda_i\\)’s the Lagrange multipliers corresponding to the inequality constraints, and the \\(\\mu_i\\)’s those corresponding to the equality constraints. The vectors \\(\\lambda\\) and \\(\\mu\\), composed of these Lagrange multipliers, are called the Lagrange multiplier vectors or, for reasons that will soon become apparent, the dual variables.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn some sources, the Lagrangian is simply stated as \\(\\mathcal{L}(x,\\lambda) = f_0(x) + \\sum_{i=1}^n \\lambda_i f_i(x)\\). Indeed, by separating the equality constraints \\(h_i(x) = 0\\) into \\(h_i(x) \\leq 0\\) and \\(-h_i(x) \\leq 0\\), we can transform a problem with equality constraints into one with only inequality constraints. So, this formulation of the Lagrangian is still general enough to account for problems with equality constraints."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#a-lagrangian-lower-bound",
    "href": "posts/optimization/duality-theory.html#a-lagrangian-lower-bound",
    "title": "Duality Theory",
    "section": "A Lagrangian Lower-Bound",
    "text": "A Lagrangian Lower-Bound\nNot only does the Lagrangian (\\(\\mathcal{L}\\)) relax the unconstrained problem, it also plays a natural role in the formulation of the dual problem.\nThe first thing to note about the Lagrangian is that the coordinate-wise \\(\\lambda \\geq 0\\) condition is crucial. This is because, in the event that an inequality constraint is violated, say \\(f_i(x) &gt; 0\\), the corresponding \\(\\lambda_i\\) must be non-negative in order to apply a positive penalty to the minimization. On the other hand, \\(\\mu\\) is free to assume any value since the equality constraints can be violated in either direction and both scenarios must be positively penalized.\nThe second thing to note about the Lagrangian is that, even though it applies a positive penalty that scales linearly in the severity of the violation, this penalty is, nevertheless, not as severe as the infinite penalty applied in \\(\\mathcal{J}\\). Also, in the Lagrangian, we may actually be rewarding feasible choices of \\(x\\) that have margin. That is, in the event that \\(f_i(x) &lt; 0\\), \\(\\lambda_if_i(x)\\) is a non-positive reward for the minimization problem.\nAll of this is to say that \\(\\mathcal{L}\\) is a point-wise lower-bound on \\(\\mathcal{J}\\). That is, the following inequality holds:\n\\[\\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu \\tag{3.1}\\]\nThis fact is also obvious by plotting each of the \\(m + p\\) linear penalties, superimposing them against the plots of the corresponding infinitely hard penalty functions, and noticing that in each case \\(\\lambda_i f_i(x) \\leq \\mathbb{1}_-(f_i(x))\\) and \\(\\mu_i h_i(x) \\leq \\mathbb{1}_0(h_i(x))\\).\nTaking \\(\\min\\) w.r.t. \\(x\\) of the LHS in \\((3.1)\\) we get:\n\\[\\min_x \\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu\\]\nFurthermore, restricting \\(x\\) to the primal feasible set \\(\\mathcal{X}\\) on which \\(J(x) = f_0(x)\\), we obtain something interesting:\n\\[\\min_x \\mathcal{L}(x,\\lambda,\\mu) \\leq f_0(x) \\ \\ \\forall x \\in \\mathcal{X}, \\lambda \\geq 0, \\mu \\tag{3.2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Lagrangian may not attain its \\(\\min\\) w.r.t. \\(x\\), in which case the LHS is simply \\(-\\infty\\). We shall see later, once we define the dual function and the duality gap, that this corresponds to the dual function being \\(-\\infty\\) \\(\\forall \\lambda \\geq0, \\mu\\) and the duality gap being \\(\\infty\\). In a sense, this is a useless lower bound. So, for now, we assume the interesting case in which the minimum is attained and thus \\(\\inf_x \\mathcal{L}(x, \\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\).\n\n\n\nDesignating the original problem as the primal, we call \\(g(\\lambda, \\mu) := \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\) the dual function because it exhibits the aforementioned property of weak duality. That is, per \\((3.2)\\), any feasible value of \\(g(\\lambda, \\mu)\\) is a lower-bound for any feasible value of the primal.\nTaking min of the other side, we have a more specific flavor of weak duality:\n\\[g(\\lambda,\\mu) \\leq \\min_x f_0(x) \\ \\ \\forall \\lambda \\geq 0, \\mu\\]\nOr simply:\n\\[g(\\lambda,\\mu) \\leq f_0(x^*) \\ \\ \\forall \\lambda \\geq 0, \\mu \\tag{3.3}\\]\nThat is, any feasible value of the dual is a lower-bound for the primal optimum.\nMaximizing both sides of \\((3.3)\\) by noticing that the RHS is a constant, and by assuming the LHS attains its \\(\\max\\) we get an even more specific flavor of weak duality:\n\\[\\max_{\\lambda \\geq 0, \\mu} g(\\lambda,\\mu) \\leq f_0(x^*)\\]\nOr simply, assuming \\(\\lambda^*\\) and \\(\\mu^*\\) to be dual-optimal:\n\\[g(\\lambda^*, \\mu^*) \\leq f_0(x^*) \\tag{3.4}\\]\nThat is, the dual optimum is a lower-bound for the primal optimum.\nFrom here we move, quite naturally, to defining the dual problem."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#the-lagrange-dual-problem",
    "href": "posts/optimization/duality-theory.html#the-lagrange-dual-problem",
    "title": "Duality Theory",
    "section": "The Lagrange Dual Problem",
    "text": "The Lagrange Dual Problem\nIt’s natural, to ask what the tightest lower bound on the primal optimal value \\(f_0(x^*)\\) is. This amounts to finding the values \\(\\lambda^* \\geq 0\\), and \\(\\mu^*\\) for which \\(g(\\lambda^*, \\mu^*)\\) is maximized. We call this the Lagrange dual problem or, simply, the dual problem.\nIt can be stated as:\n\\[\n\\begin{aligned}\n\\max_{\\lambda, \\mu} &: g(\\lambda, \\mu)\n\\\\\ns.t. &: \\lambda \\geq  0\n\\end{aligned}\n\\]\nLooking at the above, it becomes immediately clear why we were motivated to call \\(\\lambda\\), and \\(\\mu\\) the dual variables: they are the variables of the dual problem."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#the-max-min-inequality",
    "href": "posts/optimization/duality-theory.html#the-max-min-inequality",
    "title": "Duality Theory",
    "section": "The Max-Min Inequality",
    "text": "The Max-Min Inequality\nThe inequality expressed as \\((3.5)\\) is, in fact, a general result in mathematics called the Max-Min Inequality. To summarize: the Max-Min Inequality makes no assumptions about the function, it’s true for all functions of the form \\(f: X \\times Y \\rightarrow \\mathbb{R}\\) and asserts that:\n\\[\\sup_{x\\in X} \\left\\{ \\inf_{y\\in Y} f(x,y) \\right\\} \\leq \\inf_{y\\in Y} \\left\\{ \\sup_{x\\in X} f(x,y) \\right\\}\\]\nSince no assumption is made on \\(f\\), the inequality also holds for the Lagrangian, \\(\\mathcal{L}\\). And, since we’re in the special case where the optimal values of the primal and the dual are assumed to exist, the functions do attain the respective optima. That is, we can replace \\(\\sup\\) and \\(\\inf\\) in the above inequality with \\(\\max\\) and \\(\\min\\) which obtains the symmetric formulation of weak duality as in \\((3.5)\\).\nWe can now prove weak duality through a non-optimization lens by proving the Max-Min Inequality.\nFor any \\(f\\), and \\(x \\in X\\), \\(y \\in Y\\) we have:\n\\[f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x\\]\nThe right-hand side is now only a function of \\(x\\), so minimizing both sides w.r.t. \\(x\\) yields:\n\\[ \\inf_x f(x,y) \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\} \\ \\ \\forall y\\]\nThe right-hand side is now a constant, so maximizing both sides w.r.t. \\(y\\) results in the desired conclusion.\n\\[\\sup_y \\left\\{ \\inf_x f(x,y) \\right\\} \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Max-Min Inequality proof should remind us of the steps taken to obtain \\((3.2)\\) through \\((3.4)\\) from \\((3.1)\\). In fact, \\((3.1)\\) is of form \\(f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x\\), since \\(J(x)\\) is, as shown earlier, equivalent to \\(\\max_{\\lambda \\geq 0, \\mu} L(x, \\lambda, \\mu)\\).\n\n\n\n\nGame-Theoretic Interpretation\nThe Max-Min Inequality is perhaps best understood intuitively as a game between two adversarial players (the optimizers above).\nLet’s represent the game using a value tree. The nodes are the final scores, and the junctures represent player choices. The first juncture represents Player 1’s turn, and the second that of Player 2.\n\n\n\n\n\n   graph TD;\n      A[Turn 1] --&gt; B[Turn 2];\n      A--&gt;C[Turn 2];\n      B--&gt;D[2];\n      B--&gt;E[7];\n      C--&gt;F[1];\n      C--&gt;G[8];\n\n\n\n\n\n\nTo the minimizer, as Player 1, the game tree above might as well be an imaginary tree shrouded in the mist of uncertainty because the true min of \\(1\\) is unattainable. Player 2 has final say, and if the minimizer chooses \\(1\\)’s subtree, the maximizer will choose \\(8\\). The minimizer must be more pragmatic and choose not the subtree that contains the true min, but rather that which restricts the maximizer’s choice most to the compromise: \\(7\\). That means, we can compute a hidden value tree for our minimizer (giving us a glimpse into one frame of the recursive Minimax algorithm which is used to maximize the minimum gain or minimize the maximum loss).\n\n\n\n\n\n   graph TD;\n      A[Turn 1] --&gt; B[7];\n      A--&gt;C[8];\n\n\n\n\n\n\nThe outcome of this game is determinedly \\(7\\). That’s a score Player 2, the maximizer, is very satisfied with. Note that if we reverse the turns, the score of the game will favor the minimizer. That’s exactly what the max-min inequality is saying: a game that’s rigged in favor of Player 2 will always result in a better outcome for Player 2 than if the turns were reversed."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#slaters-condition---sufficient-condition-for-strong-duality",
    "href": "posts/optimization/duality-theory.html#slaters-condition---sufficient-condition-for-strong-duality",
    "title": "Duality Theory",
    "section": "Slater’s Condition - Sufficient Condition for Strong Duality",
    "text": "Slater’s Condition - Sufficient Condition for Strong Duality\nWhile the rare non-convex problem could exhibit the property, strong duality is mostly enjoyed by convex problems. However, not all convex problems are strongly dual. There are many results that establish conditions on the problem, beyond convexity and existence of a primal-optimal, under which strong duality holds. These conditions are called constraint qualifications. In this section we will explore such conditions for convex problems and discuss them in the specific case of linear programs.\nOne of these constraint qualification conditions is Slater’s condition.\n\nSlater’s Condition:   \\(\\exists \\ \\hat x\\) s.t. \\(f_i(\\hat x) &lt; 0\\), and \\(h_i(\\hat x) = 0\\) \\(\\forall i\\).\n\nInformally, Slater’s condition says that the existence of a feasible point which has margin w.r.t. all the inequality constraints is needed in addition to convexity. In even simpler terms, the feasible region must have an interior point.\nThe sufficient condition for strong duality in convex problems is then:\n\nSufficient Condition for Strong Duality:   Any convex optimization problem satisfying Slater’s condition has zero duality gap.\n\nThe proof of this is beyond what we’re trying to accomplish in this post.\nA weaker constraint qualification condition guarantees strong duality in the case of linear constraints. If \\(k\\) of the \\(m\\) inequality constraints are linear then the condition becomes:\n\\[\n\\begin{aligned}f_i(\\hat x) &\\leq 0, \\ i = 1,...,k, \\\\\nf_i(\\hat x) &&lt; 0, \\ i = k+1,...,m, \\\\\nh_i(\\hat x) &= 0, \\ i = 1,...,p\n\\end{aligned}\n\\]\nIn other words, the linear constraints need not have margin.\nNote that if all the constraints are linear, which is the case in linear programming, the above constraint qualification condition simply reduces to feasibility.\nSo, while a sufficient condition of strong duality in non-linear convex programs is, both, the existence of a feasible interior point and a primal optimal, the situation is remarkably simpler in linear programs. Since a primal optimal for a linear program is also feasible, it satisfies the weaker constraint qualification condition. Thus, for a linear program to be strongly dual the existence of a primal optimal is sufficient."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#the-max-min-equality",
    "href": "posts/optimization/duality-theory.html#the-max-min-equality",
    "title": "Duality Theory",
    "section": "The Max-Min Equality",
    "text": "The Max-Min Equality\nJust as weak duality is the Max-Min Inequality in disguise, strong duality is the Minimax Theorem in disguise. The Minimax Theorem is about the special case of the Max-Min Inequality in which the LHS and the RHS are strictly equal. It holds for any function \\(f: X \\times Y \\rightarrow \\mathbb{R}\\) that has some additional structure. Roughly speaking, when \\(f\\) is saddle-shaped, convex in one variable and concave in the other, the Max-Min Inequality holds with strict equality.\nThe following theorem, which is offered without proof, translates this result into the setting of optimization.\n\nSaddle Point Theorem:   If \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal and dual optimal solutions for a convex problem which satisfies Slater’s condition, they form a saddle point of the associated Lagrangian. Furthermore, if \\((x^*, (\\lambda^*, \\mu^*))\\) is a saddle point of a Lagrangian, then \\(x^*\\) is primal optimal and \\((\\lambda^*, \\mu^*)\\) is dual optimal for the associated problem, and the duality gap is zero.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis theorem should not be taken as a certificate of strong duality. If the Lagrangian is saddle-shaped then the associated problem is strongly dual, however the converse is not true. Since not all strongly dual problems are convex problems which satisfy Slater’s condition, if a problem is strongly dual it is not guaranteed that its Lagrangian is saddle-shaped.\n\n\n\n\nGame-Theoretic Interpretation\nIn keeping with the game theoretic intuition developed in the section on weak duality, one can imagine a game in which the first player’s optimal choice is independent of the second player’s actions. In such a game, both players are free to play their best strategies and, consequently, the order of play is not important."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#an-easier-dual-problem",
    "href": "posts/optimization/duality-theory.html#an-easier-dual-problem",
    "title": "Duality Theory",
    "section": "An Easier Dual Problem",
    "text": "An Easier Dual Problem\nLet’s further qualify what we mean when we say strong duality gives an equivalent, usually easier, problem to solve.\nAt the start of this post we considered a general convex program. However, everything we’ve discussed about Lagrangian duality applies to non-convex problems as well. Suppose the primal problem is non-convex. The task is that of finding the primal optimum:\n\\[f_0(x^*) = \\min_x \\left\\{ \\max_{\\lambda \\geq 0, \\mu} \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nBut maximizing the Lagrangian over \\(\\lambda \\geq 0\\) and \\(\\mu\\) for a fixed \\(x\\), recovers \\(\\mathcal{J}(x)\\): a non-differentiable objective. So, we cannot use the unconstrained optimality condition in finding the stationary points of \\(\\mathcal{J}(x)\\) which is what’s required in the next step.\nMeanwhile, the dual problem is that of finding the dual optimum:\n\\[g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nMinimizing the Lagrangian over \\(x\\) for fixed \\(\\lambda \\geq 0\\) and \\(\\mu\\) may still be a difficult problem but, at least, it lends itself to using the method of unconstrained optimization. Moreover, the resulting dual function \\(g(\\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)\\) is a point-wise minimum of linear functions in \\(\\lambda\\) and \\(\\mu\\), so its always concave in those variables. Additionally, the constraint \\(\\lambda \\geq 0\\) is a simple, convex (linear in fact), constraint. So, the dual problem is a convex optimization problem regardless of the convexity of the primal.\nSolving a convex dual problem is usually easier that solving a non-convex primal problem. However, even if the primal is a convex problem to begin with, the dual may still be easier to solve. The primal could have more variables than constraints in which case its dual has more constraints than variables. This is yet another way in which the dual can be an easier problem to solve than the primal."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#farkas-lemma",
    "href": "posts/optimization/duality-theory.html#farkas-lemma",
    "title": "Duality Theory",
    "section": "Farkas’ Lemma",
    "text": "Farkas’ Lemma\nFarkas’ Lemma simply states that a given vector \\(c\\) is either a conic combination of some vectors \\(a_i\\)’s (for \\(i \\in I\\)), or it’s entirely separated from their cone by some hyperplane.\nWe state Farkas’ Lemma without offering proof since it has such an obvious geometric interpretation.\n\nFarkas’ Lemma:   For any vector \\(c\\) and \\(a_i \\ \\ (i \\in I)\\) either the first or the second statement holds:\n\n\\(\\exists p \\geq 0\\) s.t. \\(c = \\sum_{i \\in I} a_ip_i\\)\n\\(\\exists\\) vector \\(d\\) s.t. \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I\\) but \\(d^Tc &lt; 0\\)"
  },
  {
    "objectID": "posts/optimization/duality-theory.html#proving-a-theorem-of-the-alternative",
    "href": "posts/optimization/duality-theory.html#proving-a-theorem-of-the-alternative",
    "title": "Duality Theory",
    "section": "Proving a Theorem of the Alternative",
    "text": "Proving a Theorem of the Alternative\nTo see how we can prove a Theorem of the Alternative, it helps to state one.\n\nTheorem:   Exactly one of the following two statements most hold for a given matrix A.\n\n\\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\)\n\\(\\exists p\\) s.t. \\(p^TA &gt; 0\\)\n\n\n\nProof using a Separation Argument\n\nPrelude\nAt the heart of separation arguments lies this simple fact.\n\nSeparating Hyperplane Theorem:   For any convex set \\(C\\), if a point \\(\\omega \\notin C\\) then there exists a hyperplane separating \\(\\omega\\) and \\(C\\).\n\nFarkas’ Lemma, for instance, is proved by a separation argument that uses, as its convex set, the conic combination of the \\(a_i\\)’s. The conclusion is immediate since in Farkas’ Lemma the first statement plainly says that a vector belongs to the convex set, and the second statement plainly says there exists a separating hyperplane between the two.\nThis is the pattern all separation arguments must follow. However, in general, it may take a bit of work to define the problem-specific convex set and also to show that the two statements are really talking about belonging to this set, and separation from it. However, once these components are in place, the proof is complete.\nUsing this idea, let’s give a proof of the above theorem using a separation argument.\n\n\nProof\nFirst order of business is to come up with a convex set.\nLet’s take \\(C = \\{ z : z = Ay, \\sum_i y_i = 1, y \\geq 0 \\}\\) to be the convex hull of the columns of \\(A\\).\nThe first statement in the theorem was that \\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\).\nSince \\(x \\ne 0\\) and \\(x \\geq 0\\) we can scale as \\(x\\) as \\(y = \\alpha x\\) until \\(\\sum_i y_i = 1\\).\nSo, the first statement is equivalent to saying the origin belongs to the convex hull \\(C\\) (i.e. \\(0 \\in C\\))\nThe second statement was that \\(\\exists p\\) s.t. \\(p^TA &gt; 0\\). This is equivalent to saying that all the columns of \\(A\\) lie to one side of the separating hyperplane introduced by \\(p\\).\nBut all \\(z \\in C\\) are convex combinations of \\(A\\)’s columns. In particular since they’re a convex combination they’re also a conic combination, so all \\(z \\in C\\) also lie on the same side of the hyperplane. That is \\(p^Tz &gt; 0 \\ \\ \\forall z \\in C\\).\nBut, of course, \\(p^T0 = 0\\) (not \\(&gt; 0\\)). So, according to the second statement, the origin is separated from \\(C\\).\nThis concludes the proof since the two statements must be mutually exclusive.\n\n\n\nProof using Strong Duality\nTo prove the theorem we need to show two things. First, we need to show \\(1 \\implies \\neg 2\\), then we need to show \\(\\neg 1 \\implies 2\\).\nThe \\(1 \\implies \\neg 2\\) direction is simple.\nSuppose \\(\\exists x \\ne 0\\) s.t. \\(Ax = 0\\) and \\(x \\geq 0\\).\nThen \\(\\forall p \\ \\ (p^TA)x = p^T(Ax) = p^T0 = 0\\) (not \\(&gt; 0\\)).\nWe tackle the \\(\\neg 1 \\implies 2\\) direction using duality.\nThe strategy is to construct an LP based on \\(\\neg 1\\) such that the feasibility of its dual implies \\(2\\).\nWe can express \\(\\neg 1\\) as ‘\\(\\forall x \\ne 0\\), either \\(Ax \\ne 0\\) or \\(x &lt; 0\\).’ Equivalently, ‘\\(x \\ne 0 \\implies Ax \\ne 0\\) or \\(x &lt; 0\\).’ Taking the contrapositive, statement \\(1\\) becomes ‘\\(Ax = 0\\) and \\(x \\geq 0 \\implies x = 0\\).’\nSo, we form the LP as:\n\\[\n\\begin{aligned}\n&\\max_x: \\textbf{1}^Tx\n\\\\\n&s.t.: \\begin{aligned} &Ax = 0\n\\\\\n&x \\geq 0\n\\end{aligned}\n\\end{aligned}\n\\]\nNote that \\(x = 0\\) is a feasible solution to the LP. Furthermore, assuming statement \\(1\\) guarantees that \\(x = 0\\) is the only feasible solution. Thus, the LP is feasible and bounded.\nBy strong duality, its dual exists and is also feasible and bounded.\nThe dual is:\n\\[\n\\begin{aligned}\n&\\min_p: \\textbf{0}^Tp\n\\\\\n&s.t.: p^TA \\geq \\textbf{1}\n\\end{aligned}\n\\]\nSince the dual is feasible, \\(\\exists p\\) s.t. \\(p^TA \\geq 1 &gt; 0\\) which demonstrates the truth of statement \\(2\\) and, in doing so, completes the proof."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#stationarity-condition",
    "href": "posts/optimization/duality-theory.html#stationarity-condition",
    "title": "Duality Theory",
    "section": "Stationarity Condition",
    "text": "Stationarity Condition\nIn the section titled an easier dual problem we mentioned that the dual problem is that of finding the dual optimal value:\n\\[g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}\\]\nIf strong duality holds, this dual optimum agrees with the primal optimum. That is:\n\\[g(\\lambda^*, \\mu^*) = f_0(x^*)\\]\nTurns out in case of strong duality there’s even more to be said. As we saw earlier optimizing the unconstrained objective \\(\\mathcal{J}(x)\\) not only resulted in the primal optimum \\(f_0(x^*)\\) for some optimal \\(x^*\\) of the constrained problem, the very same point \\(x^*\\) itself turned out to be an optimizer of \\(\\mathcal{J}(x)\\). Similarly, we can show that the primal optimum \\(x^*\\) for some primal-dual optimal pair \\((x^*, (\\lambda^*, \\mu^*))\\) optimizes \\(\\mathcal{L}(x, \\lambda^*, \\mu^*)\\). In other words, the primal optimum \\(x^*\\) is a stationary point of the Lagrangian at the dual optimum \\((\\lambda^*,\\mu^*)\\).\nThat is:\n\\[x^* = \\arg \\min_x \\mathcal{L} (x, \\lambda^*, \\mu^*) \\tag{6.1}\\]\nOr, equivalently:\n\\[\\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) = \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\tag{6.2}\\]\nWe can think of \\((6.1)\\) and \\((6.2)\\) as the analogs of \\((2.1)\\) and \\((2.2)\\) for the Lagrangian (\\(\\mathcal{L}\\)). This is exactly what we’ve been working towards. Recall that the original motivation in augmenting the constrained problem into the unconstrained \\(\\mathcal{J}\\) was to find the former’s optimizer using methods of unconstrained optimization on \\(\\mathcal{J}\\). Once found, \\((2.1)\\) or \\((2.2)\\) would guarantee that an optimizer of \\(\\mathcal{J}\\) was, itself, an optimizer of the original problem. Failing that, we relaxed \\(\\mathcal{J}\\) into \\(\\mathcal{L}\\) hoping we could still accomplish the same. \\((6.1)\\) and \\((6.2)\\) are the results which guarantee precisely that. They say that the optimizer \\(x^*\\) of the original problem can be found by optimizing the unconstrained objective \\(\\mathcal{L}\\). And, since \\(\\mathcal{L}\\) is everywhere differentiable w.r.t. \\(x\\), we can now proceed.\nIn practice, however, \\((6.1)\\) and \\((6.2)\\) only give us a way to solve for a primal-optimal \\(x^*\\) directly if a dual-optimal \\((\\lambda^*, \\mu^*)\\) is already known. That is, any time the dual problem is easier to solve than the primal.\nMore generally, this fact gives us the next best thing. It gives us a way to check if a given pair \\((x^*,(\\lambda^*,\\mu^*))\\) is primal-dual optimal – an optimality condition known as stationarity condition.\n\nStationarity Condition:   Suppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem. Then: \\[\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\]\n\nThe stationary condition is obtained simply by an application of the unconstrained optimality condition to \\(\\mathcal{L}(x, \\lambda^*, \\mu^*)\\):\n\\[\\nabla_x \\mathcal{L} (x^*, \\lambda^*, \\mu^*) = 0\\]\nExpanding the LHS gives:\n\\[\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\]\nFor the sake of completeness, since we stated them without offering a proof, let’s prove the equivalent claims \\((6.1)\\) and \\((6.2)\\) from which stationarity condition ultimately follows.\n\nProof of Claims (6.1) and (6.2)\nSuppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem.\nThe following point-wise inequality holds in general since its LHS is a minimization over \\(x\\) and its RHS is a maximization over \\((\\lambda, \\mu)\\) of the Lagrangian.\n\\[g(\\lambda, \\mu) \\leq \\mathcal{L}(x, \\lambda, \\mu) \\leq \\mathcal{J}(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu\\]\nIt is also, in particular, true for the primal-dual optimal pair. That is:\n\\[g(\\lambda^*, \\mu^*) \\leq \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\leq \\mathcal{J}(x^*) \\tag{7.1}\\]\nHowever, \\(\\mathcal{J}(x^*) = f_0(x^*)\\) and, by strong duality, \\(g(\\lambda^*, \\mu^*) = f_0(x^*)\\). Hence, \\(g(\\lambda^*, \\mu^*) = \\mathcal{J}(x^*)\\) and \\((7.1)\\) is actually the equality.\n\\[\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = g(\\lambda^*, \\mu^*) \\tag{7.2}\\]\nSubstituting, the definition of the dual function for the RHS of \\((7.2)\\), we get:\n\\[\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*)\\]\nWhich is exactly \\((6.2)\\) and, by equivalence, also \\((6.1)\\)."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#complementary-slackness",
    "href": "posts/optimization/duality-theory.html#complementary-slackness",
    "title": "Duality Theory",
    "section": "Complementary Slackness",
    "text": "Complementary Slackness\nStrong duality also obtains another optimality condition known as complementary slackness (CS).\n\nComplementary Slackness (CS):   Suppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem. Then: \\[\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i\\]\n\nInformally, if a primal constraint at an optimal \\(x^*\\) is loose, that is \\(f_i(x^*) \\ne 0\\), then its corresponding dual variable \\(\\lambda^*_i\\) in the dual optimal \\(\\lambda^*\\) must be zero. Conversely, if the dual variable \\(\\lambda_i^*\\) is positive then the corresponding constraint must be tight.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf a primal constraint is tight at \\(x^*\\), complementary slackness tells us nothing about its corresponding dual variable.\n\n\n\n\nProof of Complementary Slackness\nSuppose \\(x^*\\) and \\((\\lambda^*, \\mu^*)\\) are primal-dual optimal for a strongly dual problem.\nExpanding the RHS we obtain:\n \\[\n\\begin{aligned}\nf_0(x^*) &= g(\\lambda^*, \\mu^*) \\\\\n&= \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) \\\\\n&= \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\\\\n&=  f_0(x^*) + \\sum_{i=1}^m \\lambda_i^* f_i(x) + \\sum_{i=1}^p \\mu_i^* h_i(x^*) \\\\\n&\\leq f_0(x^*)\n\\end{aligned} \\tag{8.1}\n\\] \nThe first equality holds by strong duality, the second holds by the definition of the dual function, the third equality holds by \\((6.2)\\), and the fourth is true by the expansion of \\(\\mathcal{L}(x^*, \\lambda^*, \\mu^*)\\).\nTo see why the last inequality holds, note that:\n\\[\\sum_{i=1}^p \\mu_i^* h_i(x^*) = 0\\]\nsince, by feasibility of \\(x^*\\), \\(h_i(x^*) = 0 \\ \\ \\forall i\\). Then again, by feasibility of \\(x^*\\), we have:\n\\[f_i(x^*) \\leq 0  \\ \\ \\forall i \\tag{8.2}\\]\nFurthermore, by construction of the Lagrangian, \\(\\lambda \\geq 0\\). So, together with \\((8.2)\\), we have:\n\\[\\sum_{i=1}^m \\lambda^*_i f_i(x^*) \\leq 0\\]\nBut taken altogether \\((8.1)\\) says \\(f_0(x^*) \\leq f_0(x^*)\\) which can only hold through strict equality.\nThen it must be the case that \\(\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0\\)\nBeing a sum of non-positive terms, \\(\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0\\) if and only if\n\\[\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i \\tag{8.3}\\]\nwhich concludes the proof of complementary slackness."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#karush-kuhn-tucker-kkt-conditions",
    "href": "posts/optimization/duality-theory.html#karush-kuhn-tucker-kkt-conditions",
    "title": "Duality Theory",
    "section": "Karush-Kuhn-Tucker (KKT) Conditions",
    "text": "Karush-Kuhn-Tucker (KKT) Conditions\nComplementary slackness and stationarity condition are often bundled into the KKT Conditions.\nIn the absence of strong duality the KKT Conditions are necessary but insufficient for optimality. However, for problems which are strongly dual the KKT Conditions become a certificate of optimality. That is, they are both necessary and sufficient.\n\nKKT Conditions:   The primal-dual pair \\((x^*, (\\lambda^*, \\mu^*))\\) satisfies the KKT conditions if the following hold:\n\n\\(\\nabla_x f_0(x^*) + \\sum_{i=1}^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0\\)\n\\(\\lambda^*_if_i(x^*) = 0 \\ \\ \\forall i\\)\n\\(g_i(x^*) \\leq 0 \\ \\ \\forall i\\)\n\\(h_i(x^*) = 0 \\ \\ \\forall i\\)\n\\(\\lambda^* \\geq 0\\)\n\n\nWe recognize KKT-1 as the stationarity condition, and KKT-2 as complementary slackness. KKT-3 through KKT-5 simply ensure primal-dual feasibility.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThese conditions only apply to problems with differentiable objective and constraints. For the case in which one or more of the objective or constraints is non-differentiable, there is an easy generalization of the KKT conditions using sub-differentials. However, sub-differentials are beyond the scope of this post.\n\n\n\nPrimal-dual pairs which satisfy the KKT Conditions are called KKT pairs.\n\nGeneralization of Unconstrained Optimization\nThe KKT conditions represent a strict generalization of the unconstrained optimality condition for use in constrained problems.\nTo see this, note that if there are no constraints then the KKT conditions simply reduce to the familiar unconstrained optimality condition:\n \\[\\nabla_x f_0(x^*) = 0\\] \nIn order to discuss optimality in constrained problems, we must first define a feasible direction.\n\nFeasible Direction:   A unit vector \\(d\\) is called a feasible direction at any \\(x\\) if \\(x + \\epsilon d\\) remains feasible for \\(\\epsilon &gt; 0\\) small enough.\n\nWe are now in a position to generalize the unconstrained optimality condition into a constrained optimality condition.\nBy using Taylor expansion, for small enough \\(\\epsilon &gt; 0\\) and any feasible \\(d\\), we can estimate \\(f_0(x^* + \\epsilon d)\\) by its linear approximation as:\n \\[f_0(x^* + \\epsilon d) = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td\\] \nBut since \\(x^*\\) is optimal, we have:\n \\[\n\\begin{aligned}\nf_0(x^*) &\\leq f_0(x^* + \\epsilon d) \\\\\n& = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td\n\\end{aligned}\n\\] \nWhich necessitates that \\(\\nabla f_0(x^*)^Td \\geq 0\\). Since \\(d\\) was just an arbitrary feasible direction, this result must hold for all feasible directions. Hence, the constrained optimality condition can be given as:\n\nConstrained Optimality Condition:   If \\(x^*\\) is an optimizer of \\(f_0\\) over some constraint set then, for any feasible direction \\(d\\) at \\(x^*\\), \\(\\nabla f_0(x^*)^Td \\geq 0\\).\n\nNote that \\(\\nabla f_0(x^*)^Td\\) is simply the directional derivative of \\(f_0\\) in the direction \\(d\\). So, in plain words, the constrained optimality condition says that the directional derivative of the objective function in any feasible direction at an optimizer should be non-negative. This ensures that moving in any feasible direction does not minimize the objective any further.\n\n\nCertificate of Optimality\nAs promised, the KKT Conditions together with strong duality obtain a certificate of optimality.\n\nCertificate of Optimality:   If strong duality holds, then \\(x^*, (\\lambda^*, \\mu^*)\\) are primal-dual optimal if and only if they are a KKT pair.\n\n\nProof of Certificate of Optimality\nWe have already shown one direction of the certificate in the sections on stationarity condition and complementary slackness, where we proved that being a primal-dual optimal pair in a strongly convex problem guarantees \\((x^*, (\\lambda^*, \\mu^*))\\) is also a KKT pair.\nShowing the other direction provides us with an interesting geometric viewpoint of the KKT conditions. Incidentally, Farkas’ Lemma is the key theoretical result that underpins this proof.\nLet’s begin the proof.\nIf a particular constraint is loose at \\(x^*\\) then taking a small enough step in any direction from \\(x^*\\) does not violate it. Formally, if \\(f_i(x^*) &lt; 0\\), then \\(f_i(x^* + \\epsilon d) \\leq 0\\) \\(\\forall d\\) and for some \\(\\epsilon &gt;0\\). So, loose constraints do not pose any restrictions on the set of feasible directions.\nHowever, if a constraint is tight at \\(x^*\\), that is \\(f_i(x^*) = 0\\), then we must be careful not to violate it. Suppose the set of indices of all the tight constraints at \\(x^*\\) is given by \\(I_{x^*}\\). For small enough \\(\\epsilon &gt; 0\\), we can estimate \\(f_i(x^* + \\epsilon d)\\) by its linear Taylor expansion as:\n\\[f_i(x^* + \\epsilon d) = f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td \\ \\ \\forall i \\in I_{x^*}\\]\nFor feasibility, we want \\(f_i(x^* + \\epsilon d) \\leq 0\\). So, we require:\n\\[f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*}\\]\nBut since \\(f_i\\) is tight at \\(x^*\\), \\(f_i(x^*) = 0\\), which simply leaves us with:\n\\[\\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*}\\]\nWith the above restriction of \\(d\\), the feasible directions can now be stated as:\n\\[d \\ \\textrm{s.t.} \\ \\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\in I_{x^*} \\tag{8.1}\\]\nOr, equivalently:\n\\[d \\ \\textrm{s.t.} \\ - \\nabla f_i(x^*)^Td \\geq 0 \\ \\ \\forall i \\in I_{x^*} \\tag{8.2}\\]\nBut, since \\(x^*\\) is optimal, by the constrained optimality condition we have:\n\\[\\nabla f_0(x^*)^Td \\geq 0 \\ \\ \\forall \\ \\textrm{feasible} \\ d \\tag{8.3}\\]\nThat is, for all \\(d\\) as in \\((8.2)\\).\nPut together, \\((8.2)\\) and \\((8.3)\\) say that \\(\\not \\exists \\ d\\) which defines a separating hyperplane between \\(\\nabla f_0(x^*)\\) and \\(-\\nabla f_i(x^*)\\) for all binding constraints at \\(x^*\\). By Farka’s Lemma, this means that the only other alternative scenario must be true — it must be the case that \\(\\nabla f_0(x^*)\\) lies in the cone of the \\(-\\nabla f_i(x^*)\\)’s.\nFormally, \\(\\exists \\ \\lambda^* \\geq 0\\) s.t.\n\\[\\nabla f_0(x^*) + \\sum_{i \\in I_{x^*}} \\lambda^*_i f_i(x^*) = 0 \\tag{8.4}\\]\nUpon closer examination, \\((8.4)\\) is exactly KKT-1, KKT- 2, and KKT-5 all rolled into one condition. The remaining conditions, KKT-3 and KKT-4 simply follow from the assumed feasibility of \\(x^*\\).\nThus, we have shown that if \\(x^*\\) is primal-optimal, its KKT pair \\((x^*, (\\lambda^*, \\mu^*))\\) exists. Furthermore, as proved earlier, if strong duality holds then any KKT pair is primal-dual optimal. Hence, if strong duality holds, the \\((\\lambda^*, \\mu^*)\\) obtained through the above procedure is also dual-optimal."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#proof-of-strong-duality-in-lps",
    "href": "posts/optimization/duality-theory.html#proof-of-strong-duality-in-lps",
    "title": "Duality Theory",
    "section": "Proof of Strong Duality in LP’s",
    "text": "Proof of Strong Duality in LP’s\n\nPrelude\nAs in the general case, we construct a KKT pair through the use of Farkas’ Lemma. Then, by a structural property of LPs, we notice that the dual and the primal optima agree. This concludes the proof of LP strong duality.\n\n\nProof\nSuppose \\(x^*\\) is primal-optimal. Let the set \\(I_{x^*} = \\{ i : a_i^Tx^* = b_i\\}\\) be the set of the indices of the active constraints at \\(x^*\\). Our goal is to construct a dual optimal solution \\(p^*\\) s.t. \\(c^Tx^* = b^Tp^*\\).\nLet \\(d\\) be any vector that satisfies \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}\\). That is, \\(d\\) is a feasible direction w.r.t. to all the active constraints.\nBy the assumption that \\(x^*\\) is optimal, we have \\(c^Tx^* \\leq c^T(x^* + \\epsilon d) = c^Tx^* + \\epsilon c^Td\\). Thus, \\(c^Td = d^Tc \\geq 0\\)\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(d^Tc\\) is nothing but the directional derivative at the minimizer \\(x^*\\). So, this also follows from the optimality of \\(x^*\\) using the constrained optimality condition.\n\n\n\nBut, since \\(d\\) is a vector s.t. \\(d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}\\) and \\(d^Tc \\geq 0\\), \\(d\\) does not separate \\(c\\) from the cone of the \\(a_i\\)’s. And, since \\(d\\) was arbitrary, this puts us in the setting of Farkas’ Lemma. Namely, there exist no vectors \\(d\\) that separate \\(c\\) from the cone. This means the alternative must be true — \\(c\\) must a conic combination of the \\(a_i\\)’s that are active at the minimizer. In other words, \\(\\exists p \\geq 0\\) s.t. \\(c = \\sum_{i \\in I_{x^*}} p_ia_i\\).\nBut \\(p\\) has dimension equal to only the number of active constraints at \\(x^*\\). To be a dual variable at all, it must have dimension equal to the number of all primal constraints. We extend \\(p\\) to \\(p^*\\) by setting all the entries that do not correspond to the active constraints at \\(x^*\\) to be zero.\nThat is \\(p^*_i = \\begin{cases} p_i \\ \\ \\textrm{if} \\ \\  i \\in I_{x^*} \\\\ 0   \\ \\ \\textrm{if} \\ \\  i \\notin I_{x^*} \\end{cases}\\).\nNow \\(A^Tp^*  = \\sum_{i} p^*_ia_i = c\\), so any feasibility condition in the dual, whether it be \\(A^Tp \\leq c\\), \\(A^Tp \\geq c\\), or \\(A^Tp = c\\), is satisfied by \\(p^*\\).\nFurthermore, the dual objective at \\(p^*\\) agrees with the primal objective at \\(x^*\\).\n\\[b^Tp^* = \\sum_{i} b_ip_i^* = \\sum_{i \\in I_{x^*}} b_ip_i^* + \\sum_{i \\notin I_{x^*}} b_ip_i^* = \\sum_{i \\in I_{x^*}} a_i^Tx^*p_i^* = (\\sum_{i \\in I_{x^*}} p_ia_i^T)x^* = c^Tx^* \\]\nThis concludes the proof."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#fl-transform---a-convex-operation",
    "href": "posts/optimization/duality-theory.html#fl-transform---a-convex-operation",
    "title": "Duality Theory",
    "section": "FL Transform - a Convex Operation",
    "text": "FL Transform - a Convex Operation\nThe FL Transform \\(f^*\\) is always convex regardless of the convexity of \\(f\\).\nThat’s because, for a fixed \\(x\\), \\(y^Tx - f(x)\\) is a linear function in \\(y\\). So, \\(f^*\\) is a point-wise supremum of linear functions, making it convex."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#the-case-of-involution",
    "href": "posts/optimization/duality-theory.html#the-case-of-involution",
    "title": "Duality Theory",
    "section": "The Case of Involution",
    "text": "The Case of Involution\nThe double FL Transform \\(f^{**}\\) does not always recover \\(f\\). To see this fact note that, as an FL Transform of the some function (namely, \\(f^*\\)), \\(f^{**}\\) is always convex. Therefore, \\(f^{**} \\ne f\\) if \\(f\\) is non-convex.\nBut convexity alone is not enough to guarantee involution. We need an additional condition on \\(f\\), namely that its sub-level sets must be closed, to ensure \\(f^{**} = f\\)."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#inverse-gradients",
    "href": "posts/optimization/duality-theory.html#inverse-gradients",
    "title": "Duality Theory",
    "section": "Inverse Gradients",
    "text": "Inverse Gradients\nIf \\(f\\) has closed sub-level sets and is convex then the gradients of \\(f\\) and \\(f^*\\) are inverses. That is, assuming both \\(f\\) and \\(f^*\\) are differentiable:\n\\[y = \\nabla f(x) \\iff x = \\nabla f^*(y)\\]\nLet’s first prove the \\(\\implies\\) direction.\nSuppose \\(y = \\nabla f(x)\\). By \\(f\\)’s convexity:\n\\[f(\\hat x) \\geq f(x) + y^T(\\hat x - x) \\ \\ \\forall \\hat x\\]\nAnd so:\n\\[y^T \\hat x - f(\\hat x) \\leq y^T x - f(x) \\ \\ \\forall \\hat x\\]\nBy taking supremum over \\(x\\) and by noting that, since the sub-level sets are closed, the supremum is attained, we obtain:\n\\[f^*(y) = y^T x - f(x)\\]\nThe desired result follows by taking the gradient of both sides w.r.t. \\(y\\). That is:\n\\[\\nabla f^*(y) = x\\]\nThe \\(\\impliedby\\) direction is similar. We start from the assumption that \\(x = \\nabla f^*(y)\\) and get the desired result by using the involution property \\(f^{**} = f\\)."
  },
  {
    "objectID": "posts/optimization/duality-theory.html#fl-duality",
    "href": "posts/optimization/duality-theory.html#fl-duality",
    "title": "Duality Theory",
    "section": "FL Duality",
    "text": "FL Duality\nAs mentioned, the FL Transform has a natural role in duality.\nSuppose the unconstrained optimization problem is:\n\\[\\min_x : f(x) + h(Ax)\\]\nWhere \\(f\\) and \\(h\\) are convex functions, and \\(A\\) is a matrix representing a bounded linear transformation.\nWe introduce a dummy variable \\(y\\) and form the artificial constraint \\(y = Ax\\). The problem becomes:\n\\[\n\\begin{aligned}\n\\min_{x,y} &: f(x) + h(y) \\\\\ns.t. &: Ax = y\n\\end{aligned}\n\\]\nForming the Lagrangian gives us:\n\\[\\mathcal{L}(x,y,z) = f(x) + h(y) + z^T(Ax - y)\\]\nThen, the dual function is the following FL Transform:\n\\[\n\\begin{aligned}\ng(z) &= \\min_{x,y} \\mathcal{L}(x,y,z) \\\\\n&= \\min_{x,y} f(x) + h(y) + z^T(Ax - y) \\\\\n&= \\min_{x,y} (A^Tz)^Tx + f(x) - z^Ty + h(y) \\\\\n&= \\min_x \\left\\{ (A^Tz)^Tx + f(x) \\right\\} + \\min_y \\left\\{ -z^Ty + h(y) \\right\\} \\\\\n&= \\min_x \\left\\{ -\\left((-A^Tz)^Tx - f(x)\\right) \\right\\} + \\min_y \\left\\{ -\\left(z^Ty - h(y)\\right) \\right\\} \\\\\n&= - \\max_x \\left\\{ (-A^Tz)^Tx - f(x) \\right\\} - \\max_y \\left\\{ z^Ty - h(y) \\right\\} \\\\\n&= - f^*(-A^Tz) - h^*(z)\n\\end{aligned}\n\\]\nAnd, consequently, the dual problem is:\n\\[\\max_z: - f^*(-A^Tz) - h^*(z)\\]\nTo convince ourselves of the utility of this dual, note that the dual is, indeed, an easier problem. This is because the negative of an FL Transform is always concave regardless of the convexity of \\(f\\) and \\(h\\). So, the dual problem is a maximization of a concave function which is, in general, an easy optimization problem."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html",
    "href": "posts/optimization/introduction_to_optimization.html",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "An optimization problem can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e. minimize or maximize) some objective function. The objective function can be almost anything — cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are basically the same problem. Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\), the reflected objective function.\n\n\nThis post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, let’s define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/introduction_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In this series, we frame an optimization problem in the following form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not go over the ways in which we can model a real-world problem as one in the given form. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/introduction_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization",
    "section": "",
    "text": "First, let’s define the size of an optimization problem as: The dimensionality of the parameter \\(x\\), added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html",
    "title": "Linear Programs and LP Geometry",
    "section": "",
    "text": "A linear program is a special type of convex optimization problem in \\(n\\)-dimensions that has a linear objective and a constraint set that’s a polytope. That is, its constraint set is an intersection of \\(n\\)-dimensional linear inequalities (halfspaces) and linear equalities (hyperplanes).\nIn matrix form, it may be stated as\n\\[\n\\begin{cases}\n\\min_x: c^Tx\n\\\\\ns.t.: \\begin{aligned} &A_1x \\leq b_1\n\\\\\n&A_2x \\geq b_2\n\\\\\n&A_3x = b_3\n\\end{aligned}\n\\end{cases}\n\\]\nwhere \\(c \\in \\mathbb{R}^n\\) is the cost vector of the objective function, \\(x \\in \\mathbb{R^n}\\) is the decision variable, \\(A_1 \\in \\mathbb{R}^{m \\times n}\\), \\(b_1 \\in \\mathbb{R}^m\\) and \\(A_2 \\in \\mathbb{R}^{p \\times n}\\), \\(b_2 \\in \\mathbb{R}^p\\) together define the collection of linear inequality constraints, and \\(A_3 \\in \\mathbb{R}^{q \\times n}\\) and \\(b_3 \\in \\mathbb{R}^q\\) define the collection of linear equality constraints.\nAs we will shortly prove, an LP in any form such as the one above can be converted into its standard form\n\\[\n\\begin{cases}\n\\min_x: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax = b\n\\\\\n&x \\geq 0\n\\end{aligned}\n\\end{cases} \\dagger\n\\]\n\n\nLinear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they’re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP’s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc.\nThe reason LP’s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html#applications",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html#applications",
    "title": "Linear Programs and LP Geometry",
    "section": "",
    "text": "Linear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they’re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP’s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc.\nThe reason LP’s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html#introducing-slack-variables",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html#introducing-slack-variables",
    "title": "Linear Programs and LP Geometry",
    "section": "Introducing Slack Variables",
    "text": "Introducing Slack Variables\nThe inequality constraint \\(A_1x \\leq b_1\\) has slack. Formally, we can define vector \\(s \\geq 0\\) (component-wise), that bridges the gap between \\(A_1x\\) and \\(b_1\\), that is s.t. \\(A_1x + s = b_1\\).\nSince this introduces new variables, we have to represent those in the objective and the equality constraints in a way that doesn’t affect the optimization outcome.\nThe LP becomes\n\\[\n\\begin{cases}\n\\min_x: c^Tx + \\mathbf{0}^Ts\n\\\\\ns.t.: \\begin{aligned} &A_1x + s = b_1\n\\\\\n&A_2x + 0s = b_2\n\\\\\n&s \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\nThis LP is equivalent to the one before. Namely, if the previous optimizer was \\(x^*\\), the optimizer in the new LP is the concatenation \\([x^*, b_1 - A_1x]\\) which gives the same optimal value in the objective function.\nThis is almost in standard form, an LP with only equality constraints, and non-negativity constraints. However, the decision variable of this LP is the concatenation \\([x,s]^T\\), whereas the non-negativity applies to \\(s\\) alone.\nThe next step is to decompose \\(x\\) as \\(x = x^+ - x^-\\) where \\(x^+,x^- \\geq 0\\) respectively contain only the positive and only the negative entries of \\(x\\). That is, \\(x^+\\), and \\(x^-\\) have entries \\(x_i^+ = \\max\\{0, x_i\\}\\) and \\(x_i^- = -\\min\\{0, x_i\\}\\).\nWith this substitution we get\n\\[\n\\begin{cases}\n\\min_x: c^Tx^+ - c^Tx^- + \\mathbf{0}^Ts\n\\\\\ns.t.: \\begin{aligned} &A_1x^+ - A_1x^- + s = b_1\n\\\\\n&A_2x^+ - A_2x^- + 0s = b_2\n\\\\\n&x^+, x^-, s \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\nWhich is an LP in standard form \\(\\dagger\\)."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html#extreme-points---geometric-definitions",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html#extreme-points---geometric-definitions",
    "title": "Linear Programs and LP Geometry",
    "section": "Extreme Points - Geometric Definitions",
    "text": "Extreme Points - Geometric Definitions\nFirst, let’s give a couple of geometric definitions of an extreme point.\n\nDefinition 1:   A point \\(x\\) is an extreme point of a polytope \\(P\\) if it is not the convex combination of any other two points in the polytope.\n\nThat is, if \\(\\exists y,z \\in P\\) and \\(\\lambda \\in [0,1]\\) s.t. \\(x = \\lambda y + (1- \\lambda)z\\) then \\(x\\) is not an extreme point of \\(P\\).\n\nDefinition 2:   A point \\(x\\) is an extreme point of a polytope \\(P\\) if it is the unique optimum for some cost vector \\(c\\).\n\nThat is, if \\(\\exists c \\in \\mathbb{R}^n\\) s.t. \\(c^Tx &lt; c^Ty \\ \\ \\forall y \\in P\\) then \\(x\\) is an extreme point."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html#extreme-points---algebraic-definition",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html#extreme-points---algebraic-definition",
    "title": "Linear Programs and LP Geometry",
    "section": "Extreme Points - Algebraic Definition",
    "text": "Extreme Points - Algebraic Definition\nIt’s useful to define an extreme point algebraically. To that end, let’s define the concept of a basic feasible solution (BFS).\nSuppose we have the polytope \\(\\{x : Ax \\leq b, Dx = f\\}\\).\n\nDefinition:   An active constraint at \\(x\\) is a constraint that’s satisfied through strict equality.\n\nThat is, the \\(i\\)-th constraint is said to be active at x if \\(a_i^Tx = b_i\\).\nThis can be thought of as \\(x\\) being on the edge of the halfspace defined by \\(a_i^Tx \\leq b_i\\).\nWe can also define the active set at \\(x\\) as the set of all active constraints at \\(x\\).\nSo, the active set at \\(x\\) is \\(\\mathcal{A}_x = \\{ a_i : a_i^Tx = b_i \\} \\cup \\{ d_i : d_i^Tx = f_i \\}\\), where \\(\\{ d_i : d_i^Tx = f_i \\}\\) is included for completeness.\n\nBasic Feasible Solution\nWe are now ready to define what it means for a point \\(x\\) to be a basic feasible solution of a linear program.\n\nDefinition:   The point \\(x\\) is a basic feasible solution (BFS) of the linear program if its active set \\(\\mathcal{A}_x\\) contains exactly \\(n\\) linearly independent vectors where \\(n\\) is the dimension of \\(x\\).\n\nLet’s ponder the BFS definition for a minute.\nImagine a closed polytope in 2D. Each of its vertices are defined by, at least, two intersecting lines. It’s possible that a vertex is the result of the intersection of three or more lines, but deleting all but two of those lines will still retain the vertex. In other words, two linearly independent (i.e. non-parallel) constraints define an extreme point in 2D.\nThe BFS definition is simply a generalization of this insight to \\(n\\)-dimensions.\nAs we will prove shortly, BFS and extreme point are synonymous. In fact, the following are equivalent:\n\n\\(x\\) is an extreme point by Definition 1.\n\\(x\\) is an extreme point by Definition 2.\n\\(x\\) is a basic feasible solution.\n\n\n\nMatrix-Vector Formulation of Basic Feasible Solutions\nTaking as our starting point an LP in standard form \\(\\dagger\\) we can characterize basic feasible solutions in matrix-vector form.\nTake the standard constraint set \\(\\Omega = \\{ Ax = b, x \\geq 0 \\}\\) and let’s make a few simplifying assumptions.\n\n\\(A\\) is \\(m \\times n\\) with \\(m \\leq n\\).\n\\(A\\) is full-rank\n\\(b \\geq 0\\)\n\\(A\\) has form \\(A = [B,D]\\) where \\(B\\) is an \\(m \\times m\\) full-rank matrix and \\(D\\) is the rest of \\(A\\).\n\nSome of these assumptions impose restrictions on \\(\\Omega\\), whereas others are without loss of generality.\nAssumption 1 is simply there to make the problem interesting. Were \\(n &gt; m\\), the system of equalities would be over-determined and the constraint set would either be empty or contain a single point, which itself would be the optimum. It is, therefore an assumption which is not done without loss of generality.\nAssumption 2 is equivalent to saying \\(rank(A) = m\\). That is to say, all \\(m\\) rows of \\(A\\), as well as \\(m\\) of the \\(n\\) columns of \\(A\\), are linearly independent. This assumption is also not done without loss of generality. Having less linearly independent rows corresponds to having less non-redundant constraints which clearly affects the constraint set \\(\\Omega\\).\nAssumption 3 is done W.L.O.G. since the signs of \\(A\\)’s row entries can always be flipped.\nAssumption 4 is also done W.L.O.G. because if \\(A\\) contains a full-rank \\(m \\times m\\) submatrix per Assumption 2, then \\(A = [B,D]\\) is a re-ordering of \\(A\\) which adds no further restrictions on \\(\\Omega\\).\nFor \\(\\Omega\\) that satisfies Assumptions 1-4, the basic feasible solutions can be reformulated as follows.\n\nDefinition:   Let \\(x_B\\) be such that. \\(Bx_B = b\\). Then the concatenation \\(x = [x_B, 0]^T\\) is a solution to \\(Ax = b\\). Such solutions are called feasible solutions. Furthermore, if \\(x_B \\geq 0\\), such solutions are called basic feasible solutions.\n\nNote that, for the case we’re in, this is consistent with the earlier definition of a BFS.\nLet \\(x\\) be a BFS according to this definition. \\(Ax = b\\) poses a set of \\(m\\) linearly independent constraints since \\(rank(A) = m\\), whereas \\(x \\geq 0\\) poses a set of \\(n\\). But \\(x = [x_B, 0]^T\\) is a vector at which all \\(m\\) of the equality constraints \\(Ax = b\\) are active, and \\(n-m\\) of the inequality constraints \\(x \\geq 0\\) are also active. So in total \\(n\\) linearly independent constraints are active at a BFS, which is consistent with the earlier definition.\n\n\nBasic Feasible Solutions and Extreme Points are Equivalent\nTo formally prove that basic feasible solutions are extreme points in the geometric sense, consider the following theorem and its proof.\n\nTheorem:   The point \\(x\\) is an extreme point of \\(\\Omega = \\{ Ax =b, x \\geq 0 \\}\\) if and only if it is a basic feasible solution.\n\n\nProof\nSufficiency \\(\\implies\\):\nLet \\(x\\) be an extreme point of \\(\\Omega\\). Since it’s in \\(\\Omega\\), \\(x \\geq 0\\) and \\(Ax = b\\).\nEquivalently, \\(\\sum_{i=1}^n x_ia_i = b\\) where the \\(a_i\\)’s are the column vectors of \\(A\\).\nNote that \\(x\\) must contain zero entries, since it takes \\(n\\) linearly independent active constraints to be an extreme point and only \\(m\\) come from the equality constraints \\(Ax = b\\).\nBy Assumption 1, \\(m\\) of \\(A\\)’s columns are linearly independent. We’d like to claim that these \\(m\\) are exactly those corresponding to the non-zero \\(x_i\\) entries.\nIf this claim turns out to be true, then the full-rank \\(m \\times m\\) submatrix \\(B\\) will contain exactly those \\(m\\) columns. And \\(x = [x_B, 0]^T\\), where \\(x_B\\) are the non-zero entries of \\(x\\), would be a BFS. \\(\\ast\\)\nSo, let’s prove the linear independence claim using a contradiction argument.\nWithout loss of generality, through rearrangement, let the first \\(m\\) elements be the nonzero entries. That is, \\(x_1, ..., x_m &gt; 0\\), and \\(x_{m+1}, ... ,x_n = 0\\).\nThen \\(\\sum_{i=1}^n x_ia_i = \\sum_{i=1}^m x_ia_i = b\\).\nTowards contradiction, assume \\(a_1, ..., a_m\\) are linearly dependent. Then \\(\\exists y_1, ..., y_m \\in \\mathbb{R}\\) not all zero s.t. \\(y_1a_1 + ... + y_ma_m = 0\\)\nTake \\(\\epsilon &gt; 0\\) to be very small. Small enough so that \\(x_i \\pm \\epsilon y_i &gt; 0 \\ \\ \\forall i = 1,...,m\\).\nDefine two points as\n\\(z^1 = [x_1 - \\epsilon y_1, ..., x_p - \\epsilon y_p, 0, ..., 0]^T\\) and, \\(z^2 = [x_1 + \\epsilon y_1, ..., x_p + \\epsilon y_p, 0, ..., 0]^T\\).\nThese points clearly satisfy \\(z_1,z_2 \\geq 0\\), so they they satisfy one of \\(\\Omega\\)’s constraints.\nFurthermore,\n\\(Az^1 = \\sum_{i=1}^m z^1_ia_i = \\sum_{i=1}^m x_ia_i - \\epsilon \\sum_{i=1}^m y_ia_i = b\\) since \\(\\sum_{i=1}^m y_ia_i = 0\\).\nand similarly \\(Az^2 = b\\).\nSo, \\(z^1\\), and \\(z^2\\) are indeed in \\(\\Omega\\).\nBut note that \\(x = \\frac{z^1 + z^2}{2}\\) is a convex combination of two points in \\(\\Omega\\), which contradicts the assumption that it’s an extreme point.\nHence, \\(a_1,...,a_m\\) must be linearly independent. This concludes the proof by \\(\\ast\\).\nNecessity \\(\\impliedby\\):\nSuppose \\(x\\) is a BFS and assume, towards contradiction, that it’s not and extreme point of \\(\\Omega\\).\nThen \\(\\exists y,z \\in \\Omega\\) with \\(y \\ne z\\) s.t. \\(x = \\alpha y + (1- \\alpha) z\\) for some \\(\\alpha \\in (0,1)\\).\nBut since \\(y,z \\in \\Omega\\) they satisfy \\(Ay = Az = b\\), so \\(Ay - Az = A(y - z) = 0\\).\nThat is \\((y_1 - z_1)a_1 + ... + (y_m - z_m)a_m = 0\\).\nBut since \\(y \\ne z\\), not all \\((y_i - z_i) = 0\\). So, \\(a_1,..., a_m\\) are linearly dependent. This contradicts the assumption that \\(x\\) was a BFS."
  },
  {
    "objectID": "posts/optimization/linear-programs-and-lp-geometry.html#the-extreme-point-theorem",
    "href": "posts/optimization/linear-programs-and-lp-geometry.html#the-extreme-point-theorem",
    "title": "Linear Programs and LP Geometry",
    "section": "The Extreme Point Theorem",
    "text": "The Extreme Point Theorem\nWhy devote so much time defining extreme points geometrically, and then again algebraically? As hinted earlier and as shall be proved shortly, optima of linear programs occur at the extreme points. This is the reason LP’s are a class of easy convex optimization problems — the search space for their optima can be reduced to a finite number of extreme points.\n\nThe Extreme Point Theorem:   If a linear program has a finite optimum, and its constraint polytope has at least one extreme point, then there is an extreme point which is optimal.\n\nSo, if we want to solve linear programs we need only consider the extreme points.\nLet’s prove the theorem through induction on the dimension.\n\nProof\nTake the following general LP and assume it has a finite optimum. Assume also that its constraint polytope has, at least, one extreme point.\n\\[\n\\begin{cases}\n\\min_{x}: c^Tx\n\\\\\ns.t.: x \\in \\mathcal{P}\n\\end{cases}\n\\]\nAssume the theorem is true for this LP with an \\((n-1)\\)-dimensional constraint polytope. The objective is to show that it’s also true for the same LP with an \\(n\\)-dimensional constraint polytope.\nLet \\(v\\) be the optimal value of the LP.\nLet \\(Q = P \\cap \\{ x : c^Tx = v \\}\\) be the intersection of the constraint polytope with the level set of the objective function at the optimal value.\nSince \\(Q\\) is the intersection of an \\(n\\)-dimensional polytope \\(P\\) with an additional linear constraint (a hyperplane), it is \\((n-1)\\)-dimensional.\nBy the inductive hypothesis, there is an extreme point \\(x^* \\in Q\\) that’s optimal for the LP.\nBy a contradiction argument, \\(x^*\\) is also an extreme point in \\(P\\).\nSuppose it is not an extreme point in \\(P\\). Then by Definition 1 of extreme point, \\(x^*\\) is a convex combination of two points in \\(P\\). That is, \\(\\exists y,z \\in P\\) s.t. \\(\\lambda y + (1- \\lambda)z = x^*\\) for some \\(\\lambda \\in [0,1].\\)\nBut then \\(\\lambda c^Ty + (1- \\lambda)c^Tz = c^Tx^* = v\\), since \\(x^*\\) is optimal. But the left hand side is a convex combination of scalars, so \\(c^Ty = c^Tz = v\\). This means \\(y,z \\in Q\\), which contradicts the fact that \\(x^*\\) is an extreme point in \\(Q\\).\nHence, \\(x^*\\) must be an extreme point in \\(P\\).\n\n\nSketch for an Alternate Proof\nWe can also prove the Extreme Point Theorem using a recursive argument. Recall that a continuous \\(1\\)-dimensional function \\(f: \\mathcal{D} \\rightarrow \\mathbb{R}\\) on a closed interval \\(\\mathcal{D} \\subset \\mathbb{R}\\) necessarily achieves a min/max either on the endpoints of \\(\\mathcal{D}\\) or somewhere inside. If we additionally stipulate that \\(f\\) is linear, the only possibilities are the endpoints. Extending this logic to linear programs in \\(n\\)-dimensions which have finite optimal solutions, we conclude that the optimal solution cannot occur at any interior point of the constraint polytope \\(\\mathcal{P}\\) and, instead, must occur somewhere on its boundary. But now we can consider the \\((n-1)\\)-dimensional polytopes forming \\(\\mathcal{P}\\)’s boundary separately and apply the same logic to each one recursively. In the base case, we reach the conclusion that the optimal solution must occur at an endpoint of a \\(1\\)-dimensional polytope — a line segment such as \\(\\mathcal{D}\\). Such a point is an extreme point of the constraint polytope."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html",
    "href": "posts/leetcode/lc11_container_with_most_water.html",
    "title": "LC11 - Container with Most Water",
    "section": "",
    "text": "We are given an integer array height of length n. Each element of the array corresponds to a vertical line (of n total lines) drawn from the horizontal axis such that the i-th line’s two endpoints are (i, 0) and (i, height[i]) (see the figure below).\nFind two lines which, together with the x-axis, form a container which can hold the most water. Return the maximum amount of water this container can store.\nExample\n\n\n\nContainer with most water\n\n\nInput: height = [1,8,6,2,5,4,8,3,7]\nOutput: 49\nExplanation\nThe above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of the water (highlighted section) the container can contain is 49 (in units of area)."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#optimal-substructure",
    "href": "posts/leetcode/lc11_container_with_most_water.html#optimal-substructure",
    "title": "LC11 - Container with Most Water",
    "section": "Optimal Substructure",
    "text": "Optimal Substructure\n\nLet \\(h(i)\\) denote the height of the \\(i\\)-th vertical line.\nLet \\(a(i,j)\\) denote the area of the container formed by the pair of vertical lines \\((i,j)\\).\nLet \\(maxArea(i,j)\\) denote the maximum area formed by the lines \\({i,...,j}\\) – that is the output of the procedure on the subarray height[i:j].\n\nSuppose, without loss of generality, \\(h(1) \\leq h(n)\\) (that is, the first line is shorter than the last). Then, the problem has top-down optimal substructure:\n \\[maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\] \n\n\n\n\n\n\nNote\n\n\n\n\n\nThis optimal substructure also exposes an obvious dynamic programming approach to the problem (we simply consider the sub-problems of sizes \\(n-1\\), \\(n-2\\), and so on). However, in order to minimize the complexity, we offer a tabulation approach instead — the Two Pointer Solution.\n\n\n\n\nProof of Optimal Substructure\nFor the initial pair \\((1,n)\\) where \\(h(1) \\leq h(n)\\) we have \\(a(1,n) &gt; a(1,k)  \\ \\ \\forall k\\). This is because we’re starting out from the widest container formed by \\({(1,n)}\\) and considering containers of decreasing width formed by the pairs \\({(1, n-1), (1, n-2), ..., (1,2)}\\).\nIn case \\({h(k) &gt; h(1)}\\) for some \\({n \\geq k &gt; 1}\\) the area of the container formed by \\({(1,k)}\\) is still determined by \\({h(1)}\\), except now it’s less wide. Whereas if \\({h(k) &lt; h(1)}\\) the area of the container decreases not only in width but also in height.\nIn both cases we have \\({a(1,n) &gt; a(1,k)}\\) which means in general \\({a(1,n) &gt; a(1, k) \\ \\ \\forall k}\\).\nTherefore, we may omit the first vertical line from consideration and consider the subproblem on the indices \\({2,...,n}\\). The overall optimal solution will then be \\(maxArea(1,n) = max\\{a(1,n), maxArea(2,n)\\}\\) as was the claim."
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#the-two-pointers-algorithm",
    "href": "posts/leetcode/lc11_container_with_most_water.html#the-two-pointers-algorithm",
    "title": "LC11 - Container with Most Water",
    "section": "The Two Pointers Algorithm",
    "text": "The Two Pointers Algorithm\nAt this point, the Two Pointers Algorithm is almost trivially easy to implement:\n\nInitialize two pointers, ‘left’ and ‘right’, at the first and last index respectively.\nWhile the pointers do not intersect:\n\nCalculate the area of the container formed by the pointers and determine if it’s the maximum area encountered so far\nKeep the position of the pointer of the vertical line that’s longer fixed\nAdvance the pointer of vertical line that’s shorter towards the fixed pointer"
  },
  {
    "objectID": "posts/leetcode/lc11_container_with_most_water.html#the-code",
    "href": "posts/leetcode/lc11_container_with_most_water.html#the-code",
    "title": "LC11 - Container with Most Water",
    "section": "The Code",
    "text": "The Code\n\ndef maxArea(height) -&gt; int:\n    i, j = 0, len(height) - 1\n    water = 0\n    while i &lt; j:\n        water = max(water, (j - i) * min(height[i], height[j]))\n        if height[i] &lt; height[j]:\n            i += 1\n        else:\n            j -= 1\n    return water\n\n# Example input\nheights = [1,8,6,2,5,4,8,3,7]\nprint(f'The container with most water has area: {maxArea(heights)}')\n\nThe container with most water has area: 49"
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html",
    "title": "LC121 and LC53 - Kadane’s Algorithm",
    "section": "",
    "text": "We are given an array of prices where prices[i] is the price of a given stock on the i-th day.\nWe want to maximize our profit by choosing a single day to buy one stock and choosing a different day in the future to sell that stock. Return the maximum profit you can achieve from this transaction. If you cannot achieve any profit, return 0.\nExample 1\nInput: prices = [7,1,5,3,6,4]\nOutput: 5\nExplanation\nBuy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Note that buying on day 2 and selling on day 1 is not allowed because you must buy before you sell\nExample 2:\nInput: prices = [7,6,4,3,1]\nOutput: 0\nExplanation\nIn this case, no transactions are done and the max profit = 0.\n\n\nConsider each viable pair of days. It’s easy to see that this leads to time complexity \\(O(n^2)\\) because, for each possible day i that we choose to buy the stock on, there are \\(n-i\\) possible days that we can sell it on. Since there are \\(n\\) choices for which day to buy, the number of total pairs has a leading term of \\(n(n-1)\\), so it’s quadratic in \\(n\\).\nWe can also think of choosing a subset of size \\(2\\) and discarding those which have a reverse order of days. This essentially means choosing a subset of size \\(2\\) without order (since each pair is either in the correct order or not, and we only count the one that is), so \\({O \\left ({n \\choose 2} \\right )}\\) which is, of course, \\(O(n^2)\\).\n\n\n\nWe can solve this problem in a single pass, achieving \\(O(n)\\) complexity by using DP with tabulation similar to Kadane’s algorithm which solves the Maximum Subarray problem. The similarities between these two problems are due to fact that both are concerned with some score over a contiguous array (contiguity being what gives rise to the optimal substructure). Whereas Kadane’s is concerned with the contiguous subarray with maximum sum, this algorithm is interested in the maximum profit (which is the difference between the last element of the optimal subarray and the first one). Like Kadane’s algorithm, we can prove its correctness using loop invariants. We will give the solution and prove the correctness. As for building intuition for why this solution works, we will focus on the Maximum Subarray problem which is a more general application of this type of pattern.\n\n\n\ndef maxProfit(prices):\n    min_price = float(\"inf\") # +infinity\n    max_profit = 0 \n\n    for i in range(len(prices)):\n        if prices[i] &lt; min_price:\n            min_price = prices[i]\n        elif prices[i] - min_price &gt; max_profit:\n            max_profit = prices[i] - min_price\n\n    return max_profit\n\nprices = [7,1,5,3,6,4]\nprint(maxProfit(prices))\n\n5\n\n\n\n\n\n\nIt’s easy to see that, at the end of each iteration i, the variable min_price holds the lowest price dip in the stock up to, and including, the index i. This is called a loop invariant. Showing something is a loop invariant is a lot like proving the inductive step in a proof by induction (for example, when trying to prove recursive algorithms).\nIt can also be shown that max_profit is another loop invariant, and that it holds the maximum profit up to, and including, the index i.\n\n\nThe loop does one of two things exclusively: either it updates min_price or it doesn’t.\nSuppose it doesn’t update min_price (case 1). The first loop invariant, min_price holds the lowest price dip up to, and including, the index i. In this case, the difference of prices[i] and min_price is then calculated and max_profit is updated only if the difference is greater than the max_profit at the end of the previous iteration (iteration i-1). This guarantees that max_profit holds the maximum profit up to, and including, the index i.\nIn the other case (case 2), when the loop does update min_price] at iteration i, it enters the subsequent iteration i+1 with max_profit still holding the maximum profit up to, and including, index i (at the beginning of the iteration). If prices[i+1] is, again, less than min_price then the loop just goes on updating min_price only until it encounters the lowest price dip (unless the price keeps dipping until the very end, in which case the proof is complete). At this iteration of the loop, let’s call it k+1, max_profit still holds the maximum profit up to, and including, index k (because there have just been consecutive dips in price since index i). Since prices[k+1] is the lowest price dip, by assumption, in the next iteration we are necessarily in the familiar again (case 1). Hence, max_profit is a loop invariant.\nTherefore, once the loop is finished, max_profit will hold the maximum profit up to, and including, the last index n. In other words, it will hold the solution to the problem.\n\n\n\n\nAt the heart of Kadane’s algorithm is an optimal substructure which lends itself to optimization using the principles of dynamic programming.\nAny subarray ends (or begins, but for now let’s stick with ends) at some index. We may also define a notion such as this: let there be a solution to the problem up to some index i. Le’t call that solution global_max(i). This is, effectively, the final solution to the problem were the problem to have size i (i.e. if nums had size i).\nA relationship we may initially notice when looking at this problem is that:\nlocal_max[i] = max(local_max[i-1] + nums[i], nums[i])\nWhere local_max[i] is the maximum of all subarrays ending at index i but, crucially, not the solution of the problem up to index i (that’s global_max[i]). It’s easy to get lost in this problem by conflating these two variables which track everything we need to implement a single-pass tabulated solution. They’re also, as it turns out, loop invariants.\nFeel free to tinker with the problem to notice this optimal substructure. It’s often helpful to illustrate the arrays…\nThe final answer is the maximum over all the local maxima (obviously).\nThinking in terms of solutions to sub-problems is, in essence, what dynamic programming is. It’s a way to cut down on the number of subarrays considered in the brute force approach by coming up with a computationally cheap rule that gives us the solution to the current problem based on the retrieved value of a previously solved sub-problem (whatever the relationship between the problem and its sub-problems may be). In the case of the Maximum Subarray problem the sub-problems are expressed in terms of local_max[i] and global_max[i]. If we know local_max[i-1], local_max[i] is all but known through the above relationship and a similar relationship also exists for global_max.\nIf we work through an example, the act itself will give us insight into the implementation of the single pass, iterative algorithm. Note that the recursive approach is already somewhat betrayed by the optimal substructure and, to further make it dynamic, we could implement memoization as a layer above it. Let’s take the first step, and then generalize.\nIn the beginning, there’s just nums[0]. By virtue of being the only subarray that ends at index 0 it is its own local_max[0]. In situations like this we initialize local_max to \\(-\\infty\\) in order not to resort to handling special cases, like singleton arrays, in the loop.\nlocal_max = float('-inf')\n\nfor i, num in enumerate(nums):\n    if num &gt; local_max:\n        local_max = num\nIt’s immediately clear that, since local_max is initialized to \\(-\\infty\\) (let’s call this local_max[-1] to stay consistent with array notation of the optimal substructure), num &gt; local_max is equivalent to:\nnums[0] &gt; local_max[-1] + nums[0]\nThis is true in general for any num[0]. So the desired condition is actually:\nlocal_max = float('-inf')\nglobal_max = \n\nfor i, num in enumerate(nums):\n    if num &gt; local_max + num:\n        local_max = num\n    else\n        local_max = local_max + num\nAs it is, the loop invariant local_max will contain the local maximum at the last element of the nums array. We need to figure out what to do with the other loop invariant global_max which is supposed to be the solution to the problem of size i. Well, global_max is clearly just:\nglobal_max[i] = max(global_max[i-1], local_max[i])\nWhich is the promised greedy-choice update for global_max – the loop invariant which will hold the final solution to the problem.\nWe need to implement this relationship just as we implemented the previous one unless we just use the built in max function to essentially just update both loop invariants in sequence and be done with it. But let’s go with the more imperative version. Let’s take the first step.\nIn the beginning there’s just nums[0]. Clearly global_max[0] equals nums[0] as that’s what local_max[0] is and global_max[-1] (as per our earlier abuse of notation) is \\(-\\infty\\) by choice.\nlocal_max = float('-inf')\nglobal_max = float('-inf')\n\nfor i, num in enumerate(nums):\n    if num &gt; local_max + num:\n        local_max = num\n    else\n        local_max = local_max + num\n    if global_max &lt; local_max:\n        global_max = local_max\nWe can reference local_max and assume its value to be as the promised invariant on line 9.\nNotice that global_max has two meanings, as does local_max in the implementation. It is used as the previous iterate global_max[i-1] (in the Boolean comparison) as well as the current (or next, depending on frame of reference) iterate global_max[i] (in the assignment operation)."
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#brute-force-solution",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#brute-force-solution",
    "title": "LC121 and LC53 - Kadane’s Algorithm",
    "section": "",
    "text": "Consider each viable pair of days. It’s easy to see that this leads to time complexity \\(O(n^2)\\) because, for each possible day i that we choose to buy the stock on, there are \\(n-i\\) possible days that we can sell it on. Since there are \\(n\\) choices for which day to buy, the number of total pairs has a leading term of \\(n(n-1)\\), so it’s quadratic in \\(n\\).\nWe can also think of choosing a subset of size \\(2\\) and discarding those which have a reverse order of days. This essentially means choosing a subset of size \\(2\\) without order (since each pair is either in the correct order or not, and we only count the one that is), so \\({O \\left ({n \\choose 2} \\right )}\\) which is, of course, \\(O(n^2)\\)."
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#non-brute-force-solution",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#non-brute-force-solution",
    "title": "LC121 and LC53 - Kadane’s Algorithm",
    "section": "",
    "text": "We can solve this problem in a single pass, achieving \\(O(n)\\) complexity by using DP with tabulation similar to Kadane’s algorithm which solves the Maximum Subarray problem. The similarities between these two problems are due to fact that both are concerned with some score over a contiguous array (contiguity being what gives rise to the optimal substructure). Whereas Kadane’s is concerned with the contiguous subarray with maximum sum, this algorithm is interested in the maximum profit (which is the difference between the last element of the optimal subarray and the first one). Like Kadane’s algorithm, we can prove its correctness using loop invariants. We will give the solution and prove the correctness. As for building intuition for why this solution works, we will focus on the Maximum Subarray problem which is a more general application of this type of pattern.\n\n\n\ndef maxProfit(prices):\n    min_price = float(\"inf\") # +infinity\n    max_profit = 0 \n\n    for i in range(len(prices)):\n        if prices[i] &lt; min_price:\n            min_price = prices[i]\n        elif prices[i] - min_price &gt; max_profit:\n            max_profit = prices[i] - min_price\n\n    return max_profit\n\nprices = [7,1,5,3,6,4]\nprint(maxProfit(prices))\n\n5"
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#proof-of-correctness",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#proof-of-correctness",
    "title": "LC121 and LC53 - Kadane’s Algorithm",
    "section": "",
    "text": "It’s easy to see that, at the end of each iteration i, the variable min_price holds the lowest price dip in the stock up to, and including, the index i. This is called a loop invariant. Showing something is a loop invariant is a lot like proving the inductive step in a proof by induction (for example, when trying to prove recursive algorithms).\nIt can also be shown that max_profit is another loop invariant, and that it holds the maximum profit up to, and including, the index i.\n\n\nThe loop does one of two things exclusively: either it updates min_price or it doesn’t.\nSuppose it doesn’t update min_price (case 1). The first loop invariant, min_price holds the lowest price dip up to, and including, the index i. In this case, the difference of prices[i] and min_price is then calculated and max_profit is updated only if the difference is greater than the max_profit at the end of the previous iteration (iteration i-1). This guarantees that max_profit holds the maximum profit up to, and including, the index i.\nIn the other case (case 2), when the loop does update min_price] at iteration i, it enters the subsequent iteration i+1 with max_profit still holding the maximum profit up to, and including, index i (at the beginning of the iteration). If prices[i+1] is, again, less than min_price then the loop just goes on updating min_price only until it encounters the lowest price dip (unless the price keeps dipping until the very end, in which case the proof is complete). At this iteration of the loop, let’s call it k+1, max_profit still holds the maximum profit up to, and including, index k (because there have just been consecutive dips in price since index i). Since prices[k+1] is the lowest price dip, by assumption, in the next iteration we are necessarily in the familiar again (case 1). Hence, max_profit is a loop invariant.\nTherefore, once the loop is finished, max_profit will hold the maximum profit up to, and including, the last index n. In other words, it will hold the solution to the problem."
  },
  {
    "objectID": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#kadanes-algorithm---maximum-subarray",
    "href": "posts/leetcode/lc121_best_time_to_buy_and_sell_stock.html#kadanes-algorithm---maximum-subarray",
    "title": "LC121 and LC53 - Kadane’s Algorithm",
    "section": "",
    "text": "At the heart of Kadane’s algorithm is an optimal substructure which lends itself to optimization using the principles of dynamic programming.\nAny subarray ends (or begins, but for now let’s stick with ends) at some index. We may also define a notion such as this: let there be a solution to the problem up to some index i. Le’t call that solution global_max(i). This is, effectively, the final solution to the problem were the problem to have size i (i.e. if nums had size i).\nA relationship we may initially notice when looking at this problem is that:\nlocal_max[i] = max(local_max[i-1] + nums[i], nums[i])\nWhere local_max[i] is the maximum of all subarrays ending at index i but, crucially, not the solution of the problem up to index i (that’s global_max[i]). It’s easy to get lost in this problem by conflating these two variables which track everything we need to implement a single-pass tabulated solution. They’re also, as it turns out, loop invariants.\nFeel free to tinker with the problem to notice this optimal substructure. It’s often helpful to illustrate the arrays…\nThe final answer is the maximum over all the local maxima (obviously).\nThinking in terms of solutions to sub-problems is, in essence, what dynamic programming is. It’s a way to cut down on the number of subarrays considered in the brute force approach by coming up with a computationally cheap rule that gives us the solution to the current problem based on the retrieved value of a previously solved sub-problem (whatever the relationship between the problem and its sub-problems may be). In the case of the Maximum Subarray problem the sub-problems are expressed in terms of local_max[i] and global_max[i]. If we know local_max[i-1], local_max[i] is all but known through the above relationship and a similar relationship also exists for global_max.\nIf we work through an example, the act itself will give us insight into the implementation of the single pass, iterative algorithm. Note that the recursive approach is already somewhat betrayed by the optimal substructure and, to further make it dynamic, we could implement memoization as a layer above it. Let’s take the first step, and then generalize.\nIn the beginning, there’s just nums[0]. By virtue of being the only subarray that ends at index 0 it is its own local_max[0]. In situations like this we initialize local_max to \\(-\\infty\\) in order not to resort to handling special cases, like singleton arrays, in the loop.\nlocal_max = float('-inf')\n\nfor i, num in enumerate(nums):\n    if num &gt; local_max:\n        local_max = num\nIt’s immediately clear that, since local_max is initialized to \\(-\\infty\\) (let’s call this local_max[-1] to stay consistent with array notation of the optimal substructure), num &gt; local_max is equivalent to:\nnums[0] &gt; local_max[-1] + nums[0]\nThis is true in general for any num[0]. So the desired condition is actually:\nlocal_max = float('-inf')\nglobal_max = \n\nfor i, num in enumerate(nums):\n    if num &gt; local_max + num:\n        local_max = num\n    else\n        local_max = local_max + num\nAs it is, the loop invariant local_max will contain the local maximum at the last element of the nums array. We need to figure out what to do with the other loop invariant global_max which is supposed to be the solution to the problem of size i. Well, global_max is clearly just:\nglobal_max[i] = max(global_max[i-1], local_max[i])\nWhich is the promised greedy-choice update for global_max – the loop invariant which will hold the final solution to the problem.\nWe need to implement this relationship just as we implemented the previous one unless we just use the built in max function to essentially just update both loop invariants in sequence and be done with it. But let’s go with the more imperative version. Let’s take the first step.\nIn the beginning there’s just nums[0]. Clearly global_max[0] equals nums[0] as that’s what local_max[0] is and global_max[-1] (as per our earlier abuse of notation) is \\(-\\infty\\) by choice.\nlocal_max = float('-inf')\nglobal_max = float('-inf')\n\nfor i, num in enumerate(nums):\n    if num &gt; local_max + num:\n        local_max = num\n    else\n        local_max = local_max + num\n    if global_max &lt; local_max:\n        global_max = local_max\nWe can reference local_max and assume its value to be as the promised invariant on line 9.\nNotice that global_max has two meanings, as does local_max in the implementation. It is used as the previous iterate global_max[i-1] (in the Boolean comparison) as well as the current (or next, depending on frame of reference) iterate global_max[i] (in the assignment operation)."
  },
  {
    "objectID": "posts/game_development/unreal_engine_first_steps.html",
    "href": "posts/game_development/unreal_engine_first_steps.html",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time. You will need to download the latest version of Xcode.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local\nFollow the README to compile and run the Unreal Editor. The Editor uses a version of the Unreal Engine which can be downloaded by running a few installation scripts in the UnrealEngine project that download the engine and game files.\n\n\n\n\nWe can also install Unreal Engine through the Unreal Launcher.\n\n\n\nBefore starting a project, make sure to:\n\nLoad static content\nEnable raytracing\n\nFeel free to pick a template project from the project browser, such as a first-person or a third-person game. Once the new project opens a .uproject file is created in the default ~/Documents/Unreal Projects directory. We can click this file directly to open the project to continue where we left it off in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut\nFunction\nOptional explanation\n\n\n\n\nRMB\nRotate camera\n\n\n\nRMB + W or A/S/D\nMovement front/left/back/right\n\n\n\nRMB + E or Q\nMovement up or down\n\n\n\nScroll Wheel Up/Down + RMB\nAdjust camera speed\n\n\n\nLMB\nSelect object in world\n\n\n\nQ\nObject selection mode\n\n\n\nW\nMovement translation mode\n\n\n\nE\nRotation translation mode\n\n\n\nR\nScale translation mode\n\n\n\nDel\nDelete selected object\n\n\n\nH\nHide selected object\n\n\n\nCtrl + D\nDuplicate selected object\nAnother shortcut is holding Opt and dragging the selected object\n\n\nHold Shift + LMB\nSelect multiple objects\n\n\n\nHold Ctrl + LMB\nDeselect multiple objects from a multi-object selection\n\n\n\nF\nFocus on selected object\nIf we’re lost in our scene we can always select an object in the Outliner window and press F to re-focus on the object\n\n\nG\nGame view mode\nHides widgets, see the world as if in-game\n\n\nCmd + F11\nImmersive mode\nHides all windows and shows the world in full screen (note: we may also need to press Fn)\n\n\nCtrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nOpen Content Drawer\nThe Content Drawer is what contains all of our in-game assets (3D/2D assets, code, etc.)\n\n\nOpt + P\nPlay game\n\n\n\nEsc\nExit game and go back to Edit mode\n\n\n\nShift + 1 or 2/3/…\nChange Unreal Editor mode\nThe default is Selection Mode. Other modes include Landsaping Mode, Foliage Mode, etc. These bring up various tools to edit landscapes and make foliage (as their names suggest)\n\n\nHold L + LMB\nRotate sun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView mode\nFunction\nOptional explanation\n\n\n\n\nWireframe\nShows only the wireframe mesh\n\n\n\nUnlit\nShows the world disregarding lighting information\nUseful for visibility when editing in dark environments\n\n\nLight only\nThe world with lighting alone (i.e. without material information)\n\n\n\n\n\n\n\nThe translation modes and snap tools are available in the top-right corner of the viewport.\n\n\n\n\n\n\nFigure 1: Translation and snap tools\n\n\n\nFrom left to right, we have:\n\nObject selection mode\nTranslation mode\nRotation mode\nScale mode\nToggle for coordinate system (global vs local) relative to which the transform gizmo is shown\nSurface snapping options (controls how objects snap to existing surfaces)\nTranslation snapping\nRotation snapping\nScale snapping\nCamera speed adjustment\nMinimize/maximize current viewport\n\nThe default viewport includes a grid, this can be disabled in Show menu in the top-left corner of the viewport by un-checking Grid.\n\n\n\nThere are two ways to add objects to the scene. The first is through Add Object in the top-left corner.\nClick \nThe second way is by using the Content Drawer in the bottom-left corner.\nClick  or use shortcut Ctrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nThe latter method is for adding custom game objects (blueprints as well as 2D/3D assets and more).\nWe often need to see where a particular game asset is located in the Content Drawer. The shortcut to find any object inside the Content Drawer is Ctrl + B while having the object selected.\nAll the assets that make up our game, such as custom 3D assets, custom blueprints (code), etc are stored within the /Content folder of the project.\n\n\n\n\n\nThe Details window looks as follows.\n\n\n\n\n\n\nFigure 2: Details window\n\n\n\nIt contains all the details about a selected object such as:\n\nTransformation information (coordinates, angle of rotation, and scale)\nMaterial information\nPhysics and many more…\n\n\n\n\nThe Outliner window looks as follows.\n\n\n\n\n\n\nFigure 3: Outliner window\n\n\n\nIt contains all the objects that make up the scene, as well as options to show/hide, save, and pin objects:\n\n\n\n\nAssuming we’ve selected the third person template, go into the Content Browser and go to /Content/ThirdPerson/Blueprints to access the blueprints of the third person character.\n\n\n\n\n\n\nFigure 4: Content Drawer - Third person blueprints\n\n\n\nClick on the humanoid. Feel free to dock the newly opened window (as a new tab) next to the open viewport in the Unreal Editor.\nA blueprint consists of three views:\n\nEventGraph\nConstruction Script\nViewport\n\n\n\nContains most of the blueprint’s logic.\n\n\n\n\n\n\nFigure 5: Blueprints - Event graph\n\n\n\n\n\n\nMore on this later…\n\n\n\n\n\n\n\n\n\nFigure 6: Blueprints - Viewport\n\n\n\nThis view shows everything within the selected blueprint as an object. Objects in a blueprint are called components. The Components window is the equivalent of the Outliner window but for blueprints. Rather than showing every object in the world, it shows every component in the blueprint.\n\n\n\n\n\n\nFigure 7: Blueprints - Components window\n\n\n\nThe My Blueprint window contains all the nodes and variables contained in the blueprint. We can click on the EventGraph in the Graphs tab of this window to go into the EventGraph view directly.\n\n\n\n\n\n\nFigure 8: Blueprints - My Blueprint window"
  },
  {
    "objectID": "posts/game_development/unreal_engine_first_steps.html#option-1-cloning-unrealengine-project-from-github",
    "href": "posts/game_development/unreal_engine_first_steps.html#option-1-cloning-unrealengine-project-from-github",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "The following steps are meant broadly as the specific instructions might change over time. You will need to download the latest version of Xcode.\n\nNavigate to the Unreal Engine website.\nRegister an account.\nRequest access to the GitHub org\nFork the UnrealEngine repo and clone to local\nFollow the README to compile and run the Unreal Editor. The Editor uses a version of the Unreal Engine which can be downloaded by running a few installation scripts in the UnrealEngine project that download the engine and game files."
  },
  {
    "objectID": "posts/game_development/unreal_engine_first_steps.html#option-2-install-using-unreal-launcher",
    "href": "posts/game_development/unreal_engine_first_steps.html#option-2-install-using-unreal-launcher",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "We can also install Unreal Engine through the Unreal Launcher."
  },
  {
    "objectID": "posts/game_development/unreal_engine_first_steps.html#starting-a-project",
    "href": "posts/game_development/unreal_engine_first_steps.html#starting-a-project",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "Before starting a project, make sure to:\n\nLoad static content\nEnable raytracing\n\nFeel free to pick a template project from the project browser, such as a first-person or a third-person game. Once the new project opens a .uproject file is created in the default ~/Documents/Unreal Projects directory. We can click this file directly to open the project to continue where we left it off in the future."
  },
  {
    "objectID": "posts/game_development/unreal_engine_first_steps.html#unreal-cheat-sheet",
    "href": "posts/game_development/unreal_engine_first_steps.html#unreal-cheat-sheet",
    "title": "Unreal Engine - First Steps",
    "section": "",
    "text": "Shortcut\nFunction\nOptional explanation\n\n\n\n\nRMB\nRotate camera\n\n\n\nRMB + W or A/S/D\nMovement front/left/back/right\n\n\n\nRMB + E or Q\nMovement up or down\n\n\n\nScroll Wheel Up/Down + RMB\nAdjust camera speed\n\n\n\nLMB\nSelect object in world\n\n\n\nQ\nObject selection mode\n\n\n\nW\nMovement translation mode\n\n\n\nE\nRotation translation mode\n\n\n\nR\nScale translation mode\n\n\n\nDel\nDelete selected object\n\n\n\nH\nHide selected object\n\n\n\nCtrl + D\nDuplicate selected object\nAnother shortcut is holding Opt and dragging the selected object\n\n\nHold Shift + LMB\nSelect multiple objects\n\n\n\nHold Ctrl + LMB\nDeselect multiple objects from a multi-object selection\n\n\n\nF\nFocus on selected object\nIf we’re lost in our scene we can always select an object in the Outliner window and press F to re-focus on the object\n\n\nG\nGame view mode\nHides widgets, see the world as if in-game\n\n\nCmd + F11\nImmersive mode\nHides all windows and shows the world in full screen (note: we may also need to press Fn)\n\n\nCtrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nOpen Content Drawer\nThe Content Drawer is what contains all of our in-game assets (3D/2D assets, code, etc.)\n\n\nOpt + P\nPlay game\n\n\n\nEsc\nExit game and go back to Edit mode\n\n\n\nShift + 1 or 2/3/…\nChange Unreal Editor mode\nThe default is Selection Mode. Other modes include Landsaping Mode, Foliage Mode, etc. These bring up various tools to edit landscapes and make foliage (as their names suggest)\n\n\nHold L + LMB\nRotate sun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView mode\nFunction\nOptional explanation\n\n\n\n\nWireframe\nShows only the wireframe mesh\n\n\n\nUnlit\nShows the world disregarding lighting information\nUseful for visibility when editing in dark environments\n\n\nLight only\nThe world with lighting alone (i.e. without material information)\n\n\n\n\n\n\n\nThe translation modes and snap tools are available in the top-right corner of the viewport.\n\n\n\n\n\n\nFigure 1: Translation and snap tools\n\n\n\nFrom left to right, we have:\n\nObject selection mode\nTranslation mode\nRotation mode\nScale mode\nToggle for coordinate system (global vs local) relative to which the transform gizmo is shown\nSurface snapping options (controls how objects snap to existing surfaces)\nTranslation snapping\nRotation snapping\nScale snapping\nCamera speed adjustment\nMinimize/maximize current viewport\n\nThe default viewport includes a grid, this can be disabled in Show menu in the top-left corner of the viewport by un-checking Grid.\n\n\n\nThere are two ways to add objects to the scene. The first is through Add Object in the top-left corner.\nClick \nThe second way is by using the Content Drawer in the bottom-left corner.\nClick  or use shortcut Ctrl + ‎ ‎ ‎ ‎ Space ‎ ‎ ‎ ‎\nThe latter method is for adding custom game objects (blueprints as well as 2D/3D assets and more).\nWe often need to see where a particular game asset is located in the Content Drawer. The shortcut to find any object inside the Content Drawer is Ctrl + B while having the object selected.\nAll the assets that make up our game, such as custom 3D assets, custom blueprints (code), etc are stored within the /Content folder of the project.\n\n\n\n\n\nThe Details window looks as follows.\n\n\n\n\n\n\nFigure 2: Details window\n\n\n\nIt contains all the details about a selected object such as:\n\nTransformation information (coordinates, angle of rotation, and scale)\nMaterial information\nPhysics and many more…\n\n\n\n\nThe Outliner window looks as follows.\n\n\n\n\n\n\nFigure 3: Outliner window\n\n\n\nIt contains all the objects that make up the scene, as well as options to show/hide, save, and pin objects:\n\n\n\n\nAssuming we’ve selected the third person template, go into the Content Browser and go to /Content/ThirdPerson/Blueprints to access the blueprints of the third person character.\n\n\n\n\n\n\nFigure 4: Content Drawer - Third person blueprints\n\n\n\nClick on the humanoid. Feel free to dock the newly opened window (as a new tab) next to the open viewport in the Unreal Editor.\nA blueprint consists of three views:\n\nEventGraph\nConstruction Script\nViewport\n\n\n\nContains most of the blueprint’s logic.\n\n\n\n\n\n\nFigure 5: Blueprints - Event graph\n\n\n\n\n\n\nMore on this later…\n\n\n\n\n\n\n\n\n\nFigure 6: Blueprints - Viewport\n\n\n\nThis view shows everything within the selected blueprint as an object. Objects in a blueprint are called components. The Components window is the equivalent of the Outliner window but for blueprints. Rather than showing every object in the world, it shows every component in the blueprint.\n\n\n\n\n\n\nFigure 7: Blueprints - Components window\n\n\n\nThe My Blueprint window contains all the nodes and variables contained in the blueprint. We can click on the EventGraph in the Graphs tab of this window to go into the EventGraph view directly.\n\n\n\n\n\n\nFigure 8: Blueprints - My Blueprint window"
  },
  {
    "objectID": "posts/leetcode/connected_sinks_and_sources.html",
    "href": "posts/leetcode/connected_sinks_and_sources.html",
    "title": "Connected Sinks and Sources",
    "section": "",
    "text": "We are given a pipe system represented by a 2D rectangular grid of cells. There are three different types of items located in the cells within the grid, with each having either no Items or 1 Item:\n\nSource: There is 1 source in the system. It is represented by the asterisk character *.\nSinks: There are an arbitrary number of sinks in the system. They are each represented by a different uppercase letter (A, B, C, etc.).\nPipes: There are 10 different shapes of pipes, represented by the following characters: ═, ║, ╔, ╗, ╚, ╝, ╠, ╣, ╦, ╩\n\nNote that:\n\nEach pipe has openings on 2 or 3 sides of its cell.\nTwo adjacent cells are connected if both have a pipe opening at their shared edge.\nWe should treat the source and sinks as having pipe openings at all of their edges. For example, the two cells A╦ are connected through their shared edge, but the two cells B╔ are not.\nA sink may be connected to the source through another sink. For example, in the simple pipe system *A═B=C, all three sinks are connected to the source.\n\nOur objective is to write a function that determines which sinks are connected to the source in a given pipe system.\nAs an example, consider the following pipe system:\n*╗ ╦═A\n ╠═╝\n C ╚═B\nIn this pipe system, the source * is connected to sinks A and C but not B.\nSuch a system is specified by an input text file in the following format.\n*02\nC10\n╠11\n╗12\n═21\n╚30\n╝31\n╦32\n═40\n═42\nB50\nA52\nNote that each item is followed by its coordinate (the origin of the coordinate system is taken to be the lower left corner (0,0) corresponding to an empty space in this system).\n\n\n\nLet’s define the inputs and outputs of this program.\n\nInput: File path as a string\nOutput: The sinks connected to the source as a string\n\nFor example, for the given pipe system in the example, the solution should be AC.\nLet’s define a black box:\n\ndef get_connected_sinks(filePath: str) -&gt; str | None:\n    \"\"\"\n    Returns sinks connected to the source.\n\n    Parameters:\n    - filePath (str): Path to the input file which describes the pipe system.\n\n    Returns:\n    - str: A string containing the connected sinks,\n    \"\"\"\n    pass\n\nIn the black box above, we use the union operator | to allow the function to return None (so that we don’t have to worry about the implementation and return value for this black box implementation).\nNow that we have get_connected_sinks which takes the input file path as filePath, we can start thinking about what this function should do. Here’s a broad breakdown of the function.\n\nLoad the text file\nParse the input\nUsing parsed input, determine which sinks are connected to the source and return the answer\n\nNot that step 3 is already black-boxed by get_connected_sinks.\nLet’s create sub-functions for each of the remaining tasks:\n\nload_file: should read the file (as a string or a related representation) into the program\nparse_input: should return a programmatic representation of the pipe system\n\nWe can write load_file without much deliberation, as it’ll just be a simple wrapper for file.readLines which returns an array of strings, each containing one line from the file.\n\ndef load_file(filePath: str) -&gt; list[str]:\n    \"\"\"\n    Loads the input file and returns the contents as a list of strings.\n\n    Parameters:\n    - filePath (str): Path to the input file which describes the pipe system.\n\n    Returns:\n    - list: A list of strings containing the contents of the input file.\n    \"\"\"\n\n    with open(filePath, 'r') as file:\n        return file.readlines()\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use the keyword with rather than try-finally to leverage built in file IO safety features.\n\n\n\nLet’s save the output of load_file as the input we’ll provide to parse_input. Running load_file for the example file above gives:\n\ninput = load_file(\"./problem_assets/connected_sinks_and_sources/example.txt\")\nprint(input)\n\n['*02\\n', 'C10\\n', '╠11\\n', '╗12\\n', '═21\\n', '╚30\\n', '╝31\\n', '╦32\\n', '═40\\n', '═42\\n', 'B50\\n', 'A52']\n\n\nAs we can see the lines include the newline character, let’s clean up input by using map and rstrip.\n\ninput = list(map(lambda x: x.rstrip(\"\\n\"), input))\nprint(input)\n\n['*02', 'C10', '╠11', '╗12', '═21', '╚30', '╝31', '╦32', '═40', '═42', 'B50', 'A52']\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that we need to use list with map because Python 3’s map works by lazy as opposed to strict evaluation (which means that the Python interpreter only evaluates values one at a time, as opposed to all at once which is the case when evaluating a simple expression like, say, sum=1+2)\n\n\n\nBut we’re still dealing with simple string representations. That means it’s not immediately clear how to obtain the open edges of, say, the ╚ character programmatically (much less). Let’s parse this list down into a data structure we will call Item.\nItem will have Boolean instance variables:\n\nleft\nright\ntop\ndown\n\nWhich will indicate whether the corresponding edge is open or closed (True or False respectively).\nIt will also have instance variables x and y for the item’s coordinates. An additional type instance variable with possible values in \\(\\{\\)Source,Pipe,Sink\\(\\}\\) may be helpful for checking terminal conditions.\n\nclass Item:\n    def __init__(\n            self, type: str = \"Pipe\", \n            edges: list[bool] = [True, True, True, True], \n            coords: list[int] = [0,0]\n        ):\n        self.type = type\n        self.edges = edges\n        self.coords = coords\n    \n    def __repr__(self): # String representation of object for logging\n        return (f\"-----{type(self).__name__}-----\\n\"\n                f\"  type: {self.type}\\n\"\n                f\"  edges: {self.edges}\\n\"\n                f\"  coordinates: {self.coords}\\n\")\n\nUnfortunately, there’s no way to get around the hard-coding of the ASCII pipe to Item mappings without adding unmerited complexity to the problem.\nHere’s an implementation of mapToItems with the hard-coding mentioned above. Feel free to expand and examine the implementation (at your own peril).\n\n\nCode\ndef map_to_items(input: list[str]) -&gt; list[Item]: # Converts the input to a list of Items\n    def to_item(line: str) -&gt; Item: # Converts a single line in the input to an Item\n        objectToReturn = Item()\n        objectToReturn.coords = line[1:] # The coordinates are the rest of the line\n        objectToReturn.edges = [True, True, True, True]\n        match line[0]: # Match the first character of the line to determine the type of object\n            case \"*\":\n                # The default object is a Pipe at coordinates (0,0), so nothing else needs tp be done...\n                objectToReturn.type = \"Source\"\n            case \"═\":\n                # Note: edges are ordered [top, right, down, left]\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[2] = False\n            case \"║\":\n                objectToReturn.edges[1] = False\n                objectToReturn.edges[3] = False\n            case \"╔\":\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[3] = False\n            case \"╗\":\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[1] = False\n            case \"╚\":\n                objectToReturn.edges[2] = False\n                objectToReturn.edges[3] = False\n            case \"╝\":\n                objectToReturn.edges[2] = False\n                objectToReturn.edges[1] = False\n            case \"╠\":\n                objectToReturn.edges[3] = False\n            case \"╣\":\n                objectToReturn.edges[1] = False\n            case \"╦\":\n                objectToReturn.edges[0] = False\n            case \"╩\":\n                objectToReturn.edges[2] = False\n            case other: # The case of a Sink\n                if not other.isalpha(): # Check if the first character is a letter at all...\n                    raise ValueError(\"The first character of a Sink must be a letter.\")\n                objectToReturn.type = \"Sink\"\n        return objectToReturn\n            \n    return list(map(to_item, input))\n\n\nLet’s run input through mapToItems to obtain our programmatic representation of the pipe system. We show only the first five from the output in the interest of brevity.\n\nparsed_input = map_to_items(input)\nfor obj in parsed_input[:5]:\n    print(obj)\n\n-----Item-----\n  type: Source\n  edges: [True, True, True, True]\n  coordinates: 02\n\n-----Item-----\n  type: Sink\n  edges: [True, True, True, True]\n  coordinates: 10\n\n-----Item-----\n  type: Pipe\n  edges: [True, True, True, False]\n  coordinates: 11\n\n-----Item-----\n  type: Pipe\n  edges: [False, False, True, True]\n  coordinates: 12\n\n-----Item-----\n  type: Pipe\n  edges: [False, True, False, True]\n  coordinates: 21\n\n\n\n\n\n\nThis is a path finding problem, so we’re likely going to use either BFS or DFS. Since it’s far easier to represent the given data as a 2D array, we will implement the iterative version of whichever traversal algorithm we end up choosing, as the recursive version is better suited for the case in which the data is easier to represent as a graph.\nHere’s the graph representation of the pipe system above, for equivalency’s sake (although we won’t be using graphs).\n\n\n\n\n\n   graph TD;\n      A[\"*02\"] --&gt; B[\"╗12\"];\n      B--&gt;C[\"╠11\"];\n      C--&gt;D[\"C10\"];\n      C--&gt;E[\"═21\"]\n      E--&gt;F[\"╝31\"]\n      F--&gt;G[\"╦32\"]\n      G--&gt;H[\"═42\"]\n      H--&gt;K[\"A\"]\n      L[\"╚30\"]--&gt;M[\"═40\"]\n      M--&gt;N[\"B\"]\n\n\n\n\n\n\n\nSo we have two disjoint acyclic graphs representing the example pipe system above. Ignore the edge directions (Mermaid doesn’t provide a way to make undirected graphs to my knowledge). This graph will be undirected, as connectedness is a symmetric relationship: i.e. if an item is connected to another, like in the pipe system ═╦, then the other item is certainly also connected to the first.\nIn general, because BFS traverses a graph one level at a time, we tend to use BFS when looking for the shortest path between two nodes. The first time BFS lands on the target node constitutes the shortest path to that node (or one of the multiple such paths if it’s a tie between paths). DFS is better suited for just finding a valid path, or all valid paths (in which case we could throw in backtracking as well). Since we’re interested in a valid path between the source * and any given sink, and not necessarily the shortest such path, we use DFS (possibly with backtracking) for this problem.\nNote that BFS can, just as easily, be implemented on a 2D array as on a graph (graphs are nothing more, really, than adjacency lists). So, let’s see how to implement DFS on a 2D array. But before that, let’s expand the given input into a 2D array of Items.\n\n\nCode\nclass PipeSystem:\n    def __init__(self):\n        pass\n\n\ndef parse_input(input: list[str]) -&gt; PipeSystem:\n    \"\"\"\n    Parses the input and returns the connected sinks.\n\n    Parameters:\n    - input (list): A list of strings containing the contents of the input file.\n\n    Returns:\n    - str: A string containing the connected sinks.\n    \"\"\"\n    pass"
  },
  {
    "objectID": "posts/leetcode/connected_sinks_and_sources.html#problem-statement",
    "href": "posts/leetcode/connected_sinks_and_sources.html#problem-statement",
    "title": "Connected Sinks and Sources",
    "section": "",
    "text": "We are given a pipe system represented by a 2D rectangular grid of cells. There are three different types of items located in the cells within the grid, with each having either no Items or 1 Item:\n\nSource: There is 1 source in the system. It is represented by the asterisk character *.\nSinks: There are an arbitrary number of sinks in the system. They are each represented by a different uppercase letter (A, B, C, etc.).\nPipes: There are 10 different shapes of pipes, represented by the following characters: ═, ║, ╔, ╗, ╚, ╝, ╠, ╣, ╦, ╩\n\nNote that:\n\nEach pipe has openings on 2 or 3 sides of its cell.\nTwo adjacent cells are connected if both have a pipe opening at their shared edge.\nWe should treat the source and sinks as having pipe openings at all of their edges. For example, the two cells A╦ are connected through their shared edge, but the two cells B╔ are not.\nA sink may be connected to the source through another sink. For example, in the simple pipe system *A═B=C, all three sinks are connected to the source.\n\nOur objective is to write a function that determines which sinks are connected to the source in a given pipe system.\nAs an example, consider the following pipe system:\n*╗ ╦═A\n ╠═╝\n C ╚═B\nIn this pipe system, the source * is connected to sinks A and C but not B.\nSuch a system is specified by an input text file in the following format.\n*02\nC10\n╠11\n╗12\n═21\n╚30\n╝31\n╦32\n═40\n═42\nB50\nA52\nNote that each item is followed by its coordinate (the origin of the coordinate system is taken to be the lower left corner (0,0) corresponding to an empty space in this system)."
  },
  {
    "objectID": "posts/leetcode/connected_sinks_and_sources.html#black-boxing-the-solution",
    "href": "posts/leetcode/connected_sinks_and_sources.html#black-boxing-the-solution",
    "title": "Connected Sinks and Sources",
    "section": "",
    "text": "Let’s define the inputs and outputs of this program.\n\nInput: File path as a string\nOutput: The sinks connected to the source as a string\n\nFor example, for the given pipe system in the example, the solution should be AC.\nLet’s define a black box:\n\ndef get_connected_sinks(filePath: str) -&gt; str | None:\n    \"\"\"\n    Returns sinks connected to the source.\n\n    Parameters:\n    - filePath (str): Path to the input file which describes the pipe system.\n\n    Returns:\n    - str: A string containing the connected sinks,\n    \"\"\"\n    pass\n\nIn the black box above, we use the union operator | to allow the function to return None (so that we don’t have to worry about the implementation and return value for this black box implementation).\nNow that we have get_connected_sinks which takes the input file path as filePath, we can start thinking about what this function should do. Here’s a broad breakdown of the function.\n\nLoad the text file\nParse the input\nUsing parsed input, determine which sinks are connected to the source and return the answer\n\nNot that step 3 is already black-boxed by get_connected_sinks.\nLet’s create sub-functions for each of the remaining tasks:\n\nload_file: should read the file (as a string or a related representation) into the program\nparse_input: should return a programmatic representation of the pipe system\n\nWe can write load_file without much deliberation, as it’ll just be a simple wrapper for file.readLines which returns an array of strings, each containing one line from the file.\n\ndef load_file(filePath: str) -&gt; list[str]:\n    \"\"\"\n    Loads the input file and returns the contents as a list of strings.\n\n    Parameters:\n    - filePath (str): Path to the input file which describes the pipe system.\n\n    Returns:\n    - list: A list of strings containing the contents of the input file.\n    \"\"\"\n\n    with open(filePath, 'r') as file:\n        return file.readlines()\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use the keyword with rather than try-finally to leverage built in file IO safety features.\n\n\n\nLet’s save the output of load_file as the input we’ll provide to parse_input. Running load_file for the example file above gives:\n\ninput = load_file(\"./problem_assets/connected_sinks_and_sources/example.txt\")\nprint(input)\n\n['*02\\n', 'C10\\n', '╠11\\n', '╗12\\n', '═21\\n', '╚30\\n', '╝31\\n', '╦32\\n', '═40\\n', '═42\\n', 'B50\\n', 'A52']\n\n\nAs we can see the lines include the newline character, let’s clean up input by using map and rstrip.\n\ninput = list(map(lambda x: x.rstrip(\"\\n\"), input))\nprint(input)\n\n['*02', 'C10', '╠11', '╗12', '═21', '╚30', '╝31', '╦32', '═40', '═42', 'B50', 'A52']\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that we need to use list with map because Python 3’s map works by lazy as opposed to strict evaluation (which means that the Python interpreter only evaluates values one at a time, as opposed to all at once which is the case when evaluating a simple expression like, say, sum=1+2)\n\n\n\nBut we’re still dealing with simple string representations. That means it’s not immediately clear how to obtain the open edges of, say, the ╚ character programmatically (much less). Let’s parse this list down into a data structure we will call Item.\nItem will have Boolean instance variables:\n\nleft\nright\ntop\ndown\n\nWhich will indicate whether the corresponding edge is open or closed (True or False respectively).\nIt will also have instance variables x and y for the item’s coordinates. An additional type instance variable with possible values in \\(\\{\\)Source,Pipe,Sink\\(\\}\\) may be helpful for checking terminal conditions.\n\nclass Item:\n    def __init__(\n            self, type: str = \"Pipe\", \n            edges: list[bool] = [True, True, True, True], \n            coords: list[int] = [0,0]\n        ):\n        self.type = type\n        self.edges = edges\n        self.coords = coords\n    \n    def __repr__(self): # String representation of object for logging\n        return (f\"-----{type(self).__name__}-----\\n\"\n                f\"  type: {self.type}\\n\"\n                f\"  edges: {self.edges}\\n\"\n                f\"  coordinates: {self.coords}\\n\")\n\nUnfortunately, there’s no way to get around the hard-coding of the ASCII pipe to Item mappings without adding unmerited complexity to the problem.\nHere’s an implementation of mapToItems with the hard-coding mentioned above. Feel free to expand and examine the implementation (at your own peril).\n\n\nCode\ndef map_to_items(input: list[str]) -&gt; list[Item]: # Converts the input to a list of Items\n    def to_item(line: str) -&gt; Item: # Converts a single line in the input to an Item\n        objectToReturn = Item()\n        objectToReturn.coords = line[1:] # The coordinates are the rest of the line\n        objectToReturn.edges = [True, True, True, True]\n        match line[0]: # Match the first character of the line to determine the type of object\n            case \"*\":\n                # The default object is a Pipe at coordinates (0,0), so nothing else needs tp be done...\n                objectToReturn.type = \"Source\"\n            case \"═\":\n                # Note: edges are ordered [top, right, down, left]\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[2] = False\n            case \"║\":\n                objectToReturn.edges[1] = False\n                objectToReturn.edges[3] = False\n            case \"╔\":\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[3] = False\n            case \"╗\":\n                objectToReturn.edges[0] = False\n                objectToReturn.edges[1] = False\n            case \"╚\":\n                objectToReturn.edges[2] = False\n                objectToReturn.edges[3] = False\n            case \"╝\":\n                objectToReturn.edges[2] = False\n                objectToReturn.edges[1] = False\n            case \"╠\":\n                objectToReturn.edges[3] = False\n            case \"╣\":\n                objectToReturn.edges[1] = False\n            case \"╦\":\n                objectToReturn.edges[0] = False\n            case \"╩\":\n                objectToReturn.edges[2] = False\n            case other: # The case of a Sink\n                if not other.isalpha(): # Check if the first character is a letter at all...\n                    raise ValueError(\"The first character of a Sink must be a letter.\")\n                objectToReturn.type = \"Sink\"\n        return objectToReturn\n            \n    return list(map(to_item, input))\n\n\nLet’s run input through mapToItems to obtain our programmatic representation of the pipe system. We show only the first five from the output in the interest of brevity.\n\nparsed_input = map_to_items(input)\nfor obj in parsed_input[:5]:\n    print(obj)\n\n-----Item-----\n  type: Source\n  edges: [True, True, True, True]\n  coordinates: 02\n\n-----Item-----\n  type: Sink\n  edges: [True, True, True, True]\n  coordinates: 10\n\n-----Item-----\n  type: Pipe\n  edges: [True, True, True, False]\n  coordinates: 11\n\n-----Item-----\n  type: Pipe\n  edges: [False, False, True, True]\n  coordinates: 12\n\n-----Item-----\n  type: Pipe\n  edges: [False, True, False, True]\n  coordinates: 21"
  },
  {
    "objectID": "posts/leetcode/connected_sinks_and_sources.html#path-finding-dfs",
    "href": "posts/leetcode/connected_sinks_and_sources.html#path-finding-dfs",
    "title": "Connected Sinks and Sources",
    "section": "",
    "text": "This is a path finding problem, so we’re likely going to use either BFS or DFS. Since it’s far easier to represent the given data as a 2D array, we will implement the iterative version of whichever traversal algorithm we end up choosing, as the recursive version is better suited for the case in which the data is easier to represent as a graph.\nHere’s the graph representation of the pipe system above, for equivalency’s sake (although we won’t be using graphs).\n\n\n\n\n\n   graph TD;\n      A[\"*02\"] --&gt; B[\"╗12\"];\n      B--&gt;C[\"╠11\"];\n      C--&gt;D[\"C10\"];\n      C--&gt;E[\"═21\"]\n      E--&gt;F[\"╝31\"]\n      F--&gt;G[\"╦32\"]\n      G--&gt;H[\"═42\"]\n      H--&gt;K[\"A\"]\n      L[\"╚30\"]--&gt;M[\"═40\"]\n      M--&gt;N[\"B\"]\n\n\n\n\n\n\n\nSo we have two disjoint acyclic graphs representing the example pipe system above. Ignore the edge directions (Mermaid doesn’t provide a way to make undirected graphs to my knowledge). This graph will be undirected, as connectedness is a symmetric relationship: i.e. if an item is connected to another, like in the pipe system ═╦, then the other item is certainly also connected to the first.\nIn general, because BFS traverses a graph one level at a time, we tend to use BFS when looking for the shortest path between two nodes. The first time BFS lands on the target node constitutes the shortest path to that node (or one of the multiple such paths if it’s a tie between paths). DFS is better suited for just finding a valid path, or all valid paths (in which case we could throw in backtracking as well). Since we’re interested in a valid path between the source * and any given sink, and not necessarily the shortest such path, we use DFS (possibly with backtracking) for this problem.\nNote that BFS can, just as easily, be implemented on a 2D array as on a graph (graphs are nothing more, really, than adjacency lists). So, let’s see how to implement DFS on a 2D array. But before that, let’s expand the given input into a 2D array of Items.\n\n\nCode\nclass PipeSystem:\n    def __init__(self):\n        pass\n\n\ndef parse_input(input: list[str]) -&gt; PipeSystem:\n    \"\"\"\n    Parses the input and returns the connected sinks.\n\n    Parameters:\n    - input (list): A list of strings containing the contents of the input file.\n\n    Returns:\n    - str: A string containing the connected sinks.\n    \"\"\"\n    pass"
  },
  {
    "objectID": "posts/machine_learning/builing-an-LLM.html",
    "href": "posts/machine_learning/builing-an-LLM.html",
    "title": "Building an LLM",
    "section": "",
    "text": "GPT-3 was trained on 0.5T tokens, today’s leading models are often trained on 5T tokens and above.\n\n\nWrite a script to download the whole internet. The internet is composed of a lot of Wikipedia articles (which are of particular value, as sources that have been referenced from Wikipedia are often given more weight in the final output of the model), forums, books, scientific articles, news articles, code bases, etc.\nFor a taste of web scraping, read about my project for scraping prices of goods and sending notifications of price drops.\n\n\n\n\nCommon Crawl\nThe Pile\nHugging Face Datasets\n\nPrivate datasets are also available, like FinPile (used by BloombergGPT).\n\n\n\nAlpaca.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput is an embedding, this is most useful for classification purposes.\n\n\n\nDoes not see the future. No attention is paid by tokens to future tokens in the sentence. The output is a probability distribution over the entire corpus or, effectively, the next predictable token.\n\n\n\nTransformers are encoder-decoder models.\n\n\n\nAn LLM outputs a probability distributiom over the entire corpus. So, how does such a device work for the explicit task of multiple choice answering, for example? Well, we can tokenize and pass the entire question, along with the multiple choice answers, as raw tokens and hope the model outputs “A,” “B,” “C,” or “D” as the most probable next token. We can, of course, do much better by constraining the choices to those tokens and comparing which of those has the highest probability. We can also template our prompts. We can instruct the LLM to understand that every prompt follows a template of “question” and “answer choices.” There are many options for tweaking our models in these ways as part of pre-training or post-training.\n\n\n\n\n\n\n\n\n\nMultiple-choice tasks: ARC, Hellaswag, MMLU,…\n\n\n\n\n\n\nQuantifies the quality of the output via metrics such as Perplexity, BLEU, or ROGUE scores.\nPerplexity is a measure of how many tokens our LLM was hesitating on choosing as the most probable next token (out of the entire corpus). Fornmally…\nAnother option is using auxillary fine-tuned LLMs which was used in the TruthfulQA paper to compare output to some ground truth."
  },
  {
    "objectID": "posts/machine_learning/builing-an-LLM.html#step-1-data-curation",
    "href": "posts/machine_learning/builing-an-LLM.html#step-1-data-curation",
    "title": "Building an LLM",
    "section": "",
    "text": "GPT-3 was trained on 0.5T tokens, today’s leading models are often trained on 5T tokens and above.\n\n\nWrite a script to download the whole internet. The internet is composed of a lot of Wikipedia articles (which are of particular value, as sources that have been referenced from Wikipedia are often given more weight in the final output of the model), forums, books, scientific articles, news articles, code bases, etc.\nFor a taste of web scraping, read about my project for scraping prices of goods and sending notifications of price drops.\n\n\n\n\nCommon Crawl\nThe Pile\nHugging Face Datasets\n\nPrivate datasets are also available, like FinPile (used by BloombergGPT).\n\n\n\nAlpaca."
  },
  {
    "objectID": "posts/machine_learning/builing-an-LLM.html#step-3-architecture-choices",
    "href": "posts/machine_learning/builing-an-LLM.html#step-3-architecture-choices",
    "title": "Building an LLM",
    "section": "",
    "text": "Output is an embedding, this is most useful for classification purposes.\n\n\n\nDoes not see the future. No attention is paid by tokens to future tokens in the sentence. The output is a probability distribution over the entire corpus or, effectively, the next predictable token.\n\n\n\nTransformers are encoder-decoder models.\n\n\n\nAn LLM outputs a probability distributiom over the entire corpus. So, how does such a device work for the explicit task of multiple choice answering, for example? Well, we can tokenize and pass the entire question, along with the multiple choice answers, as raw tokens and hope the model outputs “A,” “B,” “C,” or “D” as the most probable next token. We can, of course, do much better by constraining the choices to those tokens and comparing which of those has the highest probability. We can also template our prompts. We can instruct the LLM to understand that every prompt follows a template of “question” and “answer choices.” There are many options for tweaking our models in these ways as part of pre-training or post-training."
  },
  {
    "objectID": "posts/machine_learning/builing-an-LLM.html#step-4-evaluation",
    "href": "posts/machine_learning/builing-an-LLM.html#step-4-evaluation",
    "title": "Building an LLM",
    "section": "",
    "text": "Multiple-choice tasks: ARC, Hellaswag, MMLU,…\n\n\n\n\n\n\nQuantifies the quality of the output via metrics such as Perplexity, BLEU, or ROGUE scores.\nPerplexity is a measure of how many tokens our LLM was hesitating on choosing as the most probable next token (out of the entire corpus). Fornmally…\nAnother option is using auxillary fine-tuned LLMs which was used in the TruthfulQA paper to compare output to some ground truth."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "",
    "text": "Gradient Descent (GD) is a powerful, yet incredibly simple, iterative optimization algorithm. We can think of it as a greedy algorithm in the setting of continuous optimization. That is, one step of GD is our best attempt at local optimization given limited information about the objective \\(f(x)\\), and having limited computational power at our disposal.\nWe can further qualify what we mean by ‘limited information’ about the objective by introducing a categorization on optimization algorithms – the Oracle Access Model (OAM). In this model, the objective is abstracted into a black box. For each input \\(x\\), the black box gives the algorithm access to the value of the objective, \\(f(x)\\), and, optionally, global information in the form of higher order behavior such as \\(\\nabla f(x)\\), \\(\\nabla^2 f(x)\\), etc. GD is what’s known as a first-order oracle because it’s only allowed access to \\(f(x)\\) and first-order information in the form of \\(\\nabla f(x)\\).\nIt’s important to note that the OAM is not all-inclusive, there are a number of optimization algorithms, such as composite optimization algorithms, that utilize structural information about the objective that goes beyond \\(n\\)-th order behavior. An example of such an algorithm is Proximal Gradient Descent (PGD) which, in addition to \\(f(x)\\) and \\(\\nabla f(x)\\), also has access to the prox operator: \\(Prox_{h,\\eta}(x)\\).\nWe will cover these composite optimization algorithms in later posts. Many of the composite optimization algorithms, such as PGD, are simple modifications of vanilla GD. The modification done in PGD, for example, make it suitable for constrained optimization. For now, however, we focus on the case of unconstrained optimization with oracles, particularly on Gradient Descent, in order to develop the key algorithmic intuition."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#idea-1---greedy-choice-of-direction",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#idea-1---greedy-choice-of-direction",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Idea 1 - Greedy Choice of Direction",
    "text": "Idea 1 - Greedy Choice of Direction\nLet \\(x\\) be the initial iterate, and let the update be given by:\n\\[x^+ = x + \\eta d\\]\nfor some directional unit-vector \\(d\\) and step-size parameter \\(\\eta &gt; 0\\).\nWe base the algorithm on the assumption that the linear approximation of the objective at a the next iterate \\(x^+\\) is a good-enough estimate of its true value at \\(x^+\\).\nThat is:\n\\[f(x^+) = f(x + \\eta d) \\approx f(x) + \\eta \\nabla f(x)^T d \\ \\ \\forall d \\tag{1.1}\\]\nImmediately, a locally optimal choice presents itself to us. Since we wish to minimize \\(f(x)\\), it would be wise to insist that the objective at \\(x^+\\) improves or, at least, does not worsen.\nThat is, we insist:\n\\[f(x^+) \\approx f(x) + \\eta \\nabla f(x)^T d \\leq f(x) \\tag{1.3}\\]\nAnd, since we are greedy in our approach, we wish to make \\(f(x^+)\\) as small as possible. Since, on the RHS, \\(f(x)\\) is fixed and \\(\\eta &gt; 0\\), this amounts to minimizing the scaled inner-product \\(\\nabla f(x)^Td\\). To that end, we choose \\(d\\) opposite and parallel to the gradient, i.e. \\(d = - \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}\\).\nThe update step becomes:\n\\[x^+ = x - \\eta \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}\\]\nBy re-labeling, \\(\\eta\\) can absorb the normalization constant. This obtains the gradient descent update step as it’s often introduced in the textbooks – a step in the negative gradient direction:\n\\[x^+ = x - \\eta \\nabla f(x) \\tag{1.4}\\]\nThis makes intuitive sense because the negative gradient direction is the direction in which the objective decreases most. So, it’s only natural that the update should take us in this most enticing direction."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#idea-2---greedy-choice-of-next-iterate",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#idea-2---greedy-choice-of-next-iterate",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Idea 2 - Greedy Choice of Next Iterate",
    "text": "Idea 2 - Greedy Choice of Next Iterate\nInstead of defining the update step \\(x^+ = x + \\eta d\\) and then choosing the locally optimal direction \\(d\\) greedily, we can choose the update step and the direction, both, in one fell swoop.\nStarting from the linear approximation:\n\\[\nf(y) \\approx f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y \\tag{2.1}\n\\]\nWe can now insist, in a greedy fashion, that the next iterate \\(x^+\\) be the minimizer of the linear approximation. That is, we insist:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) \\tag{2.2}\n\\]\nBut the linear approximation is only local, so it would be wise to distrust it for points far away from the current iterate. In this case, since the linear approximation is, in fact, unbounded below, \\((2.2)\\) would obtain \\(x^+ = \\pm \\infty\\). To avoid this problem, we introduce a parametrized penalty term that prevents \\(x^+\\) from venturing too far from the current iterate \\(x\\). That is:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y - x) + \\eta ||y - x||_2^2 \\tag{2.3}\n\\]\nNow, since the RHS is a a simple quadratic in \\(y\\), it has a unique minimizer which can be found by using the unconstrained optimality condition. This just boils down to taking the gradient of the RHS w.r.t. the optimization variable \\(y\\), setting it to zero, and then solving for the unique root. This procedure obtains:\n\\[x^+ = x - \\frac{1}{2 \\eta} \\nabla f(x)\\]\nBy re-labeling, we, once again, get the canonical form of the GD update step as in \\((1.3)\\) – a step in the negative gradient direction."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#initialization",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#initialization",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Initialization",
    "text": "Initialization\nFrom this point on, we will limit our discussion to convex objectives in order to eliminate the possibility of strictly local optimizers (i.e. non-global optimizers) and inflection points, both of which GD gets stuck at if initialized poorly. This ensures the only stationary points, points at which \\(\\nabla f(x) = 0\\) and the GD update makes no further progress, are global minimizers. On such convex functions, as we will soon discover, GD has a convergence guarantee for all step-sizes independently of initialization."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#fixed-step-size-gd",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#fixed-step-size-gd",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Fixed Step-Size GD",
    "text": "Fixed Step-Size GD\nTo kickstart our analysis of GD, we consider the fixed step-size algorithm first. Let’s take two quintessential convex problems in \\(\\mathbb{R}\\), \\(f(x) = x^2\\) and \\(h(x) = |x|\\), and analyze GD’s performance on them.\n\nSimple Analysis of Fixed Step-Size GD\n\nFirst, let’s run the algorithm on \\(h(x) = |x|\\) for \\(x \\in \\mathbb{R}\\):\nSince \\(|x|\\) is non-differentiable at \\(x = 0\\), the gradient has a discontinuity at \\(x = 0\\). Non-differentiability, such as this, will eventually force us to introduce the notion of sub-gradients, but for now we can get away with it simply by avoiding the gradient’s behavior at \\(0\\). So:\n\\[\nh'(x) =\n\\begin{cases}\n\\begin{aligned}\n-1 \\ &\\textrm{if $x &lt; 0$} \\\\\n1 \\ &\\textrm{if $x &gt; 0$}\n\\end{aligned}\n\\end{cases}\n\\]\nThen, for a fixed \\(\\eta &gt; 0\\), the update step is:\n\\[x^+ = x \\pm \\eta\\]\nwhere the sign of \\(\\eta\\) depends on where the previous iterate, \\(x\\), falls within the domain \\((-\\infty, 0) \\cup (0, \\infty)\\).\n\nNow, consider \\(f(x) = x^2\\) for \\(x \\in \\mathbb{R}\\):\nThe gradient of \\(f(x) = x^2\\) is \\(f'(x) = 2x\\), which means the fixed step-size update is:\n\\[x^+ = x - 2\\eta x\\]\n\nNote that \\(x^* = 0\\) is the unique optimizer of both \\(f(x)\\) and \\(h(x)\\). With this in mind, there are two key observations to make.\nThe first is that, for \\(x\\) far away from \\(x^* = 0\\), the update, \\(2\\eta x\\), is large (in magnitude). So, if the iterate is far from the optimizer, GD makes fast progress towards it.\nThe second observation is that, as \\(x \\rightarrow x^*\\), the update becomes small in magnitude. So, as the iterate comes close to the optimizer, GD takes smaller and smaller steps which converge to \\(0\\) in a summable way. This means, we can get the sub-optimality \\(f(x) - f(x^*)\\) to be \\(\\epsilon\\)-arbitrarily small for any fixed step-size \\(\\eta\\).\nNeither of these observations hold for GD on \\(h(x) = |x|\\) since the update \\(\\eta\\) is fixed regardless of the Euclidean distance between \\(x\\) and \\(x^* = 0\\). In particular, this means GD is not fast for \\(x\\) far away from \\(x^*\\) and does not slow down as \\(x\\) nears \\(x^*\\). Arbitrary accuracy is impossible with a fixed step-size, since the iterates eventually cycle between \\(x^T - \\eta\\) and \\(x^T + \\eta\\) where \\(x^T\\) is the last unique iterate – that is \\(x^T \\in (-\\eta, \\eta)\\). The sub-optimality, consequently, also cycles between two values which depend on the choice of \\(\\eta\\). This is to say that the sub-optimality cannot be \\(\\epsilon\\)-arbitrary small for a fixed choice of \\(\\eta\\). To be clear, there is still convergence but it’s slow and not arbitrarily accurate. Arbitrary accuracy for such problems as this can only be achieved by choosing a sequence of diminishing step-sizes \\(\\{ \\eta_t \\}_{t=1}^T\\) which reduce magnitude of the update since the gradient, itself, is constant. Of course, this sequence must be chosen with care since it’s possible to ‘run out of steam’, so to speak, before reaching the optimizer. The precise criterion is \\(\\eta_t \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) s.t. \\(\\sum_t^\\infty \\eta_t = \\infty\\).\nWe say GD on \\(f(x) = x^2\\) enjoys the self-tuning property, whereas GD on \\(h(x) = |x|\\) does not. This speaks to the fact that the self-tuning is a property of the objective functions, rather than GD itself.\nAs an overview of the theory we will soon develop, functions like \\(x^2\\) (or, more generally, any quadratic in \\(\\mathbb{R}^n\\)) will all have the self-tuning property while functions like \\(|x|\\) will not. This is what ends up introducing the taxonomy of easier-to-harder convex optimization problems mentioned in the previous section. What it means, precisely, to be like \\(x^2\\) or \\(|x|\\) will be made rigorous in the next few sections."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#self-tuning-property",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#self-tuning-property",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Self-Tuning Property",
    "text": "Self-Tuning Property\nFor a convex function that’s \\(M\\)-smooth and \\(m\\)-strongly-convex we have \\((3.1)\\) which, as a reminder, is:\n\\[m||x-y||_2 \\leq ||\\nabla f(x) - \\nabla f(y)||_2 \\leq M||x-y||_2 \\ \\ \\forall x,y\\]\nFixing iterate \\(x\\), and replacing \\(y\\) with the optimizer \\(x^*\\) we have:\n\\[m||x-x^*||_2 \\leq ||\\nabla f(x) - \\nabla f(x^*)||_2 \\leq M||x-x^*||_2\\]\nSince \\(x^*\\) is an optimizer \\(\\nabla f(x^*) = 0\\), so the above becomes:\n\\[m||x-x^*||_2 \\leq ||\\nabla f(x)||_2 \\leq M||x-x^*||_2\\]\nThe first inequality says that the magnitude of the gradient is at least a constant multiple of the distance from the optimizer. The second inequality says that the magnitude of the gradient is at most a constant multiple of the distance from the optimizer. So, the gradient is large for \\(x\\) far from \\(x^*\\) and gets smaller as \\(x \\rightarrow x^*\\). Since the GD update depends on the magnitude of the gradient, this ensures GD has the self-tuning property. So, smoothness and strong-convexity were, indeed, the ideas needed to encapsulate the self-tuning property."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#quadratic-bounds",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#quadratic-bounds",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Quadratic Bounds",
    "text": "Quadratic Bounds\nSmoothness and strong-convexity, should they hold for a given convex function, give a universal quadratic point-wise upper and lower-bound, respectively, on the function. This is what it means to say that the function is like a quadratic. In a sense, all we’re saying is that a function is tightly asymptotically bounded by a quadratic at every point. That is, at any given point, the function should neither grow slower nor faster than quadratically.\nTo construct the upper and lower-bounds, we use the following two lemmas:\n\nLemma 1:   If \\(f\\) is convex and \\(L\\)-Lipschitz then \\(g(x) = \\frac{L}{2}x^Tx - f(x)\\) is convex.\n\n\nLemma 2:   If \\(f\\) is \\(m\\)-strongly-convex then \\(g(x) = f(x) - \\frac{m}{2}x^Tx\\) is convex.\n\nBoth lemmas are statements of comparative convexity in disguise. Lemma 1 says that \\(\\frac{L}{2}x^Tx\\) is more convex than \\(f\\), whereas Lemma 2 says that \\(f\\) is more convex than \\(\\frac{m}{2}x^Tx\\).\nIt’s not difficult to see how these lemmas can assist in sandwiching \\(f\\) between an upper-bound that’s more convex and a lower-bound that’s less convex.\nThe bounds themselves come from the quadratic approximation of \\(f\\) as:\n\\[f(y) \\approx f(x) + \\nabla f(x)^T(y-x) + \\frac{1}{2}(y-x)^T\\nabla^2 f(x)(y-x)\\ \\ \\forall y\\]\nBy replacing the hessian with its bounds \\(mI\\) and \\(MI\\) and using the above lemmas we obtain the two bounds as:\n\\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{m}{2}||y-x||_2^2 \\ \\ \\forall y\\] \\[\\textrm{and} \\tag {5.1}\\] \\[f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2 \\ \\ \\forall y\\]"
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#the-optimal-fixed-step-size",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#the-optimal-fixed-step-size",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "The Optimal Fixed Step-Size",
    "text": "The Optimal Fixed Step-Size\nWe already showed that an \\(M\\)-smooth and \\(m\\)-strongly-convex function enjoys the advantage of self-tuning. But, without knowing how to choose the step-size, we can still cause GD to make slow progress or even diverge.\nAfter all, gradient descent is based on a local linear approximation of the objective. If we take big steps, we are counting on the linear approximation to be a good-enough estimate far from the current iterate. This may be too optimistic, in which case GD will diverge. Being too pessimistic, however, is also not good. While taking small steps won’t cause GD to diverge, it will make GD painfully slow… Slow to the point of making it worthless in practice.\nLuckily, the quadratic upper-bound in \\((5.1)\\) informs our choice of step-size both in terms of a convergence guarantee and in terms of optimality. First, let’s develop the convergence guarantee.\nBy plugging the update step \\(x^+ = x - \\eta \\nabla f(x)\\) as \\(y\\) into the upper-bound in \\((5.1)\\) we obtain:\n\\[f(x^+) \\leq f(x) + \\eta(1-\\frac{M}{2}\\eta)||\\nabla f(x)||_2^2 \\tag{5.2}\\]\nAs before, it would be wise to insist \\(f(x^+) \\leq f(x)\\), which gives the convergence interval as \\(0 &lt; \\eta &lt; \\frac{2}{M}\\).\nHere, it helps to consider the simple example of quadratics.\n\nExample 1:\nConsider the quadratic form in \\(\\mathbb{R}\\) given by \\(f(x) = \\frac{1}{2}M x^2\\) where \\(x, M \\in \\mathbb{R}\\). Here we can think of \\(M\\) as the only, and therefore the largest, eigenvalue of the \\(1 \\times 1\\) matrix \\([M]\\).\nIts GD update step looks like:\n\\[x^+ = x - \\eta M x = (1 - \\eta M)x\\]\nWhich, by recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[x^T = (1- \\eta M)^Tx_0\\]\nThen, since \\(x^* = 0\\), convergence is guaranteed by ensuring \\(|1 - \\eta M| &lt; 1\\) or, equivalently, \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) as desired.\nThis generalizes to higher dimensions as we shall see in the following example.\n\nExample 2:\nConsider the quadratic form in \\(\\mathbb{R}^n\\) given by \\(f(x) = \\frac{1}{2}x^TQx\\) where \\(x \\in \\mathbb{R}^n\\) and \\(Q \\succeq 0\\).\nIts GD update step looks like:\n\\[x^+ = x - \\eta Qx = (I - \\eta Q)x\\]\nWhich, by recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[x^T = \\underbrace{(I - \\eta Q)\\ldots(I - \\eta Q)}_{\\text{$T$ times}}x_0\\]\nThe eigenvalues \\(\\tilde \\lambda_i\\) of the matrix \\(I - \\eta Q\\) are related to the eigenvalues \\(\\lambda_i\\) of \\(Q\\) according to:\n\\[\\tilde \\lambda_i = 1 - \\eta \\lambda_i\\]\nHence, if \\(\\lambda_{min} = m\\) and \\(\\lambda_{max} = M\\), then \\(\\tilde \\lambda_{min} = 1 - \\eta M\\) and \\(\\tilde \\lambda_{max} =1 - \\eta m\\).\nThe eigenvalues of \\(I - \\eta Q\\) act on the current iterate by scaling it. So, in order to ensure convergence to \\(0\\), we need \\(\\tilde \\lambda_i \\in (-1,1) \\ \\ \\forall i\\). Or, equivalently:\n\\[\\tilde \\lambda_{min} &gt; -1\\] \\[\\textrm{and}\\] \\[\\tilde \\lambda_{max} &lt; 1\\]\nBoth of these together give us \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) as desired.\n\nBut \\(0 &lt; \\eta &lt; \\frac{2}{M}\\) is an interval, not a greedy choice. It’s just a condition that guarantees convergence. By making the greedy choice we can find the optimal step-size within this interval.\nSince the RHS of \\((5.2)\\) is strongly convex, the quadratic upper-bound is guaranteed to have a unique minimizer.\nThe idea is similar to other instances of making a greedy choice we’ve seen so far. Since the function value is upper-bounded by this quadratic, minimizing this upper-bound gives the best guarantee of smallness for the function value available to us without access to higher order information about the objective. So, the greedy choice for the next iterate is:\n\\[\nx^+ = \\arg \\min_y f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2}||y-x||_2^2\n\\]\nAs always, minimizing a quadratic is easy. After going through the motions we obtain:\n\\[x^+ = x - \\frac{1}{M}\\nabla f(x)\\]\nSo, the optimal fixed step-size is \\(\\eta = \\frac{1}{M}\\).\nWe will see this idea of minimizing a quadratic approximation of the objective, instead of the objective itself, repeat itself when we get to Newton’s Method (NM). However, NM is a second-oder oracle which has access to \\(\\nabla^2 f(x)\\), and hence the quadratic approximation at all points in the domain of the objective is accessible to NM. The remarkable thing about \\(M\\)-smoothness and \\(m\\)-strong-convexity is that they give gradient descent, a first-order oracle, access to universal quadratic bounds which eliminates the need to know \\(\\nabla^2 f(x)\\). These upper-bounds are, as we saw, what inform GD’s choice of step-size which ends up guaranteeing convergence."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#convergence-rate",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#convergence-rate",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Convergence Rate",
    "text": "Convergence Rate\nAs shown above, the theoretical best fixed step-size for an \\(M\\)-smooth objective \\(f(x)\\) is \\(\\eta = \\frac{1}{M}\\). With this choice of step-size, we can derive convergence guarantees for GD as well as its convergence rate.\n\n\\(M\\)-Smooth Objectives\nFixing the current iterate as \\(x\\), and plugging in \\(x^+ = x - \\frac{1}{M} \\nabla f(x)\\) into the upper-bound in \\((5.1)\\), we obtain the quadratic upper-bound on the next iterate in terms of the magnitude of the gradient:\n\\[f(x^+) \\leq f(x) - \\frac{1}{2M}||\\nabla f(x)||_2^2 \\tag{5.3}\\]\nFurthermore, since the underlying assumption throughout this post is that the objective is convex, we have a linear lower-bound \\(\\forall y\\), and particularly at the optimizer \\(y = x^*\\), as:\n\\[f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) \\tag{5.4}\\]\nBy reversing \\((5.4)\\) and adding it to \\((5.3)\\) we get:\n\\[f(x^+) \\leq f(x^*) + \\nabla f(x)^T(x - x^*) - \\frac{1}{2M}||\\nabla f(x)||_2^2\\]\nWith a bit of algebraic finessing, we can bring the above to the form:\n\\[f(x^+) \\leq f(x^*) + \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x - \\frac{1}{M}\\nabla f(x) - x^*||_2^2\\right]\\]\nBut \\(x - \\frac{1}{M}\\nabla f(x) = x^+\\), so we have:\n\\[f(x^+) - f(x^*) \\leq \\frac{M}{2}\\left[ ||x-x^*||_2^2 - ||x^+ - x^*||_2^2\\right]\\]\nWe recognize, \\(||x^+ - x^*||_2^2\\) as the sub-optimality of the next iterate, and \\(||x - x^*||_2^2\\) as the sub-optimality of the previous iterate. When both sides are summed over \\(T\\) iterations, the RHS sum telescopes and we’re left with:\n\\[\\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2}||x_0 - x^*||_2^2\\]\nThe LHS is the sum of sub-optimalities across all \\(T\\) iterations, and the RHS is a quantity that’s proportional to the initial condition. Taking the average error across all iterations, we get:\n\\[\\frac{1}{T} \\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2\\]\nBut, we know that the algorithm with fixed step-size \\(\\eta = \\frac{1}{M}\\) has the descent property since \\(0 &lt; \\frac{1}{M} &lt; \\frac{2}{M}\\). So, the last sub-optimality \\(f(x_{T}) - f(x^*)\\) must be the smallest. In particular, it must be smaller than the average. So, we have:\n\\[f(x_{T}) - f(x^*) \\leq \\frac{M}{2T}||x_0 - x^*||_2^2 \\tag{5.5}\\]\nSo, the error gets better with more iterations and, conversely, gets worse the larger \\(M\\), a measure of how abruptly the gradient changes anywhere on its domain, is.\nNote that \\(M\\), as well as \\(||x_0 - x^*||_2^2\\) are fixed in \\((5.5)\\). So, the convergence rate of GD on an \\(M\\)-smooth objective is \\(O(\\frac{1}{T})\\).\n\n\n\\(M\\)-Smooth and \\(m\\)-Strongly-Convex Objectives\nThe situation is drastically better if, in addition to \\(M\\)-smoothness, we also have \\(m\\)-strong-convexity. Not only do we get a much faster convergence rate, we also guarantee convergence in the iterates themselves. Note that, so far, we’ve discussed sub-optimality in objective values only. That is, the only convergence guarantee we’ve seen so far is \\(f(x^T) \\rightarrow f(x^*)\\) as \\(T \\rightarrow \\infty\\). Sometimes more is needed, we may actually want convergence of the iterates themselves. That is, we may want \\(x^T \\rightarrow x^*\\) as \\(T \\rightarrow \\infty\\)? Since strong convexity guarantees the existence of a unique optimizer \\(x^*\\), we can discuss this type of sub-optimality for \\(m\\)-strongly-convex objectives.\nIn this scenario, we have the analog of \\((5.3)\\) for the quadratic lower-bound.\nJust as \\(x - \\frac{1}{M} \\nabla f(x)\\) minimized the quadratic upper-bound, \\(x - \\frac{1}{m} \\nabla f(x)\\) minimizes the quadratic lower-bound. Plugging it into the lower-bound, we get \\((5.3)\\)’s analog as:\n\\[f(y) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\ \\ \\forall y \\tag{5.6}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis result is stronger than its analog \\((5.3)\\) because it holds \\(\\forall y\\) as opposed to \\((5.3)\\) which is only guaranteed to hold at the minimizer \\(x^+ = x - \\frac{1}{M}\\nabla f(x)\\). This is expected because \\((5.6)\\) is the result of minimizing a universal lower-bound as opposed to \\((5.3)\\) which is the result of minimizing a universal upper-bound.\n\n\n\nSince \\((5.6)\\) holds \\(\\forall y\\), it holds, in particular, at the optimizer \\(y=x^*\\):\n\\[f(x^*) \\geq f(x) - \\frac{1}{2m}||\\nabla f(x)||_2^2 \\tag{5.7}\\]\nWe can now solve for \\(||\\nabla f(x)||_2^2\\) in \\((5.7)\\) and plug the result into \\((5.3)\\). From \\((5.7)\\), we get:\n\\[||\\nabla f(x)||_2^2 \\geq 2m(f(x)-f(x^*))\\]\nWhich, when plugged into \\((5.3)\\), gives:\n\\[f(x^+) - f(x^*) \\leq \\left(1-\\frac{m}{M}\\right)(f(x)-f(x^*))\\]\nWe recognize the LHS as the sub-optimality at the next iteration, and the RHS as the sub-optimality at the current iteration. Recursion from iteration \\(T\\) down to the initial iteration, gives:\n\\[f(x_T) - f(x^*) \\leq  \\left(1-\\frac{m}{M}\\right)^T (f(x_0) - f(x^*)) \\tag{5.8}\\]\nAnd, since \\(m \\leq M\\) and both strictly positive, \\(0 &lt; \\frac{m}{M} \\leq 1\\) which guarantees convergence.\nNote that \\(m\\), \\(M\\), and the initial sub-optimality \\(f(x_0) - f(x^*)\\) are fixed quantities in \\((5.8)\\). So, the convergence rate of GD on a smooth and strongly-convex objective is \\(O(c^{-T})\\) for the constant \\(c^{-1} = 1 - \\frac{m}{M}\\). That is, the error decreases exponentially in the number of iterations. However, historically, mathematicians were concerned with the logarithm of the error, rather than the error itself, and hence this type of convergence is known as linear convergence.\nAs promised, we also have convergence of the iterates themselves. From the quadratic lower-bound in \\((5.1)\\) we can derive an upper-bound on \\(||x - x^*||_2\\) as follows. As before, letting \\(y = x^*\\) in the quadratic lower-bound gives us:\n\\[f(x^*) \\geq f(x) + \\nabla f(x)^T(x^* - x) + \\frac{m}{2}||x^* - x||_2^2\\]\nBut, by the Cauchy-Schwarz Inequality, we further have:\n\\[f(x^*) \\geq f(x) - ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2\\]\nBut, since \\(f(x^*) \\leq f(x)\\) by the optimality of \\(x^*\\), we must have:\n\\[- ||\\nabla f(x)||_2||(x^* - x)||_2 + \\frac{m}{2}||x^* - x||_2^2 \\leq 0\\]\nFrom which it follows that:\n\\[||x - x^*||_2 \\leq \\frac{2}{m}||\\nabla f(x)||_2 \\tag{5.9}\\]\nAs GD converges to \\(f(x^*)\\), \\(\\nabla f(x) \\rightarrow \\nabla f(x^*) = 0\\) where the last equality is by optimality of \\(x^*\\). So, \\(x \\rightarrow x^*\\).\n\n\nAffine Invariance\nNot only does \\((5.8)\\) give the rate of convergence of GD, it also predicts its performance on objectives with roughly spherical vs roughly elliptical level-sets. These are the level-sets of what’s referred to as badly and well conditioned objectives respectively.\nTo say that an objective is \\(M\\)-smooth and \\(m\\)-strongly convex is to say \\((3.2)\\) which, as a reminder, is:\n\\[mI \\preceq \\nabla^2 f(x) \\preceq MI \\ \\ \\forall x\\]\nThis implies that all the eigenvalues of the hessian are bounded between \\(m\\) and \\(M\\). The eigenvalues of the hessian represent the stretch of the level sets in the principal directions. So, to say that \\(\\frac{m}{M} \\approx 1\\) is to say that \\(m \\approx M\\), which means that the level-sets are not more or less stretched in any particular direction. This implies that the level-sets are roughly spherical. As we can see from \\((5.8)\\), GD converges quite fast in such cases since \\(\\left( 1 - \\frac{m}{M} \\right)\\), the factor of decrease, is small. The opposite is true in the case when the level sets are elongated.\nThis brings us to an important property called affine invariance which GD lacks. Simply put, an affine transformation of the input space (i.e. a mere change of coordinates/basis) may affect GD’s performance drastically.\nIt helps to look at an example where the objective is a simple quadratic in \\(\\mathbb{R}^n\\).\n\nTake the quadratic objective \\(f(x) = \\frac{1}{2}x^TQx\\) where \\(x \\in \\mathbb{R}^n\\) is a coordinate vector in the standard basis. Now, consider the change of coordinates from the standard basis to a basis \\(Z\\) given by \\(Az = x\\) where \\(A\\) is a matrix whose column vectors are the basis vectors in \\(Z\\).\nThe key observation is that we can choose \\(A\\) in such a way as to make the objective in the new coordinate system have more or less elliptical level-sets which would affect GD’s performance.\nFirst, let’s come up with the same quadratic in \\(Z\\)-coordinates.\n\\[f(x) = f(Az) = \\frac{1}{2}(Az)^TQAz = z^TA^TQAz\\]\nLet \\(\\tilde Q = A^TQA\\) and define the quadratic in \\(Z\\)-coordinates as \\(\\tilde f(z) = \\frac{1}{2}z^T \\tilde Q z\\) so that \\(\\tilde f(z) = f(x)\\) for all \\(z\\) in the \\(Z\\)-coordinates corresponding to \\(x\\) in the standard basis. In particular, optimizing in either coordinate system yields the same result in terms of an optimum, that is \\(\\tilde f(z^*) = f(x^*)\\).\nThe GD factor of decrease on \\(\\tilde f\\) is \\(\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)\\), where \\(\\tilde m = \\lambda_{\\min}(\\tilde Q)\\) and \\(\\tilde M = \\lambda_{\\max}(\\tilde Q)\\). But \\(\\left(1 - \\frac{\\tilde m}{\\tilde M} \\right)\\) is under no obligation to be equal to \\(\\left(1 - \\frac{m}{M} \\right)\\), the GD factor of decrease in the standard basis, proving that GD is not affine invariant.\nAnother perspective on affine invariance comes from comparing the GD update steps in both spaces.\nThe GD update in the standard basis is:\n\\[x^+ = x - \\eta \\nabla f(x) = x - \\eta Qx\\]\nWhereas the GD update in \\(Z\\)-coordinates is:\n\\[z^+ = z - \\eta \\nabla \\tilde f(z) = z - \\eta \\tilde Qz\\]\nTo go from \\(Z\\)-coordinates back to the standard basis, we apply \\(A\\) to the LHS which necessitates its application to the RHS as well. We obtain:\n\\[Az^+ = Az - \\eta A\\tilde Qz = x - \\eta AA^TQx\\]\nWhich is not the same as \\(x - \\eta Qx\\). So, although \\(Az = x\\) the linear relationship breaks down for the next iterates produced by GD (that is \\(Az^+ \\ne x^+\\)). So, doing a step of GD in the standard basis it’s not the same as doing a step of GD in \\(Z\\)-coordinates (up to a change of basis by \\(A\\)). So, gradient descent is doing something radically different in the \\(Z\\)-coordinates compared to what it does in the standard basis.\n\n\nBest Affine Transformation\nA natural question to ask, at this point, is which choice of \\(A\\) makes GD perform faster on a quadratic objective?\nAlgebraically, the best we can hope for is \\(A\\) s.t. \\(\\tilde m \\approx \\tilde M\\). One way we can accomplish this is by forcing all of the eigenvalues of \\(\\tilde Q\\) to be the same. Particularly, letting them all be \\(1\\) by enforcing \\(\\tilde Q = A^TQA = I\\) works.\nSince \\(Q \\succeq 0\\), it has an eigendecomposition as \\(Q = PDP^T\\) where \\(D\\) is diagonal and \\(P\\) is orthonormal. Then its matrix square root exists and is given by \\(Q^{-1/2} = PD^{-1/2}P^T\\). Letting \\(A = Q^{-1/2}\\) we, indeed, have:\n\\[\n\\begin{aligned}\nA^TQA &= (PD^{-1/2}P^T)^TPDP^TPD^{-1/2}P^T \\\\\n&= PD^{-1/2}DD^{-1/2}P^T \\\\\n&= PP^T \\\\\n&= I \\\\\n\\end{aligned}\\]\nWhere we have used the fact that \\(P^TP = PP^T = I\\) since \\(P\\) is orthonormal, and the fact that diagonal matrices are raised to a power simply by raising their diagonal entries to that power.\nGeometrically, the choice of \\(\\tilde Q = I\\) forces the level-sets to be spherical. A level-set of \\(f(x) = \\frac{1}{2}x^TQx\\) in the standard coordinate system, i.e. an ellipse in \\(\\mathbb{R}^n\\), is given by \\(x^TQx = c\\) for some constant \\(c\\) which has absorbed \\(\\frac{1}{2}\\). In the \\(Z\\)-coordinates, the same level-set is given by \\(\\tilde f(z) = z^T\\tilde Qz = c\\). If \\(\\tilde Q = I\\), as is the case for the choice \\(A = Q^{-1/2}\\), then the level-set in the \\(Z\\)-coordinates becomes \\(z^Tz = c\\) which is, indeed, a sphere in \\(\\mathbb{R}^n\\).\nIn conclusion, if the objective is quadratic, we can improve GD’s convergence rate by applying the above change of basis. If the objective is not quadratic, we may still assume that it’s locally quadratic. This allows us to apply the same idea to non-quadratic objectives, but, since it requires coming up with a different matrix \\(A\\) at each iteration, the payoff from this procedure may not be worth it. So, we explore other ways of accelerating the performance of gradient descent on badly conditioned objectives by developing Accelerated Gradient Descent (AGD) which will be explained shortly."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#exact-line-search-els",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#exact-line-search-els",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Exact Line Search (ELS)",
    "text": "Exact Line Search (ELS)\nLet’s go back to the general iterative update step \\(x^+ = x + \\eta d\\).\nBy restricting the objective in the direction of the update \\(d\\) we can find the optimal step-size, \\(\\eta^*\\), at each iteration by solving the following one-dimensional, unconstrained optimization problem in \\(\\eta\\):\n\\[\\eta^* = \\arg \\min_{\\eta} f(x + \\eta d) \\tag{6.1}\\]\nWe proceed with the optimization by defining the restriction of \\(f\\) in the direction \\(d\\) as \\(\\phi(\\eta) := f(x + \\eta d)\\). Then, we can use the chain-rule to find the stationary point \\(\\eta^*\\) for which \\(\\nabla \\phi(\\eta^*) = 0\\).\nBy chain-rule:\n\\[\\nabla \\phi(\\eta) = \\nabla f(x + \\eta d)^T d \\tag{6.2}\\]\nIn the case of GD, \\(d = -\\nabla f(x)\\), so we have:\n\\[\\nabla \\phi(\\eta) = \\nabla f(x - \\eta \\nabla f(x))^T (-\\nabla f(x)) \\tag{6.3}\\]\nAn interesting geometric consequence of this is that GD with ELS takes perpendicular steps that end at a point of tangency with a level-set. Setting \\(\\nabla \\phi(\\eta) = 0\\) to find the optimal step-size obtains \\(\\eta^*\\) s.t. \\(\\nabla f(x - \\eta^* \\nabla f(x))\\) is perpendicular to \\(- \\nabla f(x)\\). In other words, GD with ELS goes in the negative gradient direction until the gradient at \\(x - \\eta^* \\nabla f(x)\\) is perpendicular to the gradient at the current iterate \\(x\\). At the next iteration, GD will take a step in the \\(- \\nabla f(x - \\eta^* \\nabla f(x))\\) direction which is still perpendicular to \\(- \\nabla f(x)\\). This means, at each iteration, GD takes a step that’s perpendicular to the step it took in the previous iteration. Furthermore, since gradients are always perpendicular to level-sets, the new iterate \\(x - \\eta^* \\nabla f(x)\\) is a point of tangency with the level-set at that point.\nAlthough this subroutine is very natural, it may be infeasible to solve an optimization problem (even a one-dimensional one) at each iteration. Hence, we introduce backtracking line search."
  },
  {
    "objectID": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#backtracking-line-search-btls",
    "href": "posts/optimization/algorithms-for-unconstrained-optimization-gradient-descent.html#backtracking-line-search-btls",
    "title": "Algorithms for Unconstrained Optimization",
    "section": "Backtracking Line Search (BTLS)",
    "text": "Backtracking Line Search (BTLS)\nAs we know a convex objective \\(f\\) is always lower-bounded by its linear approximation. That is:\n\\[f(y) \\geq f(x) + \\nabla f(x)^T(y - x) \\ \\ \\forall y\\]\nPlugging \\(x^+ = x + \\eta d\\) into the above lower-bound obtains:\n\\[f(x^+) \\geq f(x) + \\eta \\nabla f(x)^T d\\]\nSo, the greatest possible reduction in value from \\(f(x)\\) to \\(f(x^+)\\) we can hope for is \\(\\eta \\nabla f(x)^T d\\) which, recall, is non-positive by a choice of \\(d\\) that guarantees descent (such as \\(d = -\\nabla f(x)\\) in gradient descent). Unless the objective is linear, in which case the above linear approximation holds with equality, we can not hope to achieve the full \\(\\eta \\nabla f(x)^T d\\) reduction in value. This is just a consequence of convexity and is, therefore, also the case when using exact line search.\nThe idea behind backtracking line search is to ensure we achieve, at least, a fraction of this maximum reduction in value by introducing a parameter \\(0 &lt; \\alpha &lt; 1\\).\nSince \\(f(x) + \\eta \\nabla f(x)^T d\\), the linear underestimate of \\(f(x^+)\\), is the tangent line to \\(f\\) at \\(x\\) in the direction \\(d\\), \\(f(x) + \\alpha \\eta \\nabla f(x)^T d\\) (for \\(0 &lt; \\alpha &lt; 1\\)) is a secant line of \\(f\\) at \\(x\\) in the direction \\(d\\). Setting \\(\\alpha = \\frac{1}{2}\\), a typical choice in practice, and finding the largest \\(\\eta\\) s.t. \\(f(x^+) = (x + \\eta d)\\) is still below this secant line ensures we get approximately half of the maximum reduction in value \\(\\eta \\nabla f(x)^T d\\).\nThis is exactly what the BTLS subroutine, detailed below, tries to achieve:\n\nThe BTLS Subroutine\n\nThe BTLS subroutine takes as input \\(0 &lt; \\alpha &lt; 1\\), and \\(0 &lt; \\beta &lt; 1\\).\nWhile \\(f(x + \\eta d) &gt; f(x) + \\alpha \\eta \\nabla f(x)^T d\\), it reduces \\(\\eta\\) by the update \\(\\eta \\leftarrow\\) \\(\\beta \\eta\\).\n\n\n\nConvergence Guarantees of GD with BTLS\nGradient descent with backtracking line search has a promising convergence guarantee for convex objectives that are also \\(M\\)-smooth.\n\nConvergence of GD with BTLS:   If \\(f\\) is \\(M\\)-smooth and convex then the step-size given by BTLS is s.t.\n\\(\\eta_{BTLS} \\geq \\frac{\\beta}{M}\\). Furthermore, \\(f(x^+) - f(x) \\leq \\frac{\\alpha \\beta}{M}||\\nabla f(x)||_2^2\\)\n\nSo, compared to the theoretical best step-size \\(\\eta = \\frac{1}{M}\\), which may not be accessible to us, we have \\(\\frac{\\beta}{M} \\leq \\eta_{BTLS} &lt; \\frac{1}{M}\\). So, \\(\\beta_{BTLS}\\) is less than the optimal step-size but, interestingly, it still keeps \\(M\\) in view despite the latter being unknown to us.\nFurthermore, for an \\(M\\)-smooth objective, the final sub-optimality after \\(T\\) iterations can be found as:\n\\[f(x_T) - f(x^*) \\leq \\frac{M}{2T \\alpha \\beta}||x_0 - x^*||_2^2 \\tag{7.1}\\]\nWhich is only a constant factor \\(\\alpha \\beta\\) worse than GD with the theoretical best fixed step-size. So, it’s still \\(O(1/T)\\).\nFor an \\(M\\)-smooth objective that’s also \\(m\\)-strongly convex, we have:\n\\[f(x_T) - f(x^*) \\leq \\left (1 - \\min \\left\\{ 2m\\alpha, \\frac{2\\alpha\\beta m}{M} \\right \\} \\right )^T||x_0 - x^*||_2^2 \\tag{7.2}\\]\nWhich is, again, comparable to GD with the theoretical best fixed step-size."
  },
  {
    "objectID": "posts/optimization/robust-lps-modelling-discrete-failures.html",
    "href": "posts/optimization/robust-lps-modelling-discrete-failures.html",
    "title": "Robust Linear Programs - Modelling Discrete Failures",
    "section": "",
    "text": "We are faced with the task of modeling a scenario in which at most \\(k\\) of the total \\(n\\) workers, machines, sensors, or other system components can fail. The task is to minimize the amount of system components needed, thereby minimizing cost, subject to certain known and robust constraints.\nWe will assume the linear cost function to be \\(c^Tx\\), and the known constraints to be \\(Ax \\leq b\\). The robust constraint is \\(a_R^Tx \\leq b_R, \\ \\ \\forall a_R \\in D_k\\) where \\(b_R\\) is a known vector, and \\(D_k\\) is the following interval uncertainty set with an additional combinatorial constraint:\n\\[\nD_k = \\{ a : a_i \\in [\\hat a_i - \\delta_i, \\hat a_i + \\delta_i] \\wedge \\textrm{at least $n-k$ of the $a_i$'s exactly equal $\\hat a_i$}\\}\n\\]\nIn \\(D_k\\), we can think of each \\(\\hat a_i\\) as the spec at which the \\(i\\)-th component should operate, and the \\(\\delta_i\\)’s as the \\(i\\)-th component’s deviation from this spec. Thus, \\(D_k\\) models the discrete failures scenario exactly…"
  },
  {
    "objectID": "posts/optimization/robust-lps-modelling-discrete-failures.html#formulating-the-inner-problem-as-a-linear-program",
    "href": "posts/optimization/robust-lps-modelling-discrete-failures.html#formulating-the-inner-problem-as-a-linear-program",
    "title": "Robust Linear Programs - Modelling Discrete Failures",
    "section": "Formulating the Inner Problem as a Linear Program",
    "text": "Formulating the Inner Problem as a Linear Program\nLet’s focus on the inner problem\n\\[\n\\begin{cases}\nmax_{a_R}: &a_R^Tx\n\\\\\ns.t.: &a_R \\in D_k\n\\end{cases} \\leq b_R\n\\]\nOur strategy now is to expand the constraint set \\(D_k\\).\nTo that end, let’s introduce slack variables \\(-1 \\leq z_i \\leq 1 \\ \\ \\forall i\\), which represent the direction of each component’s deviation from its spec. We can now rewrite the objective as:\n\\[\n\\begin{aligned}\na_R^Tx & = \\sum a_ix_i\n\\\\\n& = \\sum (\\hat a_i + z_i\\delta_i)x_i\n\\\\\n& = \\sum \\hat a_ix_i + \\sum \\delta_iz_ix_i\n\\end{aligned}\n\\]\nSo the optimization problem, which is now in the variables \\(z_i\\), becomes\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_ix_i\n\\\\\ns.t.: \\begin{aligned} &-1 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\textrm{at most $k$ of the $z_i \\ne 0$}\n\\end{aligned}\n\\end{cases}\n\\]\nWe still have the combinatorial constraint ‘at most \\(k\\) of the \\(z_i \\ne 0\\),’ which makes this into a mixed optimization problem…\nWe need to massage this problem more to bring it to standard form.\nNote that we’re dealing with a problem of maximization. In the objective, \\(\\sum \\hat a_ix_i\\) is fixed by virtue of the \\(\\hat a_i\\)’s being fixed by the given \\(D_k\\) and the \\(x_i\\)’s being fixed by the outer optimization problem. Note that the \\(\\delta_i\\)’s are also fixed by \\(D_k\\). Therefore, what would maximize the objective is each term of \\(\\sum \\delta_i z_i x_i\\) contributing positively to the sum.\nThis happens when \\(z_i\\) and \\(x_i\\) have the same sign \\(\\forall i\\). That is, their product \\(z_ix_i\\) takes values in \\([0, |x_i|]\\).\nThe remaining cases can be disposed of without changing the optimal value of the optimization problem.\nRewriting the problem, we have\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\textrm{at most $k$ of the $z_i \\ne 0$}\n\\end{aligned}\n\\end{cases} \\dagger\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that \\(|x_i|\\) does not make the objective non-linear because the \\(x_i\\)’s are fixed values, and not the decision variables.\n\n\n\n\nRelaxing the Combinatorial Constraint with a Continuous Constraint\nWe will relax the combinatorial constraint ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ by replacing it with \\(\\sum z_i \\leq k\\).\nAlthough this is a relaxation of the constraint, we will show that it makes no difference within the context of preserving the optimization problem. That is, it does not affect the optimal value of the problem.\nFirst and foremost, it’s easy to see that ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ \\(\\implies\\) \\(\\sum z_i \\leq k\\) since each \\(z_i \\in [0,1]\\).\nWe claim the converse is true as well, given that we restrict our attention to the optimal solution to the above LP \\(\\dagger\\). That is ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ \\(\\impliedby\\) \\(\\sum z_i \\leq k\\).\nThis claim is true by the geometry of linear programs. An optimal solution to the LP can only occur at an extreme point, and those are defined exactly by \\(n\\) independent active constraints.\nIn the above LP, \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\) represent a set of \\(2n\\) independent constraints, and \\(\\sum z_i \\leq k\\) is just one additional constraint.\nIf all of the \\(n\\) active constraints come from \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\), then since a given \\(z_i\\) cannot simultaneously be \\(0\\) and \\(1\\) the \\(z_i\\)’s of the optimal solution must take integral values (that is, either \\(0\\) or \\(1\\) and nothing in between).\nIn the general case, at least \\(n-1\\) constraints must come from \\(0 \\leq z_i \\leq 1 \\ \\ \\forall i\\), which implies at least \\(n-1\\) of the \\(z_i\\)’s take integral values and the remaining active constraint is \\(\\sum z_i = k\\). But \\(n\\) numbers, of which \\(n-1\\) are integers, cannot add up to an integer value \\(k\\) unless the remaining number is also an integer. So, once again we have that all the \\(z_i\\)’s are integral valued.\nThen \\(\\sum z_i \\leq k\\) \\(\\implies\\) at most \\(k\\) of the \\(z_i = 1\\) \\(\\implies\\) ‘at most \\(k\\) of the \\(z_i \\ne 0\\)’ as was the claim.\nThis leaves us with the inner optimization problem\n\\[\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\sum z_i \\leq k\n\\end{aligned}\n\\end{cases}\n\\]\nwhich is finally a linear program."
  },
  {
    "objectID": "posts/optimization/robust-lps-modelling-discrete-failures.html#putting-the-inner-and-outer-problems-together",
    "href": "posts/optimization/robust-lps-modelling-discrete-failures.html#putting-the-inner-and-outer-problems-together",
    "title": "Robust Linear Programs - Modelling Discrete Failures",
    "section": "Putting the Inner and Outer Problems Together",
    "text": "Putting the Inner and Outer Problems Together\nThe combined optimization problem becomes\n\\[\n\\begin{cases}\nmin: c^Tx\n\\\\\ns.t.: Ax \\leq b\n\\\\\n\\begin{cases}\nmax_{z}: \\sum \\hat a_ix_i + \\sum \\delta_iz_i|x_i|\n\\\\\ns.t.: \\begin{aligned} &0 \\leq z_i \\leq 1 \\ \\ \\forall i\n\\\\\n& \\sum z_i \\leq k\n\\end{aligned}\n\\end{cases} \\leq b_R\n\\end{cases}\n\\]\nThis is, of course, still not a linear program. Firstly, it’s a mixture between minimization and maximization. Secondly, since the variables are \\(x_i\\), and \\(z_i\\), the term \\(\\sum \\delta_iz_ix_i\\) is not linear in the decision variables. Thirdly, \\(|x_i|\\) is not linear in \\(x_i\\).\nWe can address these issues one by one…\n\nTaking the Dual of the Inner\nWe can turn the inner maximization problem to an inner minimization problem by taking its dual. As we know, by LP-duality (otherwise known as strong duality) this does not affect the optimal value of the problem.\nThe overall problem becomes\n\\[\n\\begin{cases}\nmin: c^Tx\n\\\\\ns.t.: Ax \\leq b\n\\\\\n\\begin{cases}\nmin_{\\lambda}: \\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k\n\\\\\ns.t.: \\begin{aligned} &\\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases} \\leq b_R\n\\end{cases}\n\\]\nFlattening the problems, since both are now minimization, we arrive at the following\n\\[\n\\begin{cases}\nmin_{x,\\lambda}: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax \\leq b\n\\\\\n&\\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k \\leq b_R\n\\\\\n&\\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]\n\n\nLinearizing the Absolute Value Constraints\nThis is almost a linear program, except for the fact that \\(|x_i|\\)’s are nonlinear terms in the constraint. The last step is to split these constraints into corresponding pairs of linear constraints.\nFor each \\(i\\),\n\\[\n\\begin{aligned} \\lambda_0 + \\lambda_i \\geq \\delta_i|x_i| &\\implies -\\lambda_0 - \\lambda_i \\leq \\delta_ix_i \\leq \\lambda_0 + \\lambda_i  \\\\ &\\implies \\begin{cases} \\lambda_0 + \\lambda_i &\\geq \\delta_ix_i \\\\ &\\textrm{and} \\\\ \\lambda_0 + \\lambda_i &\\geq -\\delta_ix_i \\end{cases}\\end{aligned}\n\\]\nSo, the final problem, which is a linear program in every right, is\n\\[\n\\begin{cases}\nmin_{x,\\lambda}: c^Tx\n\\\\\ns.t.: \\begin{aligned} &Ax \\leq b\n\\\\\n&\\sum \\hat a_ix_i + \\sum \\lambda_i + \\lambda_0k \\leq b_R\n\\\\\n&\\lambda_0 + \\lambda_i \\geq \\delta_ix_i \\ \\ \\forall i\n\\\\\n&\\lambda_0 + \\lambda_i \\geq -\\delta_ix_i \\ \\ \\forall i\n\\\\\n& \\lambda \\geq 0\n\\end{aligned}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html",
    "href": "posts/functional_programming/functional_programming.html",
    "title": "Functional Programming",
    "section": "",
    "text": "Functional programming draws inspiration from the mathematical definition of a function which is a well-defined operation on sets.\n\n\nTake \\(f: X \\rightarrow Y\\), which is a function that maps elements of the set \\(X\\) to those of the set \\(Y\\), such that for each \\(x\\) in \\(X\\) (also denoted “\\(x \\in X\\)”), there’s one and only one \\(y \\in Y\\) that satisfies the equation \\(f(x) = y\\). In plain language we say that a mathematical function maps any given input to its own unique output (that depends only on the input). That’s not to say that \\(f\\) can’t map the two different inputs, \\(x_i\\), and \\(x_j\\), to the same output \\(y\\), but it cannot map the same input \\(x_i\\) to more than one output.\nNotice that the output in \\(Y\\) depends only on the input from set \\(X\\), and that the function \\(f\\) only operates on set \\(X\\) and nothing external to it. In other words there is no hidden state (some value outside of \\(X\\)) that affects \\(f\\)-s output, so \\(f\\) always produces predictable output. What’s more \\(f\\) doesn’t really alter any element in \\(X\\) itself (or, for that matter, in \\(Y\\)). The expression \\(f(x)\\) is simply understood as the function \\(f\\) applied to an element \\(x \\in X\\) which maps it to an element in set \\(Y\\). However, it’s not like the specific element in the set \\(X\\) is somehow retrieved (as it is sometimes, by reference, in programming) and overwritten in any way.\nThe idea behind functional programming is to bring code close to this mathematical elegance, allowing us to better reason about the systems we write.\n\n\n\nWe can define some rules for the functions we write in our code to match the mathematical properties of functions, bridging the concrete world of mathematics with the practical world of software engineering.\nA pure function, in the FP sense, is a function which depends only on its input (and not on any other value stored elsewhere in external computer memory or other external source). A pure function affects nothing outside itself. Additionally, pure functions must output a value and that value must be unique for a given input.\nTo recap:\n\nA pure function must return a single output for a given input\nIts output should only depend on its input\nA pure function shouldn’t change any external state\n\nThe last property effectively means that pure functions don’t mutate state, in general. Given that a functional program is just a composition of pure functions, state mutation is often frowned upon in general. This presents unique challenges, as you might expect given that so many operations, in the traditional paradigm of programming, mutate a state (for instance, a for loop increments its index on each iteration). We will see how functional programming languages attempt to solve this problem (with a lot of syntactic sugar).\n\n\nWorking with pure functions conveys some great benefits. For instance, properties (1) and (2) make pure functions interchangeable with their output (just as, say, \\(f(2)\\) given \\(f(x)=x^2\\) can reliably be substituted for the number \\(4\\) in math). This allows us to pass in pure functions as arguments into other pure functions (as well as return them as output) with entirely predictable results. If a function, by contrast, printed something to the console, along with evaluating the square, we would consider that an effectful function (and therefore it would be considered impure). Such a function cannot be reliably substituted by its output because it also affects an external state, producing an effect that the output alone does not capture. This benefit, to reliably substitute the representation of a value for the value itself, yields nice benefints. In mathematics, for example, we can cheaply compute the gradient of a loss function using the back-propagation algorithm by storing intermediate values during the forward pass so that, during the backwards pass (back-propagation) we avoid redundant calculations. By contrast, if our functions affected external state somewhere, or produced other such effects (more of which we will see in the section on side effects), it would be a lot more difficult to model our programs as chains of pure function calls making them harder to reason about mathematically.\nIn the next section we look at the differences between declarative and imperative styles of writing software and why functional programming prefers the former style.\n\n\n\nAt a basic level, an imperative style of programming can be likened to cooking at home with a cookbook. Imperative languages look more like a list of commands directed at the computer. Declarative writing, by contrast, can be compared to dining at a restaurant. We aren’t issuing commands at a grueling level of granularity (e.g. iterating over an array manually, or appending to a list). Instead, we’re specifying the desired outcome without the implementation details like we would in mathematics when we, for instance, write \\(f(x)=x^2\\) succinctly (implying to square every feature of the input vector \\(x\\)). We prefer declarative code to imperative in FP partly because imperative code involves a lot of state mutation and partly because writing pure functions facilitates writing declarative code.\nIn the imperative style, we’re saying “step through the list, read each item, square it and append it to a new list.” In the declarative style we’re saying “just square every element of this list.” These differences are mostly semantic and, in real life, software contains a mix of both styles. The distinction is also not really black or white, and is often dependent on the implementation of the given language.\nAn example is worth a thousand words and, since Python provides a good enough playground for showcasing these styles, here is an example in Python.\nImperative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = []\nfor num in numbers:\n    squared.append(num ** 2)\n\nprint(squared)\n\n[1, 4, 9, 16, 25]\n\n\nDeclarative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = map(lambda x: x**2, numbers)\n\nprint(list(squared))\n\n[1, 4, 9, 16, 25]\n\n\nNotice how in the declarative style we merely instructed our function to square each feature, but we didn’t tell the program how to do it in grueling detail and we avoided the use of a for loop (which means we avoided mutating the state of the index of the loop). Also, more lines in the declarative style return a value, rather than just carrying our instructions (we will see the difference between mere instructions, or statements and pure expressions later on). However, since there’s printing to the console at the end, even the declarative program would not be considered functional.\n\n\n\nFunctions which violate any of the three afforementioned properties are said to produce side effects (or simply effects). The most common side effect is when a function modifies a state (i.e. a chunk of computer memory) outside itself (violating property (3)). Examples of side effects include:\n\n\n\n\n\n\n\nEffect\nFunctional Programming Way\n\n\n\n\nA function directly modifying a variable defined in the global scope.\nThe FP approach is to pass the global variable as input instead, and have the function return a modified copy of the input.\n\n\nA function writing to an external database.\nThis is an example of an unavoidable side effect in practice. The FP approach is to mitigate. Specifics are language dependent, but usually the strategy involves gathering all such unavoidable side effects into one impure corner of the code, and keeping the rest of the code pure.\n\n\nA function like the built-in functions of printing to the console, retrieving system time, or a random number generator (or those functions which use them)\nYet more examples of unavoidable side effects. Such functions are inherently dependent on external or hidden state such as the time of day in the real world and, in general, things other than their input.\n\n\n\nAlthough some side effects are unavoidable, we should minimize their use in our code. Functional programming languages offer just that ability.\n\n\n\n\nIn functional programming, we distinguish between mere instructions to the computer (which are also sometimes known as statements) and expressions (or pure expressions). This distinction is similar to that between functions, in the programming sense, and pure functions in the mathematical sense – Expressions, like functions, must always return a value. Contrast this with instructions like the traditional if/else statements or loops like while which control the flow of execution, but don’t evaluate to anything.\nAs mentioned earlier, such effects are unavoidable at times. However, functional languages have different strategies of mitigating these impurities and writing pure code anyway. Usually they aim to gather the impurities together at the top or bottom of the code. Some languages (such as Scala which is a blend of OOP and FP), go to great lengths to minimize side effects by enforcing the return requirement of its syntactic structures. Even though Scala has the traditional for loop as an instruction, it favors the use of for-comprehensions which are essentially syntactic sugar (enabled by monadic types that capture effects, more on these later). Each line of a for-comprehension in Scala evaluates to a value.\nThe idea is to use a clever type system to capture effects. If side effects must exist, they should be known to Scala. To achieve this, Scala has a monadic type known as Unit which can hold only () as its value. This is its designated side effect type. So, functional programming languages elevate instructions or statements, which normally don’t return anything, the status of pure expressions by returning a dedicated side effect type. In practice there are many types for different side effects (for example, an IO monadic type captures side effects produced by operations like console logging). Let’s see some examples of how Scala does away with traditional for loops and if/else statements and uses pure expressions instead.\n\n\nIn Scala, if statements are implemented as expressions similar to the familiar ternary expressions in Python. Scala has traditional if statements too, but the if pure expression is what’s preferred. Here are some examples that demonstrate difference the difference in both Python and Scala:\nPython:\nx = 1 if condition == True else 0\nScala:\nval x = if (condition) 1 else 0\nIn this example, x necessarily evaluates to a value: one of possible two. This if expression will not produce a side effect as would an open-ended if statement. Inside an if statement, the programmer might just do something crazy and unheard of like accessing a database, or printing a line to the console (both considered side effects).\nThis brings us to an important point. It’s not that if statements would necessarily result in side effects, it’s just that functional programming discourages the use of language constructs that lend themselves to producing side effects more easily. Syntactic choices like this are a common theme in FP. For instance, Scala’s choice to treat () as a returnable value (of type Unit) rather than just a piece of syntax is very deliberate. Let’s see why by examining a Scala, for-comprehension.\nScala For Comprehension:\nval result = \n    for {\n        _ &lt;- print(\"Hello\")\n        _ &lt;- print(\"World!\")\n    } yield ()\nIt may not look like it, but the code snippet above (showing a for-comprehension) is one of the ways in which Scala actually chains many potentially side-effect producing operations together via function composition (which is what’s going on in the background). Notice three things about it:\n\nThe for comprehension returns a value captured by result.\nThe print statement produces a side effect which is discarded as _.\nAt the end we simply say yield ()… If we wished to return a value instead we would do so inside the () however, because Scala associates a type with (), what’s actually returned is the side effect captured as a Unit (so the for comprehension returns something)\n\n\n\n\nBecause FP frowns upon the use of if/else and for/while statements, it prefers function composition to iteration. In fact, the for-comprehension above is just cleverly disguised function composition.\nTake, for example, a while loop that runs until a key press (or any other user input). Of course, this may be an unavoidable side effect in the real world. The FP approach would, then, just be to contain this impurity somewhere with the rest of its kind.\nIn general, instead of iteration, function composition is preferred (mathematical readers will understand that recursion, which often offers itself as an alternative to iteration when there’s optimal substructure, is a type of function composition). Functional programming prefers this approach in general. This may sound tedious and almost like re-learning how to program at first, but syntactic sugars and other abstractions exist to make this pattern more readable (like the for-comprehension we already saw in Scala).\nThere are already a few familiar examples of function composition that have been adopted by popular languages like Python, and are very intuitive (especially when dealing with data pipelines or modelling real world interactions in terms of sequential actions). Some examples are the map and the filter functions in Python. We already saw an example of map in the declarative code snippet above, so we won’t dive into its specifics here. Both mapand filter are examples of higher-order functions (HoFs) – functions which take other functions as input and/or themselves output functions. map and filter show that function composition can be very readable and intuitive. Furthermore, neither map nor filter modify their input in-place. Rather, they return a modified copy of the input to avoid external state mutation which is considered poor practice in FP. Later on we shall see that a related method called flatMap exists which turns out to play a key role in letting functional programmers write useful programs by chaining multiple potentially side effect-producing functions together. This is due to flatMap’s unique function signature (which flattens arrays of arrays into one array).\n\n\nTo pipe functions into other functions (as in function composition), we need functions that take other functions as input and can also output functions. When we treat functions this way, we basically treat them as first-class values which means like any other value, they can be passed and returned around. There is a mathematically-inspired reason, other than designing software as function composition, for using HoFs.\nWhen mathematicians write:\n\\[\n\\sum_{x=a}^{b} f(x)\n\\]\nwhere, say \\(a,b \\in \\mathcal{Z}\\), they understand that \\(f\\) stands for some general function. So there’s no need to write a separate expression for summing the integers, one for summing the squares of the integers, and one for summing the factorials of integers between \\([a,b]\\).\nLet’s write a function in Scala that sums the integers:\ndef sumInts(a: Int, b: Int): Int = \n    if a &gt; b then 0 else a + sumInts(a + 1, b)\nTo get the sum of squares, we’d need to define another function:\ndef square(x: Int): Int = x * x\n\ndef sumSquares(a: Int, b: Int): Int = \n    if a &gt; b then 0 else square(a) + sumSquares(a + 1, b)\nBut there’s clearly some repetition here, so we can factor out a common pattern. What if we changed the signature of sumInts to take a function as argument?\ndef sum(f: Int =&gt; Int, a: Int, b: Int): Int =\n    if a &gt; b then 0 else f(a) + sum(f, a + 1, b)\nNow we can write:\ndef sumInts(a: Int, b: Int) = sum(id, a, b) \nwhere id is the identity function:\ndef id(x: Int): Int = x\nand:\ndef sumSquares(a: int, b: Int) = sum(square, a, b)\nAdmittedly this doesn’t look great because we’re creating a lot of boilerplate functions. To get rid of this boilerplate, Scala was the first language to introduce the notion of anonymous functions (in Python these are known by the lambda keyword). We can think of anonymous functions as literals. Similar to how we can do println(\"hello world!\") without having to name the string literal \"hello world!\" using a variable, we can declare functions as literals. Using anonymous functions, the id and square functions above can be written respectively as:\n(x: Int): Int =&gt; x\n(x: Int): Int =&gt; x * x\nThis reduces our sumInts and sumSquares to:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b) // Types can be omitted if they can be inferred from context\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nAnonymous functions are syntactic sugar. That is, they aren’t necessary but make life easier.\n\n\n\nBut so far we’ve only used the HoF’s ability to accept functions as input. Let’s also use their ability to output functions. The pattern we are about to learn is called currying (after Haskell Curry), and it’s useful for, among other things, dependency injection.\nNote, again, the functions:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b)\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nBoth a and b are passed into each unchanged. Is there a common pattern we can extract? We can use currying which is just partial application of the function. Here’s the curried version of sum:\ndef sum(f: Int =&gt; Int): (Int, Int) =&gt; Int =\n    def sumFn(a: Int, b: Int): Int =\n        if a &gt; b then 0 else f(a) + sumF(a + 1, b)\n    sumF\nOur sum functions now constructs and returns a new function which takes the rest of input the (a and b). This is called partial-application. We’ve split the sum into two parts: the first part accepts only the argument f as input, the second one accepts the rest of the arguments.\nNow we can define sumInts and sumSquares respectively as just:\ndef sumInts = sum(x =&gt; x)\ndef sumSquares = sum(x =&gt; x*x)\nNote that when we call sum, we get back a function with signature (Int, Int) =&gt; Int (which is exactly the signature we want for sumInts and sumSquares). We can now use better readable syntax like sum(cube)(1,5) + sum(squares)(5,10) doing away with sumInts and sumSquares (using which we’d have to write the above as: sumInts(1,5) + sumSquares(5,10)).\nSince it can get quite clumsy to write curried functions, Scala provides a shorthand. This is equivalent to the curried sum written above.\ndef sum(f: Int =&gt; Int)(a: Int, b: Int): Int = \n    if a &gt; b then 0 else f(a) sum(f)(a + 1, b)\nIn Python, there’s support for curried functions in the functools library (functools.partial, which implements a curried version of a function you pass to it (by itself currying the input).\nCurrying, in general, can be applied \\(n\\) times to an \\(n\\)-dimensional function, each outer function returning an (anonymous) inner function which partially applies the rest of the parameters. This means languages need not have support for functions with parameters, as long as they have support for anonymous functions.\n\n\n\nMonads are a term borrowed from Category Theory. They have a strict definition in mathematics, but for our purposes they’re useful black boxes that provide the following benefits (some already mentioned before in passing). Monads provide a way to compose potentially side-effect producing functions together, and in general they make function composition lend itself easier to being abstracted behind syntactic sugars (like do notation in Haskell) which make functional programs more readable and more useful for real-world applications. As a sidenote, to the readers who are familiar with JavaScript Promises, these are, in essence, the same as the side-effect type Unit in Scala (or, rather, more like the more specific IO side-effect). The main point is that a Promise also a monad, which makes it possible to come up with nice, syntactic sugars like async await. In fact, do notation in Haskell is actually the generalized version of this type of async await pattern that works with any monad and not just a Promise.\nMonads achieve this mostly by just being classes that have an implementation of flatMap. Yes, the same flatMap we discussed earlier in the context of it being one of the higher-order functions that are popularly used more mainstream languages like Python. It turns out, chaining two side-effect producing operations in function compositional produces nested side effect types. For example, chaining two IO operations that prompt for two strings may produce something like IO(IO(String)) and flatMap, with its unique capacity to flatten, is exactly the thing that’s needed to get an IO(String)) back. We will discuss this flattening property of flatMap in more detail later on. For now, it’s important to reiterate that having an implementation of flatMap (which may go by other names in other languages, e.g. Bind in Haskell) is mostly (along a few other key properties) what qualifies a class to be a monad.\nSo, it’s no wonder that in a functional programming language we want our side effect-producing expressions to return a monadic type so that you can chain two or more of such expressions together (using function composition).\n\n\n\n\n\n\nIn terms of parallelization, both iterative and recursive solutions can be sequential processes, which don’t lend themselves well to parallelization, or independent processes which do. However, FP still confers some benefit in terms of parallelization – not because it favors recursion but, instead, because:\n\nA common challenge in parallel programming is to avoid mutating data while another thread is using it. Due to state immutability principles in FP, this problem is eliminated\nFP avoids writing functions which rely on hidden state (i.e. any state that’s not a direct input), so functions can be executed in parallel without the concern of synchronizing access to some shared state.\nFP can make it easier to identify opportunities for parallelization\nLanguages which are built around FP have powerful parallelization libraries that offer parallelized versions of common operations like map\n\n\n\n\n\n\n\nIf we’re going to favor the use of recursion (or, in general function composition) in FP over the more imperative style of writing iterative algorithms, we ought to tread carefully as to not cause stack overflow (which, as we know, is when the system runs out of working memory). Tail recursion optimization (similar to other techniques like memoization) helps us drastically cut the amount of stack memory used. It takes a constant amount of memory on the stack, instead of the linear, with input size, or worse. Read more about tail recursive optimization here."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "href": "posts/functional_programming/functional_programming.html#mathematical-functions",
    "title": "Functional Programming",
    "section": "",
    "text": "Take \\(f: X \\rightarrow Y\\), which is a function that maps elements of the set \\(X\\) to those of the set \\(Y\\), such that for each \\(x\\) in \\(X\\) (also denoted “\\(x \\in X\\)”), there’s one and only one \\(y \\in Y\\) that satisfies the equation \\(f(x) = y\\). In plain language we say that a mathematical function maps any given input to its own unique output (that depends only on the input). That’s not to say that \\(f\\) can’t map the two different inputs, \\(x_i\\), and \\(x_j\\), to the same output \\(y\\), but it cannot map the same input \\(x_i\\) to more than one output.\nNotice that the output in \\(Y\\) depends only on the input from set \\(X\\), and that the function \\(f\\) only operates on set \\(X\\) and nothing external to it. In other words there is no hidden state (some value outside of \\(X\\)) that affects \\(f\\)-s output, so \\(f\\) always produces predictable output. What’s more \\(f\\) doesn’t really alter any element in \\(X\\) itself (or, for that matter, in \\(Y\\)). The expression \\(f(x)\\) is simply understood as the function \\(f\\) applied to an element \\(x \\in X\\) which maps it to an element in set \\(Y\\). However, it’s not like the specific element in the set \\(X\\) is somehow retrieved (as it is sometimes, by reference, in programming) and overwritten in any way.\nThe idea behind functional programming is to bring code close to this mathematical elegance, allowing us to better reason about the systems we write."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "href": "posts/functional_programming/functional_programming.html#pure-functions-and-side-effects",
    "title": "Functional Programming",
    "section": "",
    "text": "We can define some rules for the functions we write in our code to match the mathematical properties of functions, bridging the concrete world of mathematics with the practical world of software engineering.\nA pure function, in the FP sense, is a function which depends only on its input (and not on any other value stored elsewhere in external computer memory or other external source). A pure function affects nothing outside itself. Additionally, pure functions must output a value and that value must be unique for a given input.\nTo recap:\n\nA pure function must return a single output for a given input\nIts output should only depend on its input\nA pure function shouldn’t change any external state\n\nThe last property effectively means that pure functions don’t mutate state, in general. Given that a functional program is just a composition of pure functions, state mutation is often frowned upon in general. This presents unique challenges, as you might expect given that so many operations, in the traditional paradigm of programming, mutate a state (for instance, a for loop increments its index on each iteration). We will see how functional programming languages attempt to solve this problem (with a lot of syntactic sugar).\n\n\nWorking with pure functions conveys some great benefits. For instance, properties (1) and (2) make pure functions interchangeable with their output (just as, say, \\(f(2)\\) given \\(f(x)=x^2\\) can reliably be substituted for the number \\(4\\) in math). This allows us to pass in pure functions as arguments into other pure functions (as well as return them as output) with entirely predictable results. If a function, by contrast, printed something to the console, along with evaluating the square, we would consider that an effectful function (and therefore it would be considered impure). Such a function cannot be reliably substituted by its output because it also affects an external state, producing an effect that the output alone does not capture. This benefit, to reliably substitute the representation of a value for the value itself, yields nice benefints. In mathematics, for example, we can cheaply compute the gradient of a loss function using the back-propagation algorithm by storing intermediate values during the forward pass so that, during the backwards pass (back-propagation) we avoid redundant calculations. By contrast, if our functions affected external state somewhere, or produced other such effects (more of which we will see in the section on side effects), it would be a lot more difficult to model our programs as chains of pure function calls making them harder to reason about mathematically.\nIn the next section we look at the differences between declarative and imperative styles of writing software and why functional programming prefers the former style.\n\n\n\nAt a basic level, an imperative style of programming can be likened to cooking at home with a cookbook. Imperative languages look more like a list of commands directed at the computer. Declarative writing, by contrast, can be compared to dining at a restaurant. We aren’t issuing commands at a grueling level of granularity (e.g. iterating over an array manually, or appending to a list). Instead, we’re specifying the desired outcome without the implementation details like we would in mathematics when we, for instance, write \\(f(x)=x^2\\) succinctly (implying to square every feature of the input vector \\(x\\)). We prefer declarative code to imperative in FP partly because imperative code involves a lot of state mutation and partly because writing pure functions facilitates writing declarative code.\nIn the imperative style, we’re saying “step through the list, read each item, square it and append it to a new list.” In the declarative style we’re saying “just square every element of this list.” These differences are mostly semantic and, in real life, software contains a mix of both styles. The distinction is also not really black or white, and is often dependent on the implementation of the given language.\nAn example is worth a thousand words and, since Python provides a good enough playground for showcasing these styles, here is an example in Python.\nImperative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = []\nfor num in numbers:\n    squared.append(num ** 2)\n\nprint(squared)\n\n[1, 4, 9, 16, 25]\n\n\nDeclarative Style\n\nnumbers = [1, 2, 3, 4, 5]\nsquared = map(lambda x: x**2, numbers)\n\nprint(list(squared))\n\n[1, 4, 9, 16, 25]\n\n\nNotice how in the declarative style we merely instructed our function to square each feature, but we didn’t tell the program how to do it in grueling detail and we avoided the use of a for loop (which means we avoided mutating the state of the index of the loop). Also, more lines in the declarative style return a value, rather than just carrying our instructions (we will see the difference between mere instructions, or statements and pure expressions later on). However, since there’s printing to the console at the end, even the declarative program would not be considered functional.\n\n\n\nFunctions which violate any of the three afforementioned properties are said to produce side effects (or simply effects). The most common side effect is when a function modifies a state (i.e. a chunk of computer memory) outside itself (violating property (3)). Examples of side effects include:\n\n\n\n\n\n\n\nEffect\nFunctional Programming Way\n\n\n\n\nA function directly modifying a variable defined in the global scope.\nThe FP approach is to pass the global variable as input instead, and have the function return a modified copy of the input.\n\n\nA function writing to an external database.\nThis is an example of an unavoidable side effect in practice. The FP approach is to mitigate. Specifics are language dependent, but usually the strategy involves gathering all such unavoidable side effects into one impure corner of the code, and keeping the rest of the code pure.\n\n\nA function like the built-in functions of printing to the console, retrieving system time, or a random number generator (or those functions which use them)\nYet more examples of unavoidable side effects. Such functions are inherently dependent on external or hidden state such as the time of day in the real world and, in general, things other than their input.\n\n\n\nAlthough some side effects are unavoidable, we should minimize their use in our code. Functional programming languages offer just that ability."
  },
  {
    "objectID": "posts/functional_programming/functional_programming.html#instructions-statements-vs-expressions",
    "href": "posts/functional_programming/functional_programming.html#instructions-statements-vs-expressions",
    "title": "Functional Programming",
    "section": "",
    "text": "In functional programming, we distinguish between mere instructions to the computer (which are also sometimes known as statements) and expressions (or pure expressions). This distinction is similar to that between functions, in the programming sense, and pure functions in the mathematical sense – Expressions, like functions, must always return a value. Contrast this with instructions like the traditional if/else statements or loops like while which control the flow of execution, but don’t evaluate to anything.\nAs mentioned earlier, such effects are unavoidable at times. However, functional languages have different strategies of mitigating these impurities and writing pure code anyway. Usually they aim to gather the impurities together at the top or bottom of the code. Some languages (such as Scala which is a blend of OOP and FP), go to great lengths to minimize side effects by enforcing the return requirement of its syntactic structures. Even though Scala has the traditional for loop as an instruction, it favors the use of for-comprehensions which are essentially syntactic sugar (enabled by monadic types that capture effects, more on these later). Each line of a for-comprehension in Scala evaluates to a value.\nThe idea is to use a clever type system to capture effects. If side effects must exist, they should be known to Scala. To achieve this, Scala has a monadic type known as Unit which can hold only () as its value. This is its designated side effect type. So, functional programming languages elevate instructions or statements, which normally don’t return anything, the status of pure expressions by returning a dedicated side effect type. In practice there are many types for different side effects (for example, an IO monadic type captures side effects produced by operations like console logging). Let’s see some examples of how Scala does away with traditional for loops and if/else statements and uses pure expressions instead.\n\n\nIn Scala, if statements are implemented as expressions similar to the familiar ternary expressions in Python. Scala has traditional if statements too, but the if pure expression is what’s preferred. Here are some examples that demonstrate difference the difference in both Python and Scala:\nPython:\nx = 1 if condition == True else 0\nScala:\nval x = if (condition) 1 else 0\nIn this example, x necessarily evaluates to a value: one of possible two. This if expression will not produce a side effect as would an open-ended if statement. Inside an if statement, the programmer might just do something crazy and unheard of like accessing a database, or printing a line to the console (both considered side effects).\nThis brings us to an important point. It’s not that if statements would necessarily result in side effects, it’s just that functional programming discourages the use of language constructs that lend themselves to producing side effects more easily. Syntactic choices like this are a common theme in FP. For instance, Scala’s choice to treat () as a returnable value (of type Unit) rather than just a piece of syntax is very deliberate. Let’s see why by examining a Scala, for-comprehension.\nScala For Comprehension:\nval result = \n    for {\n        _ &lt;- print(\"Hello\")\n        _ &lt;- print(\"World!\")\n    } yield ()\nIt may not look like it, but the code snippet above (showing a for-comprehension) is one of the ways in which Scala actually chains many potentially side-effect producing operations together via function composition (which is what’s going on in the background). Notice three things about it:\n\nThe for comprehension returns a value captured by result.\nThe print statement produces a side effect which is discarded as _.\nAt the end we simply say yield ()… If we wished to return a value instead we would do so inside the () however, because Scala associates a type with (), what’s actually returned is the side effect captured as a Unit (so the for comprehension returns something)\n\n\n\n\nBecause FP frowns upon the use of if/else and for/while statements, it prefers function composition to iteration. In fact, the for-comprehension above is just cleverly disguised function composition.\nTake, for example, a while loop that runs until a key press (or any other user input). Of course, this may be an unavoidable side effect in the real world. The FP approach would, then, just be to contain this impurity somewhere with the rest of its kind.\nIn general, instead of iteration, function composition is preferred (mathematical readers will understand that recursion, which often offers itself as an alternative to iteration when there’s optimal substructure, is a type of function composition). Functional programming prefers this approach in general. This may sound tedious and almost like re-learning how to program at first, but syntactic sugars and other abstractions exist to make this pattern more readable (like the for-comprehension we already saw in Scala).\nThere are already a few familiar examples of function composition that have been adopted by popular languages like Python, and are very intuitive (especially when dealing with data pipelines or modelling real world interactions in terms of sequential actions). Some examples are the map and the filter functions in Python. We already saw an example of map in the declarative code snippet above, so we won’t dive into its specifics here. Both mapand filter are examples of higher-order functions (HoFs) – functions which take other functions as input and/or themselves output functions. map and filter show that function composition can be very readable and intuitive. Furthermore, neither map nor filter modify their input in-place. Rather, they return a modified copy of the input to avoid external state mutation which is considered poor practice in FP. Later on we shall see that a related method called flatMap exists which turns out to play a key role in letting functional programmers write useful programs by chaining multiple potentially side effect-producing functions together. This is due to flatMap’s unique function signature (which flattens arrays of arrays into one array).\n\n\nTo pipe functions into other functions (as in function composition), we need functions that take other functions as input and can also output functions. When we treat functions this way, we basically treat them as first-class values which means like any other value, they can be passed and returned around. There is a mathematically-inspired reason, other than designing software as function composition, for using HoFs.\nWhen mathematicians write:\n\\[\n\\sum_{x=a}^{b} f(x)\n\\]\nwhere, say \\(a,b \\in \\mathcal{Z}\\), they understand that \\(f\\) stands for some general function. So there’s no need to write a separate expression for summing the integers, one for summing the squares of the integers, and one for summing the factorials of integers between \\([a,b]\\).\nLet’s write a function in Scala that sums the integers:\ndef sumInts(a: Int, b: Int): Int = \n    if a &gt; b then 0 else a + sumInts(a + 1, b)\nTo get the sum of squares, we’d need to define another function:\ndef square(x: Int): Int = x * x\n\ndef sumSquares(a: Int, b: Int): Int = \n    if a &gt; b then 0 else square(a) + sumSquares(a + 1, b)\nBut there’s clearly some repetition here, so we can factor out a common pattern. What if we changed the signature of sumInts to take a function as argument?\ndef sum(f: Int =&gt; Int, a: Int, b: Int): Int =\n    if a &gt; b then 0 else f(a) + sum(f, a + 1, b)\nNow we can write:\ndef sumInts(a: Int, b: Int) = sum(id, a, b) \nwhere id is the identity function:\ndef id(x: Int): Int = x\nand:\ndef sumSquares(a: int, b: Int) = sum(square, a, b)\nAdmittedly this doesn’t look great because we’re creating a lot of boilerplate functions. To get rid of this boilerplate, Scala was the first language to introduce the notion of anonymous functions (in Python these are known by the lambda keyword). We can think of anonymous functions as literals. Similar to how we can do println(\"hello world!\") without having to name the string literal \"hello world!\" using a variable, we can declare functions as literals. Using anonymous functions, the id and square functions above can be written respectively as:\n(x: Int): Int =&gt; x\n(x: Int): Int =&gt; x * x\nThis reduces our sumInts and sumSquares to:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b) // Types can be omitted if they can be inferred from context\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nAnonymous functions are syntactic sugar. That is, they aren’t necessary but make life easier.\n\n\n\nBut so far we’ve only used the HoF’s ability to accept functions as input. Let’s also use their ability to output functions. The pattern we are about to learn is called currying (after Haskell Curry), and it’s useful for, among other things, dependency injection.\nNote, again, the functions:\nsumInts(a: Int, b: Int) = sum(x =&gt; x, a, b)\nsumSquares(a: Int, b: Int) = sum(x =&gt; x * x, a, b)\nBoth a and b are passed into each unchanged. Is there a common pattern we can extract? We can use currying which is just partial application of the function. Here’s the curried version of sum:\ndef sum(f: Int =&gt; Int): (Int, Int) =&gt; Int =\n    def sumFn(a: Int, b: Int): Int =\n        if a &gt; b then 0 else f(a) + sumF(a + 1, b)\n    sumF\nOur sum functions now constructs and returns a new function which takes the rest of input the (a and b). This is called partial-application. We’ve split the sum into two parts: the first part accepts only the argument f as input, the second one accepts the rest of the arguments.\nNow we can define sumInts and sumSquares respectively as just:\ndef sumInts = sum(x =&gt; x)\ndef sumSquares = sum(x =&gt; x*x)\nNote that when we call sum, we get back a function with signature (Int, Int) =&gt; Int (which is exactly the signature we want for sumInts and sumSquares). We can now use better readable syntax like sum(cube)(1,5) + sum(squares)(5,10) doing away with sumInts and sumSquares (using which we’d have to write the above as: sumInts(1,5) + sumSquares(5,10)).\nSince it can get quite clumsy to write curried functions, Scala provides a shorthand. This is equivalent to the curried sum written above.\ndef sum(f: Int =&gt; Int)(a: Int, b: Int): Int = \n    if a &gt; b then 0 else f(a) sum(f)(a + 1, b)\nIn Python, there’s support for curried functions in the functools library (functools.partial, which implements a curried version of a function you pass to it (by itself currying the input).\nCurrying, in general, can be applied \\(n\\) times to an \\(n\\)-dimensional function, each outer function returning an (anonymous) inner function which partially applies the rest of the parameters. This means languages need not have support for functions with parameters, as long as they have support for anonymous functions.\n\n\n\nMonads are a term borrowed from Category Theory. They have a strict definition in mathematics, but for our purposes they’re useful black boxes that provide the following benefits (some already mentioned before in passing). Monads provide a way to compose potentially side-effect producing functions together, and in general they make function composition lend itself easier to being abstracted behind syntactic sugars (like do notation in Haskell) which make functional programs more readable and more useful for real-world applications. As a sidenote, to the readers who are familiar with JavaScript Promises, these are, in essence, the same as the side-effect type Unit in Scala (or, rather, more like the more specific IO side-effect). The main point is that a Promise also a monad, which makes it possible to come up with nice, syntactic sugars like async await. In fact, do notation in Haskell is actually the generalized version of this type of async await pattern that works with any monad and not just a Promise.\nMonads achieve this mostly by just being classes that have an implementation of flatMap. Yes, the same flatMap we discussed earlier in the context of it being one of the higher-order functions that are popularly used more mainstream languages like Python. It turns out, chaining two side-effect producing operations in function compositional produces nested side effect types. For example, chaining two IO operations that prompt for two strings may produce something like IO(IO(String)) and flatMap, with its unique capacity to flatten, is exactly the thing that’s needed to get an IO(String)) back. We will discuss this flattening property of flatMap in more detail later on. For now, it’s important to reiterate that having an implementation of flatMap (which may go by other names in other languages, e.g. Bind in Haskell) is mostly (along a few other key properties) what qualifies a class to be a monad.\nSo, it’s no wonder that in a functional programming language we want our side effect-producing expressions to return a monadic type so that you can chain two or more of such expressions together (using function composition).\n\n\n\n\n\n\nIn terms of parallelization, both iterative and recursive solutions can be sequential processes, which don’t lend themselves well to parallelization, or independent processes which do. However, FP still confers some benefit in terms of parallelization – not because it favors recursion but, instead, because:\n\nA common challenge in parallel programming is to avoid mutating data while another thread is using it. Due to state immutability principles in FP, this problem is eliminated\nFP avoids writing functions which rely on hidden state (i.e. any state that’s not a direct input), so functions can be executed in parallel without the concern of synchronizing access to some shared state.\nFP can make it easier to identify opportunities for parallelization\nLanguages which are built around FP have powerful parallelization libraries that offer parallelized versions of common operations like map\n\n\n\n\n\n\n\nIf we’re going to favor the use of recursion (or, in general function composition) in FP over the more imperative style of writing iterative algorithms, we ought to tread carefully as to not cause stack overflow (which, as we know, is when the system runs out of working memory). Tail recursion optimization (similar to other techniques like memoization) helps us drastically cut the amount of stack memory used. It takes a constant amount of memory on the stack, instead of the linear, with input size, or worse. Read more about tail recursive optimization here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "v-poghosyan.github.io",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nUnreal Engine - First Steps\n\n\nJan 13, 2023\n\n\n\n\nBuilding an LLM\n\n\nJan 13, 2023\n\n\n\n\nFunctional Programming\n\n\nJan 13, 2023\n\n\n\n\nLC121 and LC53 - Kadane’s Algorithm\n\n\nJan 23, 2022\n\n\n\n\nConnected Sinks and Sources\n\n\nJan 23, 2022\n\n\n\n\nLC11 - Container with Most Water\n\n\nJan 23, 2022\n\n\n\n\nLinear Programs and LP Geometry\n\n\nJan 23, 2022\n\n\n\n\nAlgorithms for Unconstrained Optimization\n\n\nJan 23, 2022\n\n\n\n\nIntroduction to Optimization\n\n\nJan 23, 2022\n\n\n\n\nRobust Linear Programs - Modelling Discrete Failures\n\n\nJan 23, 2022\n\n\n\n\nDuality Theory\n\n\nJan 23, 2022\n\n\n\n\nLinear Algebra for Optimization\n\n\nJan 23, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html",
    "href": "unpublished_posts/neuroscience_notes/grief.html",
    "title": "Nature of Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "href": "unpublished_posts/neuroscience_notes/grief.html#spatial-temporal-and-emotional-brain-activation",
    "title": "Nature of Grief",
    "section": "",
    "text": "The same area of the brain lights up when we do spatial calculations, as when we do temporal ones, as when we calculate our attachment to a given person, animal, or a thing. This suggests that attachment consists of two brain processes, among possibly some others: spatial calculation of how to access a person, an animal, or a thing that is the object of our grief, and a temporal calculation of when we saw them last, or how soon we can access the subject."
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "Secure Sockets Layer (SSL) and its successor Transport Layer Security (TLS) are cryptographic protocols that provide security in communication over a computer network using a combination of symmetric and asymmetric encryption (both of which are introduced later in this post). The protocol is widely used for such applications as HTTPS (HTTP protocol extended with TLS encryption), email, instant messaging, etc.\n\n\n\n\n\n\n\n\n\n\n\n(a) Private and public keys\n\n\n\n\n\nFigure 1: Each party has a set of public and private keys.\n\n\n\n\n\n\n\n\n\nSSL figure 1\n\n\n\n\n\n\n\n\nSSL figure 3\n\n\n\n\n\n\n\n\nSSL figure w"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#asymmetric-encryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "(a) Private and public keys\n\n\n\n\n\nFigure 1: Each party has a set of public and private keys."
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#encryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure 1"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#secure-communication",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure 3"
  },
  {
    "objectID": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "href": "unpublished_posts/network_and_security/ssl_the_internets_trust_protocol.html#decryption",
    "title": "SSL - The Internet’s Trust Protocol",
    "section": "",
    "text": "SSL figure w"
  },
  {
    "objectID": "unpublished_posts/game_development/conways_game_of_life.html",
    "href": "unpublished_posts/game_development/conways_game_of_life.html",
    "title": "Conway’s Game of Life - First Scala Project",
    "section": "",
    "text": "Conway’s Game of Life\nConway’s Game of Life is a zero-player game on a two dimensional grid of cells with 4 simple rules:\n\nAny live cell with &lt; 2 live neighbors dies (as if by underpopulation)\nAny live cell with 2-3 live neighbors lives on to the next generation\nAny live cell with &gt; 3 live neighbors dies (as if by overpopulation)\nAny dead cell with exactly 3 live cells becomes alive (as if by reproduction)\n\nThese rules are inteded to loosely model the mechanisms of reproduction in evolutionary biology. Interestingly, these simple rules result in a highly complex world in which perpetual patterns (such as gliders) can be used to transmit information over long distances and execute computational tasks by coming together in specific arrangements to form logic gates. To see this in action, check out this cool post by Nicholas Carlini. The existence of logic gates in Conway’s Game of Life makes it possible to program within the game, leading to incredible projects like Life in Life, where Conway’s Game of Life runs itself.\nFor the sake of practice, we will use functional programming to implement the game. Functional programming draws inspiration from the mathematical definition of a function – a well-defined operation on sets. For more information see the this post on the topic of FP."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html",
    "href": "unpublished_posts/python/introduction-to-pandas.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python’s equivalent of Excel.\nWe import Pandas into our development environment as we import any other library — using the import command.\nimport pandas\nIt’s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time.\nimport pandas as pd\nThis gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#series",
    "href": "unpublished_posts/python/introduction-to-pandas.html#series",
    "title": "Introduction to Pandas",
    "section": "Series",
    "text": "Series\nThe basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It’s a one-dimensional data structure that’s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren’t limited to the integer values \\(0,1,...n\\) — they can also be descriptive labels.\nLet’s create a Series object representing the populations of the G-7 countries in units of millions.\n\n\nCode\nimport pandas as pd\ng7_pop = pd.Series([35,63,80,60,127,64,318])\n\n\nAs we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor.\n\nIndexing\nIndexing a Series object is similar to indexing a Python list. For instance, let’s print the first element in the above series.\n\n\nCode\ng7_pop[0]\n\n\n35\n\n\nLet’s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten.\n\n\nCode\ng7_pop.index = [\n    'Canada',\n    'France',\n    'Germany',\n    'Italy',\n    'Japan',\n    'UK',\n    'US'\n]\n\n\nNow, we can print the first element of the series using its descriptive label.\n\n\nCode\ng7_pop['Canada']\n\n\n35\n\n\nWe may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it’s possible to create a labeled Series object directly from a Python dictionary.\n\n\nCode\ng7_pop = pd.Series({\n    'Canada' : 35,\n    'France' : 63,\n    'Germany' : 80,\n    'Italy' : 60,\n    'Japan' : 127,\n    'UK' : 64,\n    'US' : 318\n})\ng7_pop\n\n\nCanada      35\nFrance      63\nGermany     80\nItaly       60\nJapan      127\nUK          64\nUS         318\ndtype: int64\n\n\nIn the event of overriding the integer-based indices, it’s still possible to access the elements of a Series sequentially using the iloc method (short for “integer location”). To retrieve the population of Canada, we can do as follows:\n\n\nCode\ng7_pop.iloc[0]\n\n\n35\n\n\nIt’s also possible to use a range when indexing. For instance, suppose we’d like to retrieve the populations of the first three countries from g7_pop. We can simply write:\n\n\nCode\ng7_pop['Canada':'Germany']\n\n\nCanada     35\nFrance     63\nGermany    80\ndtype: int64\n\n\nOr equivalently:\n\n\nCode\ng7_pop.iloc[0:3]\n\n\nCanada     35\nFrance     63\nGermany    80\ndtype: int64\n\n\nSince the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask.\nFor instance, to print the populations of Canada and Germany at the same time, we can pass in the list ['Canada','Germany'] or the Boolean mask [True, False, True, False, False, False, False].\n\n\nCode\ng7_pop[['Canada','Germany']]\n\n\nCanada     35\nGermany    80\ndtype: int64\n\n\n\n\nCode\ng7_pop[[True, False, True, False, False, False, False]]\n\n\nCanada     35\nGermany    80\ndtype: int64\n\n\n\n\nBroadcasted and Vectorized Operations\nSince it’s based on a NumPy array, a Series object also supports vectorization and broadcasted operations.\nAs a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood.\nBroadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. It’s an overloading of arithmetic and Boolean operators.\nFor instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row (or using a list comprehension), we can simply use broadcasted addition.\n\n\nCode\ng7_2030_pop = g7_pop + 10\ng7_2030_pop\n\n\nCanada      45\nFrance      73\nGermany     90\nItaly       70\nJapan      137\nUK          74\nUS         328\ndtype: int64\n\n\n\n\nFiltering\nThanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it’s possible to write concise and readable filtering expressions on Series.\nFor instance, let’s return the list of countries with a population over 70 mln.\n\n\nCode\ng7_pop[g7_pop &gt;= 70]\n\n\nGermany     80\nJapan      127\nUS         318\ndtype: int64\n\n\nThe expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask.\nAs another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population.\n\n\nCode\ng7_pop[g7_pop &gt;= g7_pop.mean()]\n\n\nJapan    127\nUS       318\ndtype: int64"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#dataframe",
    "href": "unpublished_posts/python/introduction-to-pandas.html#dataframe",
    "title": "Introduction to Pandas",
    "section": "DataFrame",
    "text": "DataFrame\nEach DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself.\nThe DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary.\nIf we’re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object.\n\n\nCode\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndf = pd.DataFrame(data, index = ['R1','R2','R3'], columns = ['C1', 'C2', 'C3'])\ndf\n\n\n\n\n\n\n\n\n\nC1\nC2\nC3\n\n\n\n\nR1\n1\n2\n3\n\n\nR2\n4\n5\n6\n\n\nR3\n7\n8\n9\n\n\n\n\n\n\n\nWe may bypass specifying columns manually by passing in a dictionary instead:\n\n\nCode\ndata = {\n    'C1' : [1, 2, 3],\n    'C2' : [4, 5, 6],\n    'C3' : [7, 8, 9]\n}\ndf = pd.DataFrame(data, index = ['R1','R2','R3'])\ndf\n\n\n\n\n\n\n\n\n\nC1\nC2\nC3\n\n\n\n\nR1\n1\n4\n7\n\n\nR2\n2\n5\n8\n\n\nR3\n3\n6\n9\n\n\n\n\n\n\n\n\nNote: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they’re the column labels.\n\nIn practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so:\n\n\nCode\n#hide-output\ncsv_path = 'file.csv' #Stores the path to a CSV\ndf = pd.read_csv(csv_path)\n\n\n\nTip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header.\n\n\n\nTip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method.\n\n\nCommon Methods\nHere are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up.\n\ndf.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display.\ndf.tail() shows the last 5 rows, and also accepts an integer argument.\ndf.info() gives a bird’s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types.\ndf.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values.\ndf.shape - returns the dimension of the dataset as an \\((m,n)\\) tuple.\n\n\n\nIndexing\nLet’s add to the dataset of G-7 countries the columns 'GDP' and 'Surface Area'.\n\n\nCode\ng7_df = pd.DataFrame({\n    'Population' : g7_pop,\n    'GDP' : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7],\n    'Surface Area' : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0]\n})\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nWhereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected.\n\n\nCode\ng7_df['GDP']\n\n\nCanada     1.7\nFrance     2.8\nGermany    3.8\nItaly      2.1\nJapan      4.6\nUK         2.9\nUS         1.7\nName: GDP, dtype: float64\n\n\nMulti-indexing also works in the familiar way:\n\n\nCode\ng7_df[['Population','GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\nItaly\n60\n2.1\n\n\nJapan\n127\n4.6\n\n\nUK\n64\n2.9\n\n\nUS\n318\n1.7\n\n\n\n\n\n\n\nIf we want to index rows, however, we must use the loc or iloc methods.\nFor instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so:\n\n\nCode\ng7_df.loc['Canada':'Germany']\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\n\n\n\n\n\nIf we’re only interested in the population and GDP of the first three countries, then we could instead do the following:\n\n\nCode\ng7_df[['Population', 'GDP']].loc['Canada':'Germany']\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\n\n\n\n\n\nNoting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows:\n\n\nCode\ng7_df.loc['Canada':'Germany', ['Population', 'GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nCanada\n35\n1.7\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\n\n\n\n\n\n\n\nFiltering\nSince the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows:\n\n\nCode\ng7_df.loc[g7_df['Population'] &gt;= 70, 'GDP']\n\n\nGermany    3.8\nJapan      4.6\nUS         1.7\nName: GDP, dtype: float64\n\n\nAs additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers.\n\n\nCode\ng7_df.loc[g7_df['Surface Area'] &lt;= 1.0, ['Population','GDP']]\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\n\n\n\n\nFrance\n63\n2.8\n\n\nGermany\n80\n3.8\n\n\nItaly\n60\n2.1\n\n\nJapan\n127\n4.6\n\n\nUK\n64\n2.9\n\n\n\n\n\n\n\n\n\nAdding, Dropping, and Renaming Columns and Rows\nLet’s add a 'Languages' column to g7_df. We simply follow the same syntax as adding a key to a dictionary…\n\n\nCode\n# A DataFrame row is a series, so first we define one...\nlanguages = pd.Series({\n    'Canada' : 'French, English',\n    'France' : 'French',\n    'Germany' : 'German',\n    'Italy' : 'Italian',\n    'Japan' : 'Japanese',\n    'UK' : 'English',\n    'US' : 'English'\n})\n# Next, we add the series as a column to the DataFrame\ng7_df['Languages'] = languages\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nLanguages\n\n\n\n\nCanada\n35\n1.7\n9.0\nFrench, English\n\n\nFrance\n63\n2.8\n0.6\nFrench\n\n\nGermany\n80\n3.8\n0.3\nGerman\n\n\nItaly\n60\n2.1\n0.3\nItalian\n\n\nJapan\n127\n4.6\n0.3\nJapanese\n\n\nUK\n64\n2.9\n0.2\nEnglish\n\n\nUS\n318\n1.7\n9.0\nEnglish\n\n\n\n\n\n\n\nWe can also drop a column or a row using the drop() method.\nLet’s drop the newly created 'Languages' column. To drop a column, we specify a columns argument in the drop() method like so:\n\n\nCode\ng7_df.drop(columns = 'Languages')\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nThe method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let’s print it:\n\n\nCode\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nLanguages\n\n\n\n\nCanada\n35\n1.7\n9.0\nFrench, English\n\n\nFrance\n63\n2.8\n0.6\nFrench\n\n\nGermany\n80\n3.8\n0.3\nGerman\n\n\nItaly\n60\n2.1\n0.3\nItalian\n\n\nJapan\n127\n4.6\n0.3\nJapanese\n\n\nUK\n64\n2.9\n0.2\nEnglish\n\n\nUS\n318\n1.7\n9.0\nEnglish\n\n\n\n\n\n\n\nAs we can see the 'Languages' column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place.\n\n\nCode\ng7_df.drop(columns = 'Languages', inplace = True)\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nIn order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset.\n\n\nCode\ng7_df.drop(index = ['Canada', 'Italy'])\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\n\n\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUK\n64\n2.9\n0.2\n\n\nUS\n318\n1.7\n9.0\n\n\n\n\n\n\n\nIt is also possible to rename a column or a row using the rename() method.\nSuppose we’d like to rename the columns to include the units of measurement, and suppose we’d also like to expand the UK and US to their full names.\n\n\nCode\ng7_df.rename(\n    columns = {\n        'Population' : 'Population (mln)',\n        'GDP' : 'GDP (USD)',\n        'Surface Area' : 'Surface Area (mln sq. km)'\n    },\n    index = {\n        'UK' : 'United Kingdom',\n        'US' : 'United States'\n    }\n)\n\n\n\n\n\n\n\n\n\nPopulation (mln)\nGDP (USD)\nSurface Area (mln sq. km)\n\n\n\n\nCanada\n35\n1.7\n9.0\n\n\nFrance\n63\n2.8\n0.6\n\n\nGermany\n80\n3.8\n0.3\n\n\nItaly\n60\n2.1\n0.3\n\n\nJapan\n127\n4.6\n0.3\n\n\nUnited Kingdom\n64\n2.9\n0.2\n\n\nUnited States\n318\n1.7\n9.0\n\n\n\n\n\n\n\n\n\nManipulating Columns\nThose of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations.\nFor instance, suppose we’d like to add a new 'GDP Per Capita' column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population.\nUsing broadcasted division, the code is simply:\n\n\nCode\ng7_df['GDP Per Capita'] = g7_df['GDP'] / g7_df['Population']\ng7_df\n\n\n\n\n\n\n\n\n\nPopulation\nGDP\nSurface Area\nGDP Per Capita\n\n\n\n\nCanada\n35\n1.7\n9.0\n0.048571\n\n\nFrance\n63\n2.8\n0.6\n0.044444\n\n\nGermany\n80\n3.8\n0.3\n0.047500\n\n\nItaly\n60\n2.1\n0.3\n0.035000\n\n\nJapan\n127\n4.6\n0.3\n0.036220\n\n\nUK\n64\n2.9\n0.2\n0.045312\n\n\nUS\n318\n1.7\n9.0\n0.005346\n\n\n\n\n\n\n\n\n\nWorked Example - Bitcoin Price Timeseries: Cleaning and Reindexing\nSometimes we may wish to use a certain column to index a DataFrame. For instance, if we’re working with a dataset of Bitcoin prices, it would be wise to use the 'Time' column as the index so that we can easily access the value of Bitcoin at a given time.\n\nNote: Data that’s indexed by time is called a timeseries…\n\nFor this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below.\n\n\nCode\n#collapse-hide\n\nimport requests\nimport json\n\n# Specifying request URL, payload, and headers\nurl = 'https://api.coincap.io/v2/assets/bitcoin/history?interval=d1'\npayload = {}\nheaders = {}\n# Making the request and parsing it as JSON\nresponse = requests.request('GET', url, headers = headers, data = payload)\njson_data = json.loads(response.text)['data']\n# Converting the result into a DataFrame\nbitcoin_df = pd.json_normalize(json_data)\n# Cleanup\nbitcoin_df.rename(\n    columns = {\n        'priceUsd' : 'priceInUSD',\n        'time' : 'Time',\n        'date' : 'Date'\n    },\n    inplace = True\n)\n\n\nThe result of this is the following DataFrame:\n\n\nCode\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nTime\nDate\n\n\n\n\n0\n37480.8939504110899664\n1610668800000\n2021-01-15T00:00:00.000Z\n\n\n1\n36853.8623471143090244\n1610755200000\n2021-01-16T00:00:00.000Z\n\n\n2\n35670.6623897365179078\n1610841600000\n2021-01-17T00:00:00.000Z\n\n\n3\n36061.4760792230247237\n1610928000000\n2021-01-18T00:00:00.000Z\n\n\n4\n36868.3293610208260669\n1611014400000\n2021-01-19T00:00:00.000Z\n\n\n\n\n\n\n\nNow that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work.\nFirst order of business is to re-index the dataset based on the 'Time' column. We can set a column as index using the set_index() method in the following way:\n\n\nCode\nbitcoin_df.set_index('Time', inplace=True)\nbitcoin_df.index.name = None # The index column shouldn't have a name — this removes the name 'Time'\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nDate\n\n\n\n\n1610668800000\n37480.8939504110899664\n2021-01-15T00:00:00.000Z\n\n\n1610755200000\n36853.8623471143090244\n2021-01-16T00:00:00.000Z\n\n\n1610841600000\n35670.6623897365179078\n2021-01-17T00:00:00.000Z\n\n\n1610928000000\n36061.4760792230247237\n2021-01-18T00:00:00.000Z\n\n\n1611014400000\n36868.3293610208260669\n2021-01-19T00:00:00.000Z\n\n\n\n\n\n\n\nAs we can see the column that was previously named 'Time' now acts as index.\nNext, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. While doing this, let’s also convert the entries of the 'Date' column which are currently in string format.\n\n\nCode\nbitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit='us') # Converting index to datetime from Unix seconds\nbitcoin_df['Date'] = pd.to_datetime(bitcoin_df['Date']) # Converting 'Date' to datetime from string\nbitcoin_df.head()\n\n\n\n\n\n\n\n\n\npriceInUSD\nDate\n\n\n\n\n2021-01-15\n37480.8939504110899664\n2021-01-15 00:00:00+00:00\n\n\n2021-01-16\n36853.8623471143090244\n2021-01-16 00:00:00+00:00\n\n\n2021-01-17\n35670.6623897365179078\n2021-01-17 00:00:00+00:00\n\n\n2021-01-18\n36061.4760792230247237\n2021-01-18 00:00:00+00:00\n\n\n2021-01-19\n36868.3293610208260669\n2021-01-19 00:00:00+00:00\n\n\n\n\n\n\n\nNow we can comfortably access the price of Bitcoin on any given day. Suppose we’d like to know its price on 2021-12-28, the day of writing this post… We can simply do:\n\n\nCode\nbitcoin_df.loc['2021-12-28', 'priceInUSD']\n\n\n'48995.0145281203441155'"
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#finding-the-unique-elements-in-a-column",
    "href": "unpublished_posts/python/introduction-to-pandas.html#finding-the-unique-elements-in-a-column",
    "title": "Introduction to Pandas",
    "section": "Finding the Unique Elements in a Column",
    "text": "Finding the Unique Elements in a Column\nPandas comes with the method unique() which can be applied to a Series object.\nLet’s fetch some data about the planets in our solar system from the devstronomy repository.\n\n\nCode\nplanets_df = pd.read_csv('https://raw.githubusercontent.com/devstronomy/nasa-data-scraper/master/data/csv/planets.csv')\nplanets_df\n\n\n\n\n\n\n\n\n\nplanet\nmass\ndiameter\ndensity\ngravity\nescape_velocity\nrotation_period\nlength_of_day\ndistance_from_sun\nperihelion\n...\norbital_period\norbital_velocity\norbital_inclination\norbital_eccentricity\nobliquity_to_orbit\nmean_temperature\nsurface_pressure\nnumber_of_moons\nhas_ring_system\nhas_global_magnetic_field\n\n\n\n\n0\nMercury\n0.3300\n4879\n5427\n3.7\n4.3\n1407.6\n4222.6\n57.9\n46.0\n...\n88.0\n47.4\n7.0\n0.205\n0.034\n167\n0\n0\nNo\nYes\n\n\n1\nVenus\n4.8700\n12104\n5243\n8.9\n10.4\n-5832.5\n2802.0\n108.2\n107.5\n...\n224.7\n35.0\n3.4\n0.007\n177.400\n464\n92\n0\nNo\nNo\n\n\n2\nEarth\n5.9700\n12756\n5514\n9.8\n11.2\n23.9\n24.0\n149.6\n147.1\n...\n365.2\n29.8\n0.0\n0.017\n23.400\n15\n1\n1\nNo\nYes\n\n\n3\nMars\n0.6420\n6792\n3933\n3.7\n5.0\n24.6\n24.7\n227.9\n206.6\n...\n687.0\n24.1\n1.9\n0.094\n25.200\n-65\n0.01\n2\nNo\nNo\n\n\n4\nJupiter\n1898.0000\n142984\n1326\n23.1\n59.5\n9.9\n9.9\n778.6\n740.5\n...\n4331.0\n13.1\n1.3\n0.049\n3.100\n-110\nUnknown*\n79\nYes\nYes\n\n\n5\nSaturn\n568.0000\n120536\n687\n9.0\n35.5\n10.7\n10.7\n1433.5\n1352.6\n...\n10747.0\n9.7\n2.5\n0.057\n26.700\n-140\nUnknown*\n62\nYes\nYes\n\n\n6\nUranus\n86.8000\n51118\n1271\n8.7\n21.3\n-17.2\n17.2\n2872.5\n2741.3\n...\n30589.0\n6.8\n0.8\n0.046\n97.800\n-195\nUnknown*\n27\nYes\nYes\n\n\n7\nNeptune\n102.0000\n49528\n1638\n11.0\n23.5\n16.1\n16.1\n4495.1\n4444.5\n...\n59800.0\n5.4\n1.8\n0.011\n28.300\n-200\nUnknown*\n14\nYes\nYes\n\n\n8\nPluto\n0.0146\n2370\n2095\n0.7\n1.3\n-153.3\n153.3\n5906.4\n4436.8\n...\n90560.0\n4.7\n17.2\n0.244\n122.500\n-225\n0.00001\n5\nNo\nUnknown\n\n\n\n\n9 rows × 21 columns\n\n\n\nIf we want to find out the unique number of moons each planet has, we can simply do:\n\n\nCode\nplanets_df['number_of_moons'].unique()\n\n\narray([ 0,  1,  2, 79, 62, 27, 14,  5], dtype=int64)\n\n\nAs we can see, the 9 planets in our solar system (counting Pluto) have 8 unique number of moons. This is because, as we can see from the dataset, Mercury and Venus both have 0 moons."
  },
  {
    "objectID": "unpublished_posts/python/introduction-to-pandas.html#saving-data",
    "href": "unpublished_posts/python/introduction-to-pandas.html#saving-data",
    "title": "Introduction to Pandas",
    "section": "Saving Data",
    "text": "Saving Data\nAfter all the data manipulation, it would be useful to save the resulting dataset locally on our machine. Pandas offers us a way to do that using the DataFrame.to_csv() method.\nWorking with the planets_df defined above, we can narrow the dataset down to the planets which have a gravitational force that’s close to that of the Earth (\\(9.8  \\ m/s^2\\)).\n\n\nCode\nearthlike_planets_df = planets_df[(planets_df['gravity'] &gt;= 9.8 - 1) & (planets_df['gravity'] &lt;= 9.8 + 1)]\nearthlike_planets_df\n\n\n\n\n\n\n\n\n\nplanet\nmass\ndiameter\ndensity\ngravity\nescape_velocity\nrotation_period\nlength_of_day\ndistance_from_sun\nperihelion\n...\norbital_period\norbital_velocity\norbital_inclination\norbital_eccentricity\nobliquity_to_orbit\nmean_temperature\nsurface_pressure\nnumber_of_moons\nhas_ring_system\nhas_global_magnetic_field\n\n\n\n\n1\nVenus\n4.87\n12104\n5243\n8.9\n10.4\n-5832.5\n2802.0\n108.2\n107.5\n...\n224.7\n35.0\n3.4\n0.007\n177.4\n464\n92\n0\nNo\nNo\n\n\n2\nEarth\n5.97\n12756\n5514\n9.8\n11.2\n23.9\n24.0\n149.6\n147.1\n...\n365.2\n29.8\n0.0\n0.017\n23.4\n15\n1\n1\nNo\nYes\n\n\n5\nSaturn\n568.00\n120536\n687\n9.0\n35.5\n10.7\n10.7\n1433.5\n1352.6\n...\n10747.0\n9.7\n2.5\n0.057\n26.7\n-140\nUnknown*\n62\nYes\nYes\n\n\n\n\n3 rows × 21 columns\n\n\n\n\nTip: Pandas prefers the use of bitwise Boolean operators & and |, instead of the Python’s default and and or. This is because Pandas relies on NumPy, which in turn relies on the capacity of the bitwise operators to be overloaded.\n\n\nWe can now save this new dataset to our desktop as follows:\n\n\nCode\nearthlike_planets_df.to_csv('C:/Users/Vahram/Desktop/earthlike_planets.csv')"
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "",
    "text": "In this post we’ll be going over how to set up our local development environment for making machine learning applications and blogging about the process. This is my attempt at installing the required software packages on a Windows machine. I’ll do my best to keep things general but some of the steps will be Windows specific."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#installing-python---system-level",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Installing Python - System Level",
    "text": "Installing Python - System Level\nFirst, we download and install the latest version of Python for our OS from the official website.\n\n\n\n\n\n\n💡 Tip\n\n\n\n\n\nMake sure to tick the “add to PATH” box during the installation so that the path of the Python executible is added to our system’s PATH environment variable. The path in question, by default, is ~\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n\n\n\n\n\n\n\n\n\n🔧 Troubleshooting\n\n\n\n\n\nIf the command python is unrecognized on Windows after installation, try py. We should be able to issue the command py to invoke the Python interpreter. Running py --version should return the version number (e.g. Python 3.11.5)"
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#why-use-conda",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Why use Conda",
    "text": "Why use Conda\nWhile pip is Python’s built-in package manager and venv is its built-in virtual environment manager, we use Conda because it attempts to do more than what pip and venv try to accomplish do individually by extending support to library dependencies not written in Python.\nOccasionally, when a Conda distribution is not available, but an PyPI distribution exists, it makes sense to combine use of conda and pip. This is done by:\n\nInstalling pip within a Conda environment: conda install pip\nInstalling the required package from inside the active Conda environment: pip install &lt;package_name&gt;\n\nThis way, the packages do not go to the system-level Python’s packages directory C:\\Users\\&lt;username&gt;\\AppData\\Local\\Python\\&lt;version&gt;\\ (or Roaming instead of Local, if Python was installed only for a specific user on Windows). Instead, pip installs them in the Conda environment’s C:\\ProgramData\\anaconda3\\Lib\\site-packages (or similar) package directory. We can check each package, along with its installation destination by running pip list -v.\n\nInstalling Anaconda Navigator (or Miniconda)\nNext, download and install Anaconda Navigator (or Miniconda, which installs the Conda scientific package and Python environment manager without additional software and without the GUI navigator). This installation includes tools like Jupyter Notebooks, Spyder, PyCharm, and other scientific packages and IDEs.\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nAnaconda’s built in Python distribution: Anaconda comes with its own latest Python version distribution (by default installed into path c:\\ProgramData\\anaconda3\\python.exe). The installer will prompt us to select an option which enables third-party editors, such as VSCode, to recognize this Python distribution.\n\n\n\n\n\n\n\n\n\n📖 Note\n\n\n\n\n\nDifferent Python distributions can live on the same machine: Running python --version in the Anaconda Prompt returns Python 3.11.4 as of the time of writing this, which is the version of Python that Conda installed in its base environment. Crucially, running py --version, even in the Anaconda Prompt, still returns Python 3.11.5, which is the system’s version of Python."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#choosing-the-right-python-kernel-in-vscode",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Choosing the Right Python Kernel in VSCode",
    "text": "Choosing the Right Python Kernel in VSCode\nIn VSCode, we can open the Command Palette and run the command Notebook: Select Notebook Kernel. At first, this will prompt us to install the Jupyter and Python VSCode extensions. Once that’s done, we can rerun the command and select the Python kernel in the desired Conda environment (by default base)."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#initializing-conda-in-the-shell",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Initializing Conda in the Shell",
    "text": "Initializing Conda in the Shell\nBefore we can use the full capabilities of Conda in the terminal, we need to initialize it by running the command:\nconda init &lt;bash|powershell|tsh|...&gt; # Depending on the shell we're using\nRestart your terminal for changes to take hold.\n\n\n\n\n\n\n🔧 Troubleshooting\n\n\n\n\n\nFor Windows users, Powershell may throw the following error in trying to load the user profile: execution of scripts is disabled on this system. This is Powershell’s security measure against command hijacking, its way of enforcing control of execution and establishing identity. If this is the case, run cmd.exe as Administrator and execute command powershell Set-ExecutionPolicy RemoteSigned -Scope CurrentUser. We should now see the active environment in parentheses (e.g. base) to the left of the input in Powershell."
  },
  {
    "objectID": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "href": "unpublished_posts/python/set_up_a_local_development_environment_for_ML.html#conda-commands",
    "title": "Set Up a Local Development Environment for Machine Learning",
    "section": "Conda Commands",
    "text": "Conda Commands\nSome common Conda commands are:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nconda env list\nShows all the Conda environments (the active environment is marked with *)\n\n\nconda list\nShows all ther packages installed in the currently active environment\n\n\nconda update --all\nUpdates all packages in the active environment (frequently resolves environment is inconsistent errors)\n\n\nconda info\nShows, among other things, the directory where the environment is stored\n\n\nconda activate &lt;myenv&gt;\nActivate environment &lt;myenv&gt;\n\n\nconda deactivate\nDeactivates the currently active environment\n\n\nconda create --name &lt;myenv&gt;\nCreate a new empty environment\n\n\nconda create --name &lt;myenv&gt; --clone base\nClone the base environment\n\n\nconda env export -f &lt;path/to/envfile.yml&gt;\nExport the package list of the active environment (e.g. conda env export -f  /Users/&lt;username&gt;/Documents/MyFiles/personal-blog.yml)\n\n\nconda compare &lt;path/to/envfile.yml&gt;\nCompare the active environment to the exported file of another environment\n\n\nconda remove --name &lt;myenv&gt; --all\nDeletes the environment\n\n\n\n\nComparing Conda Environments\nOften we need to compare the packages between two environments. Here’s the workflow to do that:\n\nActivate one of the environments using activate\nExport its package list using export as a .yml file to a destination of our choice\nActivate the second environment\nExecute the compare command, providing the path to the .yml file created in the previous step"
  },
  {
    "objectID": "unpublished_posts/web_scraping/web_scraping.html",
    "href": "unpublished_posts/web_scraping/web_scraping.html",
    "title": "Price Scraper: Web Scraping and a Simple Notification System",
    "section": "",
    "text": "/etc/sbin/crontab contains a blueprint of a CRON job. These are run by a cron binary file in Linux systems periodically according to a set schedule. They are provided the schedule in a specific format (found in the crontab) and a tool + a script to run. For example python3 /my_script.py.\nFor periodic web-scraping, we can spin up an ECS cluster (or use self-hosting, more on that later). We can spin up a very small instance of a Linux container, optimizing for high availability."
  },
  {
    "objectID": "unpublished_posts/web_scraping/web_scraping.html#cron-jobs",
    "href": "unpublished_posts/web_scraping/web_scraping.html#cron-jobs",
    "title": "Price Scraper: Web Scraping and a Simple Notification System",
    "section": "",
    "text": "/etc/sbin/crontab contains a blueprint of a CRON job. These are run by a cron binary file in Linux systems periodically according to a set schedule. They are provided the schedule in a specific format (found in the crontab) and a tool + a script to run. For example python3 /my_script.py.\nFor periodic web-scraping, we can spin up an ECS cluster (or use self-hosting, more on that later). We can spin up a very small instance of a Linux container, optimizing for high availability."
  },
  {
    "objectID": "unpublished_posts/general_computer_science/data_structures_primer.html",
    "href": "unpublished_posts/general_computer_science/data_structures_primer.html",
    "title": "Data Structures Primer",
    "section": "",
    "text": "Graphs\nAll trees are graphs, but not all graphs are trees. Graphs that are acyclic are trees.\nBut how do you represent a tree data structure anyway?\n\n\nCode\nclass Graph:\n    def __init__(self):\n        self.graph = {}  # Initialize the adjacency list\n\n    def add_vertex(self, v):\n        if v not in self.graph:\n            self.graph[v] = []  # Add a new vertex with an empty adjacency list\n\n    def add_edge(self, v1, v2):\n        # Assuming an undirected graph...\n        if v1 in self.graph:\n            self.graph[v1].append(v2)\n        else:\n            self.graph[v1] = [v2]\n        \n        if v2 in self.graph:\n            self.graph[v2].append(v1)\n        else:\n            self.graph[v2] = [v1]\n\n    def __repr__(self): # String representation of object for logging\n        print(f\"-----{type(self).__name__}-----\\n\")\n        string_repr = \"\" # Initialize the string representation\n        for v, e in self.graph.items():\n            string_repr += f\"{v}: {e}\\n\"\n        return string_repr\n\n# Example usage\ng = Graph()\ng.add_vertex('╠11')\ng.add_vertex('C10')\ng.add_edge('╠11', 'C10')\ng.add_edge('╠11', '═21')  # Our add_edge assumes ═21 is automatically added as a vertex\nprint(g)\n\n\n-----Graph-----\n\n╠11: ['C10', '═21']\nC10: ['╠11']\n═21: ['╠11']"
  },
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let’s define a package: com.myservice."
  },
  {
    "objectID": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "href": "unpublished_posts/scala/microservices_in_scala_and_zio.html#zio-http",
    "title": "Microservices in Scala and Zio",
    "section": "",
    "text": "Let’s define a package: com.myservice."
  }
]
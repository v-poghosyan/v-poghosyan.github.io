{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Computer Graphics\"\n",
    "author: \"Vahram Poghosyan\"\n",
    "date: \"2025-05-24\"\n",
    "categories: [\"Game Development\", \"Computer Graphics\"]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "  text: |\n",
    "    <script type=\"application/javascript\" src=\"../../../javascript/light-dark.js\"></script>\n",
    "    <script type=\"importmap\">\n",
    "      {\n",
    "          \"imports\": {\n",
    "              \"three\": \"https://cdn.jsdelivr.net/npm/three@0.173.0/+esm\",\n",
    "              \"OBJLoader\": \"https://cdn.jsdelivr.net/npm/three@0.173.0/examples/jsm/loaders/OBJLoader.js\",\n",
    "              \"MTLLoader\": \"https://cdn.jsdelivr.net/npm/three@0.173.0/examples/jsm/loaders/MTLLoader.js\",\n",
    "              \"OrbitControls\": \"https://cdn.skypack.dev/three@0.133.0/examples/jsm/controls/OrbitControls.js\",\n",
    "              \"Cannon\": \"https://cdn.jsdelivr.net/npm/cannon-es@0.20.0/dist/cannon-es.js\"\n",
    "          }\n",
    "      }\n",
    "    </script>\n",
    "    <script type=\"module\" src=\"./javascript/three-js-basic-shader-demo-1.js\"></script>\n",
    "    <script type=\"module\" src=\"./javascript/three-js-basic-shader-demo-2.js\"></script>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenGL/WebGL & Shaders\n",
    "\n",
    "OpenGL is a cross-platform graphics API that is widely used in video game development and computer graphics (it's a competitor to Nvidia's DirectX). WebGL is a [language binding](https://en.wikipedia.org/wiki/Language_binding) of OpenGL in JavaScript. I will use WebGL and OpenGL interchangeably throughout these posts. It provides a set of functions for rendering 2D and 3D graphics, allowing developers to create complex visual effects and realistic environments. GLSL is part of OpenGL. In the Three.js post [Three.js: Introduction to 3D Graphics](../../visualization/three_js_in_jupyter/three_js_in_jupyter.ipynb), we learned how to create a simple 3D scene using Three.js, which is a JavaScript library that abstracts the use of OpenGL API. It uses OpenGL's API calls, internally, to compile, link, and send our GLSL shader code to the GPU.\n",
    "\n",
    "In this post, we will explore how to use shaders in Three.js to create custom visual effects. Shaders are small programs that run on the GPU and are used to control the rendering of graphics. They allow developers to manipulate the appearance of objects in a scene, such as their color, texture, and lighting. Shaders are separate from the [*rendering pipeline*](#the-three-stages-of-the-rendering-pipeline) (see below), they can be thought of as ad-hoc programs that run on the GPU in a massively parallel way. They can run at *any* point of the rendering pipeline, and do so totally independently of it. Because of this, shaders give us fine-grained control over the rendering process.\n",
    "\n",
    "Let's create a new scene containing a simple quadrilateral mesh (created using Three.js, which is an abstraction layer over OpenGL). We will use this quad mesh as our canvas to draw on (with shaders). First, we'll stick to 2D. We'll create a vertex shader that takes the vertices of our quadrelateral and maps them to the full screen. The fragment shader will contain most of the magic, for now. It will essentially paint the surface of the quad... \n",
    "\n",
    "<details><summary>Click to expand the code used to create a basic scene</summary>\n",
    "\n",
    "```js\n",
    "import * as three from 'https://cdn.jsdelivr.net/npm/three@0.173.0/+esm';\n",
    "\n",
    "/* Scene / Camera / renderer ---------------------------------------------- */ \n",
    "const bodyWidth = document.getElementById(\"quarto-document-content\").clientWidth;\n",
    "const bodyHeight = 600;\n",
    "const canvas = document.getElementById(\"three-d-canvas\");\n",
    "\n",
    "const scene = new three.Scene();\n",
    "const camera = new three.PerspectiveCamera(75, bodyWidth / bodyHeight, 0.1, 1000);\n",
    "const renderer = new three.WebGLRenderer({ canvas: canvas });\n",
    "renderer.setPixelRatio(window.devicePixelRatio);\n",
    "renderer.setSize(bodyWidth, bodyHeight);\n",
    "\n",
    "/* Lights ---------------------------------------------- */\n",
    "scene.add(new three.AmbientLight(0xffffff, 1));\n",
    "const dir = new three.DirectionalLight(0xffffff, 1);\n",
    "dir.position.set(10, 20, 10);\n",
    "scene.add(dir);\n",
    "\n",
    "/* Plane  ---------------------------------------------- */\n",
    "const quadMesh = new three.Mesh(\n",
    "    new three.PlaneGeometry(250, 250), // Plane is added to the XY plane (x+ = right, y+ = up, z+ = out of the screen)\n",
    ");\n",
    "scene.add(quadMesh); \n",
    "\n",
    "/* Camera ---------------------------------------------- */\n",
    "camera.position.set(0, 0, 200); // The z+ represetns 100 units towards the viewer (out of the screen)\n",
    "camera.lookAt(quadMesh.position);\n",
    "\n",
    "renderer.render(scene, camera);\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "This is pretty much it for the CPU-side code that creates the scene, camera, and renderer. The next step is to create the shaders.\n",
    "\n",
    "Final Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<canvas id='three-d-canvas-1'></canvas>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<canvas id='three-d-canvas-1'></canvas>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is, our empty canvas. Let's add some color to it. \n",
    "\n",
    "## Shader Code (GLSL)\n",
    "\n",
    "As mentioned above, the shader code is written in GLSL (OpenGL Shading Language). The *vertex shader* is responsible for transforming the vertices of the geometry. The fragment shader is responsible for determining the color of each pixel. Note that the fragment shader is also sometimes called *the pixel shader*.\n",
    "\n",
    "First, we create a vertex shader in the `/shaders` subdirectory of this page. Then, a fragment shader. We import these shaders into our JavaScript code and use them to create a `ShaderMaterial`. Finally, we apply this material to our quad mesh.\n",
    "\n",
    "```js\n",
    "import vertexShader from 'https://raw.githubusercontent.com/v-poghosyan/v-poghosyan.github.io/refs/heads/main/unpublished_posts/game_development/computer_graphics/shaders/shader2.vert?raw';\n",
    "import fragmentShader from 'https://raw.githubusercontent.com/v-poghosyan/v-poghosyan.github.io/refs/heads/main/unpublished_posts/game_development/computer_graphics/shaders/shader2.frag?raw';\n",
    "const shaderMaterial = new three.ShaderMaterial({\n",
    "    vertexShader: vertexShader,\n",
    "    fragmentShader: fragmentShader,\n",
    "    uniforms: {\n",
    "        uTime: { value: 0.0 },\n",
    "        uResolution: { value: new three.Vector2(bodyWidth, bodyHeight) }\n",
    "    }\n",
    "});\n",
    "\n",
    "quadMesh.material = shaderMaterial;\n",
    "```\n",
    "\n",
    "Note, if we add empty shaders to an object in our canvas we will see pitch black and the following error will be printed to the console:\n",
    "\n",
    "```\n",
    "Program Info Log: Vertex shader is not compiled.\n",
    "```\n",
    "This is expected, because we haven't written any code in our vertex shader yet. Let's code up our shaders! \n",
    "\n",
    "### GLSL Basics\n",
    "\n",
    "#### Main Function\n",
    "\n",
    "Every shader has a main function, which is the entry point for the shader code. We can write more functions, which main will call.\n",
    "\n",
    "```glsl\n",
    "void otherFunction() {\n",
    "    // More code here\n",
    "}\n",
    "\n",
    "void main() {\n",
    "    // Your code here\n",
    "}\n",
    "```\n",
    "#### Types\n",
    "\n",
    "GLSl is strongly typed, meaning we need to declare the types of our variables. The most common types are `float`, `int`, `vec2`, `vec3`, and `vec4`.\n",
    "\n",
    "- A `float` variable needs to explicitly have a decimal point, e.g. `1.0` or `3.14` or `2.` (otherwise it's interpreted as an `int`).\n",
    "- All `vec` types are vectors, which are arrays of floats. For example, `vec2` is a 2D vector (array of 2 floats). Vectors can be initialized using the `vec2(x, y)` constructor, where `x` and `y` are the components of the vector. Initializing based on one parameter will create a vector with all components equal to that parameter, e.g. `vec2(1.0)` will create a vector with both components equal to `1.0`.\n",
    "\n",
    "#### Reading Vectors\n",
    "\n",
    "We can read the components of a vector using the `x`, `y`, `z`, and `w` properties (`w` is the translation in homogeneous coordinates). If the vector represents a color, we can also use `r`, `g`, `b`, and `a` properties (where `a` is the opacity value). \n",
    "\n",
    "Given the vector `vec4 v = vec4(0.1,0.2,0.3,0.4)`, we can access its components as follows:\n",
    "\n",
    "| 0.1 | 0.2 | 0.3 | 0.4 |\n",
    "|-----|-----|-----|-----|\n",
    "| `v.x`   | `v.y`   | `v.z`   | `v.w`   |\n",
    "| `v.r`   | `v.g`   | `v.b`   | `v.a`   |\n",
    "| `v.s`   | `v.t`   | `v.p`   | `v.q`   |\n",
    "\n",
    "We can also access multiple components at once using the `xy`, `xyz`, and `xyzw` properties. For example, `v.xy` will return a `vec2` with the first two components of `v`, i.e. `vec2(0.1, 0.2)`. This is called vector *swizzling*.\n",
    "\n",
    "Swizzling use cases include:\n",
    "- Extracting a 2D vector from a 3D vector (such as for simple projection), e.g. `vec3 v = vec3(1.0, 2.0, 3.0); vec2 v2 = v.xy;` will create a `vec2` with the first two components of `v`.\n",
    "- Swapping color channels, e.g. `vec4 color = vec4(1.0, 0.0, 0.0, 1.0); vec4 swappedColor = color.bgra;` will create a new color with the blue channel in the first position, green in the second, red in the third, and alpha in the fourth.\n",
    "\n",
    "This is a very powerful way to go hop from dimension into dimension, and to manipulate vectors in a very flexible way. Say we wanted to go from 2D to 3D and we didn't care about the z-coordinate, we could do this:\n",
    "\n",
    "```glsl\n",
    "vec2 v1 = vec2(1.0, 2.0);\n",
    "vec3 v2 = v1.xyx // This will initialize a 3D vector with its z-coordinate equal to the x-coordinate of the 2D vector.\n",
    "```\n",
    "\n",
    "\n",
    "#### Attributes, Uniforms, and Varying\n",
    "\n",
    "Just as we can instantiate variables inside our GLSL shader code, some are also passed into our shaders from the external OpenGL context. These are called *attributes*, and *uniforms*.\n",
    "\n",
    "We've already seen how to pass uniforms into our shaders using Three.js's `ShaderMaterial`. Here's a quick recap:\n",
    "\n",
    "```js\n",
    "const shaderMaterial = new three.ShaderMaterial({\n",
    "    vertexShader: vertexShader,\n",
    "    fragmentShader: fragmentShader,\n",
    "    uniforms: {\n",
    "        uTime: { value: 0.0 },\n",
    "        uResolution: { value: new three.Vector2(bodyWidth, bodyHeight) }\n",
    "    }\n",
    "});\n",
    "```\n",
    "The above code passes two uniforms into our shaders: `uTime` and `uResolution`. These can be used in both the vertex and fragment shaders to control the rendering process. For example, `uTime` can be used to create animations, while `uResolution` can be used to adjust the rendering based on the size of the HTML canvas.\n",
    "\n",
    "The difference between attributes and uniforms is that attributes are per-vertex data, while uniforms are global data that is shared across all vertices and fragments. See table below for a summary of the differences:\n",
    "\n",
    "|              | Attribute      | Uniform      |\n",
    "|--------------|---------------|-------------|\n",
    "| Available in Vertex Shader?      | Read Only      | Read only   |\n",
    "| Available in Fragment Shader?    | N.A           | Read only   |\n",
    "| Set From     | CPU           | CPU         |\n",
    "| Contain Information      | Per Vertex    | Constant    |\n",
    "\n",
    "There are also *varying* variables, which are used to pass data from the vertex shader to the fragment shader (because they're the only variables that are writable to). Varying variables are written to inside the vertex shader and passed as read-only to the fragment shader. They are commonly used for interpolating values across the surface of a mesh. For example, if we want to pass the color of a vertex to the fragment shader, we can declare a varying variable in the vertex shader and assign it a value. Then, in the fragment shader, we can read that varying variable to get the interpolated color for each pixel.\n",
    "\n",
    "Here's the table summarizing the differences between attributes, uniforms, and varying variables:\n",
    "\n",
    "|              | Attribute      | Uniform      | Varying      |\n",
    "|--------------|---------------|-------------|-------------|\n",
    "| Available in Vertex Shader?      | Read Only      | Read only   | Read/Write  |\n",
    "| Available in Fragment Shader?    | N.A           | Read only   | Read Only   |\n",
    "| Set From     | CPU           | CPU         | Vertex Shader |\n",
    "| Contain Information      | Per Vertex    | Constant    | Per Fragment |\n",
    "\n",
    "Attributes contain information per vertex (i.e. data about each vertex, such as its color) because they're passed to the vertex shader as variables (which works on vector geometry -- i.e. vertices). Varying Variables, on the other hand, are used to pass data from the vertex shader down to the fragment shader, which works with fragments, hence they contain information *per fragment*.\n",
    "\n",
    "#### Normalized Vectors\n",
    "\n",
    "In GLSL, vectors are often normalized to have a length of 1. This is useful for many operations, such as lighting calculations and texture mapping. Normalizing a vector is done using the `normalize()` function. For example, if we have a vector `vec3 v = vec3(1.0, 2.0, 3.0);`, we can normalize it by calling `vec3 normalizedV = normalize(v);`. This will create a new vector with the same direction as `v`, but with a length of 1.\n",
    "\n",
    "Individual entries are normalized too. For example white is represented as `vec3(1.0, 1.0, 1.0)`, rather than `vec3(255, 255, 255)`. Note, this vector is not normalized. But *normalization* is also often used in this sense.\n",
    "\n",
    "### Writing Our First Basic Shaders\n",
    "\n",
    "Let's start with a fragment shader that maps all pixels to a single color. The function `main()` below runs per each pixel and sets their value to purple. We use this to simply color the entire quad purple.\n",
    "\n",
    "```glsl\n",
    "void main() {\n",
    "    // Make every pixel purple\n",
    "    gl_FragColor = vec4(0.5, 0.0, 0.5, 1.0); // Set the color to purple (RGBA)\n",
    "}\n",
    "```\n",
    "\n",
    "Now, let's write our first vertex shader. It will simply stretch out the rectangle to cover the entire screen. Then, we will pass the final vertex positions to the fragment shader, which will use them to color the pixels.\n",
    "\n",
    "As mentioned before, the vertex shader receives attributes (per vertex data). These include:\n",
    "\n",
    "```glsl\n",
    "attribute vec3 aPosition;\n",
    "attribute vec2 aTexCoord;\n",
    "```\n",
    "\n",
    "- `aPosition` is the position (relative to world space) of the vertex in 3D space (x, y, z). Our vertex shader will run once per vertex, and it will receive the position of that vertex as an attribute.\n",
    "- `aTexCoord` is the texture coordinate of the vertex (u, v). This is used to map textures onto the geometry in the fragment shading stage. This is the coordinate we would use in the fragment shader to draw on the pixel corresponding to this vertex.\n",
    "\n",
    "These attributes are automatically set by Three.js (via the OpenGL API, internally).\n",
    "\n",
    "In our vertex shader we define a Varying Variable `pos`, which will be used to pass the texture coordinates (`aTexCoord`) to the fragment shader. Essentially, after the vertex shader is done scrambling the vertices, the new texture coordinates of these scrambled vertices are passed down to the fragment shader.\n",
    "\n",
    "Let's stretch the quad to cover the entire screen.\n",
    "\n",
    "<details><summary>Click to expand the vertex shader code</summary>\n",
    "\n",
    "```glsl\n",
    "attribute vec3 aPosition;\n",
    "attribute vec2 aTexCoord;\n",
    "\n",
    "varying vec2 pos;\n",
    "\n",
    "void main() {\n",
    "    pos = aTexCoord;\n",
    "\n",
    "    vec4 position = vec4(aPosition, 1.0);\n",
    "    position.xy = position.xy * 2.0 - 1.0; // Map the positions from the [0,1] range to the [-1,1] range\n",
    "\n",
    "    gl_Position = position;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "This gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<canvas id='three-d-canvas-2'></canvas>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<canvas id='three-d-canvas-2'></canvas>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Three Main Stages of the Rendering Pipeline\n",
    "\n",
    "After some CPU side pre-processing, the rendering pipeline consists of three main stages:\n",
    "\n",
    "## Vertex Shading\n",
    "\n",
    "The first stage of the rendering pipeline is the vertex shading stage. In this stage, the vertex shader is executed for each vertex of the geometry. The vertex shader is responsible for transforming the vertices of the geometry from object space to clip space. It can also be used to perform other operations, such as calculating normals or texture coordinates.\n",
    "\n",
    "### World Matrix, Local Matrix, and Model Matrix\n",
    "\n",
    "This is where three crucial matrices come into play: the *model matrix*, the *view matrix*, and the *projection matrix*. The model matrix transforms the vertices from object space to world space, the view matrix transforms the vertices from world space to camera space, and the projection matrix transforms the vertices from camera space to clip space. The final position of each vertex is calculated by multiplying these three matrices together. As always with matrices, the order of multiplication matters. The final position of each vertex is given by:\n",
    "\n",
    "The position of a given vertex $\\mathbf{v}$ in clip space is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{\\text{clip}} = \\mathbf{P} \\cdot \\mathbf{V} \\cdot \\mathbf{M} \\cdot \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Once the position of each vertex is calculated, a *primitives assembly* stage is performed, where the vertices are grouped into primitives (e.g., triangles, lines, etc.). The primitives are then rasterized to generate fragments, which are the pixels that will be drawn on the screen.\n",
    "\n",
    "Geometry shading is a more advanced stage that can be used to generate new primitives based on the existing primitives. It is not always used, but it can be useful for certain effects, such as generating shadows or reflections.\n",
    "\n",
    "A *tessellation stage* can also be used to subdivide the primitives into smaller ones, allowing for more detailed rendering. This is often used in high-end graphics applications, such as video games and simulations.\n",
    "\n",
    "## Rasterization\n",
    "\n",
    "The magical step where vector-based primitives are converted into fragments (pixels). This is where the geometry is transformed into a 2D representation that can be displayed on the screen. The rasterization stage takes the primitives generated in the previous stage and converts them into fragments, which are the pixels that will be drawn on the screen. Each fragment is assigned a color based on the lighting and shading calculations performed in the next stage.\n",
    "\n",
    "Rasterization and *Ray-Tracing* (described briefly in the Three.js post [Three.js: Introduction to 3D Graphics](../../visualization/three_js_in_jupyter/three_js_in_jupyter.ipynb)) are two different approaches. Rasterization is a brute-force and more common approach, while ray-tracing is more accurate and produces more realistic images. Ray-tracing is often used in high-end graphics applications, such as movies and animations, where the quality of the image is more important than the speed of rendering. Rasterization, on the other hand, is used in real-time applications, such as video games, where speed is more important than quality. However, this is changing with the advent of modern GPUs that can perform ray-tracing in real-time.\n",
    "\n",
    "\n",
    "What's the difference? Rasterization and [ray-tracing](https://en.wikipedia.org/wiki/Ray_tracing_(graphics)) are two different algorithms that both, essentially, take *primitives* (which are vector based) and output *fragments* (which are the last abstraction layer before a pixel color value). The value of a given pixel is determined from the fragment that covers that pixel (and on whether other fragments are in front of *it* and the camera, at least in the case of rasterization). To understand the surface-level difference between rasterization and ray tracing, think in terms of two nested loops. Each object covers a certain area of pixels on the screen, so the rasterization algorithm takes each object first and determines which pixels it covers. Then, for each pixel it determines if the object is the closest one to the given pixel or not. Ray tracing, on the other hand, flips the loops. It asks, for each pixel, which object is the closest one to it. It does this by casting rays from the camera into the scene and checking for intersections with objects. \n",
    "\n",
    "Here's a great [video](https://www.youtube.com/watch?v=ynCxnR1i0QY&t=111s) from Nvidia that explains the difference between rasterization and ray-tracing in more detail. I will embed it here.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=ynCxnR1i0QY&t=111s >}}\n",
    "\n",
    "Raytracing can be used in conjunction with rasterization to achieve a balance between quality and performance. For example, ray-tracing can be used to calculate reflections and refractions, while rasterization can be used for the rest of the scene. This is often referred to as *hybrid rendering*. For example, *screen-space reflections (SSR)* is a technique that uses ray-tracing to calculate reflections in a scene, while rasterization is used for the rest of the scene. This allows for more realistic reflections without the performance overhead of full ray-tracing. *Ambient occlusion* is another technique that uses ray-tracing to calculate the amount of light that reaches a surface, while rasterization is used for the rest of the scene. While these can be thought of as *post-processing effect* I like to reserve the term *post-processing* for effects that are applied after the entire scene has been drawn and colored (i.e. after *fragment shading*). These effects include: *bloom*, *motion blur*, *depth of field*, *aliasing*, etc. However, after rasterization we already have a colored scene, more or less, so I can see why the term post-processing can still apply. Given that *fragment shading* and *post-processing* are at the same level in the pipeline, it's a matter of preference whether to call these effects post-processing or not. *Post-processing* is *fragment shading* (in essence).\n",
    "\n",
    "## Fragment Shading\n",
    "\n",
    "The final stage of the rendering pipeline is the fragment shading stage. In this stage, the fragment shader is executed for each fragment generated in the rasterization stage. The fragment shader is responsible for determining the color of each pixel based on the lighting and shading calculations performed in the previous stages. It can also be used to apply *textures*, perform *post-processing* effects, and more.\n",
    "\n",
    "A key object in this stage is the *frame buffer*, which is a memory buffer that stores the color and depth information of each pixel. A *Frame Buffer Object (FBO)* is an object that's used to store the shader calculations on a given scene to a texture rather than to the screen itself. This allows us to overlay post-processing effects. Each effect is drawn to its own *texture* within a separate FBO. What's shown on the screen is, then, all of these *textures* (along with other shader calculations performed during the fragment shading stage) overlaid on top of the rasterized scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler Angles and Rotation Matrices\n",
    "\n",
    "## Intertial Frame and Body Frame\n",
    "\n",
    "$R_{yaw}$, $R_{pitch}$, and $R_{roll}$.\n",
    "\n",
    "# Quaternion Rotation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
